<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 154]
- [cs.AI](#cs.AI) [Total: 66]
- [cs.IR](#cs.IR) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths](https://arxiv.org/abs/2601.11564)
*Ahilan Ayyachamy Nadar Ponnusamy,Karthic Chandran,M Maruf Hossain*

Main category: cs.CL

TL;DR: 研究大型语言模型扩展上下文窗口时面临的计算开销与模型质量之间的权衡，分析密集Transformer架构在无关上下文下的性能退化，以及MoE架构在不同上下文规模下的异常行为


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型不断扩展上下文窗口以支持复杂长文本推理和文档分析，管理扩展上下文带来了严重的计算开销。本研究旨在探索系统性能与模型质量之间的关键权衡，特别是在模型暴露于大量无关和干扰性上下文时的情况。

Method: 使用密集Transformer架构（Llama-3.1-70B和Qwen1.5-14B）进行研究，分析其在大量无关上下文下的性能表现。特别关注Key-Value缓存增长与性能退化的非线性关系。进一步扩展分析到混合专家架构，研究其在不同上下文规模下的行为异常。

Result: 研究发现性能退化与KV缓存的增长呈非线性关系。对MoE架构的扩展分析揭示了在不同上下文规模下的独特行为异常，表明架构优势可能在高令牌量下被基础设施瓶颈所掩盖。

Conclusion: 扩展大型语言模型的上下文窗口虽然有利于复杂推理任务，但会带来显著的计算开销和性能退化风险。需要更精细的架构设计和基础设施优化来平衡模型质量与系统性能，特别是在处理大量无关上下文时。

Abstract: The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.

</details>


### [2] [Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings](https://arxiv.org/abs/2601.11565)
*Pakorn Ueareeworakul,Shuman Liu,Jinghao Feng,Ling Hu,Zhantang Shi,Chengqi Sun,Liang Yao,Panyi Ouyang,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: Compass-Embedding v4是一个针对东南亚电商场景优化的多语言嵌入框架，通过类感知掩码、多样化训练语料构建和高效推理优化，解决了低资源语言语义表示、噪声监督和生产约束等核心挑战。


<details>
  <summary>Details</summary>
Motivation: 随着全球电商向新兴市场扩张，低资源语言缺乏高质量语义表示已成为检索、推荐和搜索系统的关键瓶颈。东南亚电商场景面临数据稀缺、噪声监督和严格生产约束的联合挑战，需要专门优化的多语言嵌入解决方案。

Method: 1. 提出Class-Aware Masking(CAM)：改进InfoNCE目标函数，抑制批次内无效负样本，提升语义判别能力；2. 构建多样化训练语料：通过上下文合成数据生成、跨语言翻译和结构化电商数据构建；3. 生产部署优化：结合鲁棒性驱动的大批次训练与球面模型融合，使用vLLM和FP8量化优化推理。

Result: 在多项多语言基准测试和专有电商任务评估中，Compass-Embedding v4在主要东南亚语言上达到最先进性能，在领域特定检索和分类任务上显著优于通用嵌入模型，同时在高资源语言上保持竞争力。

Conclusion: Compass-Embedding v4成功解决了东南亚电商场景中的多语言嵌入挑战，通过创新的训练策略和部署优化，为低资源语言提供了高质量的语义表示，在保持效率的同时显著提升了领域特定任务的性能。

Abstract: As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.

</details>


### [3] [Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology](https://arxiv.org/abs/2601.11567)
*Vanessa D'Amario,Randy Daniel,Alessandro Zanetti,Dhruv Edamadaka,Nitya Alaparthy,Joshua Tarkoff*

Main category: cs.CL

TL;DR: 评估六个小型开源医疗大语言模型在儿科内分泌学中的表现，发现模型一致性并非正确性的指标，提示微小变化会导致输出差异，强调需要更全面的诊断框架


<details>
  <summary>Details</summary>
Motivation: 小型开源医疗大语言模型在资源受限环境中具有应用潜力，但现有评估主要关注多选题准确率，缺乏对一致性、鲁棒性和推理行为的全面评估，需要更全面的诊断框架来理解实际临床决策支持中的潜在问题

Method: 使用多选题结合人工评估和临床审查，评估六个小型开源医疗LLM；在确定性设置中检查提示变化对模型输出和自我评估偏差的影响；在随机性设置中评估输出变异性，并研究一致性与正确性之间的关系；进行专家审查错误推理理由

Result: HuatuoGPT-o1-8B表现最佳；模型响应的高一致性并非正确性的指标，尽管HuatuoGPT-o1-8B一致性最高；HuatuoGPT-o1-8B和Diabetica-o1表现出自我评估偏差和对候选解释顺序的依赖；专家审查发现错误推理中包含临床可接受响应和临床疏忽；系统级扰动（如CUDA构建差异）会导致模型输出的统计显著变化

Conclusion: 小型医疗LLM对微小语义无关的提示扰动敏感，导致输出分歧，这对基于LLM评估的可重复性提出担忧；不同随机机制下的输出变异性凸显了需要更广泛的诊断框架来理解实际临床决策支持场景中的潜在陷阱

Abstract: Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models' output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.

</details>


### [4] [An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT](https://arxiv.org/abs/2601.11573)
*Muhammad Muneeb,David B. Ascher*

Main category: cs.CL

TL;DR: 提出一个用于生物信息学领域大语言模型微调的九步可重复流程，通过PRSGPT（多基因风险评分工具）和BioStarsGPT（社区论坛）两个用例验证，生成超过18万QA对，Qwen2.5-7B表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂生物信息学应用中缺乏专业知识，需要开发可扩展的领域特定微调方法，创建可在本地部署的隐私保护型生物信息学助手。

Method: 九步流程：整合多样数据源、结构化预处理、基于提示的QA生成（使用Google Gemini）、自然语言推理质量控制、语义去重、基于聚类的数据分割、使用LoRA的参数高效微调。对LLaMA-3.2-3B、Qwen2.5-7B、Gemma三个模型进行微调。

Result: Qwen2.5-7B表现最佳：PRSGPT的BLEU-4和ROUGE-1分别提升82%和70%，BioStarsGPT分别提升6%和18%。生成28,000+ PRSGPT QA对和154,282 BioStarsGPT QA对。PRSGPT在PRS工具比较任务上获得61.9%准确率（与Google Gemini的61.4%相当），但提供更丰富的方法细节和准确引用。BioStarsGPT在142个生物信息学问题上达到59%概念准确率。

Conclusion: 该流程实现了可扩展的领域特定LLM微调，支持隐私保护的本地部署生物信息学助手，探索了实际应用，并解决了开发和使用中的挑战、限制及缓解策略。

Abstract: Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on polygenic risk score (PRS) tools, and BioStarsGPT, trained on community forum discussions. The nine-step pipeline integrates diverse data sources, structured preprocessing, prompt-based question-answer (QA) generation (via Google Gemini), natural language inference (NLI) for quality control, semantic deduplication, clustering-based data splitting, and parameter-efficient fine-tuning using LoRA. We fine-tuned three LLMs (LLaMA-3.2-3B, Qwen2.5-7B, Gemma) and benchmarked them on over 14 lexical and semantic metrics. Qwen2.5-7B emerged as the best performer, with BLEU-4 and ROUGE-1 improvements of 82\% and 70\% for PRSGPT and 6\% and 18\% for BioStarsGPT, respectively. The open-source datasets produced include over 28,000 QA pairs for PRSGPT and 154,282 for BioStarsGPT. Human evaluation of PRSGPT yielded 61.9\% accuracy on the PRS tools comparison task, comparable to Google Gemini (61.4\%), but with richer methodological detail and accurate citations. BioStarsGPT demonstrated 59\% conceptual accuracy across 142 curated bioinformatics questions. Our pipeline enables scalable, domain-specific fine-tuning of LLMs. It enables privacy-preserving, locally deployable bioinformatics assistants, explores their practical applications, and addresses the challenges, limitations, and mitigation strategies associated with their development and use.

</details>


### [5] [Concept Attractors in LLMs and their Applications](https://arxiv.org/abs/2601.11575)
*Sotirios Panagiotis Chytas,Vikas Singh*

Main category: cs.CL

TL;DR: 论文提出LLM内部表示可通过迭代函数系统解释，层作为收缩映射向概念吸引子收敛，并基于此开发无需训练的吸引子干预方法解决多种任务


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常将语义相关但表面形式不同的提示映射到相似内部表示，这种现象需要理论解释，并探索利用这种特性开发高效、无需训练的方法解决实际任务

Method: 将LLM层解释为迭代函数系统中的收缩映射，向概念特定吸引子收敛；开发直接操作这些吸引子的无训练方法，包括语言翻译、幻觉减少、护栏设置和合成数据生成等任务

Result: 基于吸引子的干预方法在多项任务中匹配或超越专门基线，提供比大量微调更高效的替代方案，在基线表现不佳的场景中具有更好的泛化能力

Conclusion: LLM内部表示的吸引子结构为理解模型行为提供了理论框架，基于吸引子的无训练方法在多种实际任务中表现出色，为高效模型干预提供了新途径

Abstract: Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS), where layers act as contractive mappings toward concept-specific Attractors. We leverage this insight and develop simple, training-free methods that operate directly on these Attractors to solve a wide range of practical tasks, including language translation, hallucination reduction, guardrailing, and synthetic data generation. Despite their simplicity, these Attractor-based interventions match or exceed specialized baselines, offering an efficient alternative to heavy fine-tuning, generalizable in scenarios where baselines underperform.

</details>


### [6] [Bielik 11B v3: Multilingual Large Language Model for European Languages](https://arxiv.org/abs/2601.11579)
*Krzysztof Ociepa,Łukasz Flis,Remigiusz Kinas,Krzysztof Wróbel,Adrian Gwoździej*

Main category: cs.CL

TL;DR: Bielik 11B v3是一个针对波兰语优化的11B参数语言模型，基于Mistral 7B v0.2架构通过深度扩展构建，采用四阶段训练流程，在波兰语任务上超越其他专业模型和更大模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个针对波兰语优化的高性能语言模型，同时保持对其他欧洲语言的良好支持，为资源较少语言建立高效模型开发的新基准。

Method: 基于Mistral 7B v0.2架构，通过深度扩展到11B参数；采用四阶段训练流程：持续预训练、监督微调(SFT)、直接偏好优化(DPO)和强化学习。

Result: 在波兰语任务上显著超越其他专业波兰语模型，并优于许多参数多2-6倍的更大模型；在从基础语言理解到复杂推理的广泛任务中表现出色。

Conclusion: Bielik 11B v3不仅推进了波兰语的AI能力，还为资源较少语言开发参数高效、高性能模型建立了新基准，支持多种量化选项以适应不同硬件配置。

Abstract: We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.
  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.
  The model's parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.

</details>


### [7] [Speculative Decoding: Performance or Illusion?](https://arxiv.org/abs/2601.11580)
*Xiaoxuan Liu,Jiaxiang Yu,Jongseok Park,Ion Stoica,Alvin Cheung*

Main category: cs.CL

TL;DR: 本文首次在生产级推理引擎(vLLM)上系统研究推测解码(SD)的实际效果，发现验证阶段主导执行时间，接受长度在不同场景下差异显著，实测性能与理论上限存在较大差距。


<details>
  <summary>Details</summary>
Motivation: 推测解码已成为加速大语言模型推理的流行技术，但先前评估多基于研究原型和不切实际的小批量大小，其实际生产环境效果尚不明确。

Method: 在生产级推理引擎vLLM上系统研究多种SD变体(n-gram、EAGLE/EAGLE-3、Draft-Model、Multi-Token Prediction)，覆盖多样化工作负载、模型规模和批量大小，分析影响SD性能的关键因素并量化理论加速上限。

Result: 目标模型的验证阶段主导执行时间；接受长度在不同输出token位置、请求和数据集间差异显著；实测性能与理论上限存在实质性差距，揭示了SD改进的新研究机会。

Conclusion: 本研究首次在生产环境中系统评估推测解码，揭示了实际性能与理论预期的差距，为改进SD技术指明了新的研究方向。

Abstract: Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We present, to our knowledge, the first systematic study of SD on a production-grade and widely deployed inference engine (vLLM), covering multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token Prediction) across diverse workloads, model scales, and batch sizes. We analyze key factors governing SD performance, and quantify a theoretical upper bound on SD speedup. Our results show that verification by the target model dominates the execution, while acceptance length varies markedly across output token positions, requests, and datasets. Comparing measured performance with theoretical bounds reveals substantial gaps between observed and theoretical upper bounds, and we leverage this observation to highlight new research opportunities that our study opens up in improving SD.

</details>


### [8] [Enhancing the QA Model through a Multi-domain Debiasing Framework](https://arxiv.org/abs/2601.11581)
*Yuefeng Wang,ChangJae Lee*

Main category: cs.CL

TL;DR: ELECTRA-small模型在SQuAD v1.1及对抗数据集上存在偏见问题，通过多领域去偏框架显著提升了模型性能


<details>
  <summary>Details</summary>
Motivation: 尽管QA模型在机器阅读理解方面取得显著进展，但在复杂查询和对抗条件下仍存在偏见问题，这限制了其性能和可靠性

Method: 评估ELECTRA-small模型在SQuAD v1.1及对抗数据集AddSent和AddOneSent上的表现，识别词汇偏见、数值推理和实体识别错误，开发包含知识蒸馏、去偏技术和领域扩展的多领域去偏框架

Result: 在所有测试集上实现了高达2.6个百分点的Exact Match和F1分数提升，在对抗环境下也获得了显著增益

Conclusion: 针对性的偏见缓解策略能够有效提升自然语言理解系统的鲁棒性和可靠性

Abstract: Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.

</details>


### [9] [Towards AGI A Pragmatic Approach Towards Self Evolving Agent](https://arxiv.org/abs/2601.11658)
*Indrajit Kar,Sammy Zonunpuia,Zonunfeli Ralte*

Main category: cs.CL

TL;DR: 提出分层自进化多智能体框架，通过工具合成和进化算法实现LLM智能体的自主能力扩展和持续适应


<details>
  <summary>Details</summary>
Motivation: 传统LLM智能体部署后静态化，缺乏自主扩展能力、生成新工具或进化推理的能力，需要实现持续自适应

Method: 分层自进化多智能体框架：集成基础LLM、操作SLM智能体、代码生成LLM和教师LLM；采用三级工作流：任务尝试→工具合成→进化阶段（课程学习、奖励学习、遗传算法）

Result: 在TaskCraft数据集上评估：课程学习实现快速恢复和强泛化，奖励学习在高难度任务上表现优异，遗传算法提供高行为多样性；所有进化智能体均优于原始版本

Conclusion: 分层自进化框架实现了稳健、自主、自我改进的智能体进化，为LLM智能体的持续适应提供了有效解决方案

Abstract: Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.

</details>


### [10] [RAC: Retrieval-Augmented Clarification for Faithful Conversational Search](https://arxiv.org/abs/2601.11722)
*Ahmed Rayane Kebir,Vincent Guigue,Lynda Said Lhadj,Laure Soulier*

Main category: cs.CL

TL;DR: RAC框架通过检索增强生成基于语料库的澄清问题，使用对比偏好优化确保问题有文档依据，在四个基准测试中显著优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现有对话搜索系统的澄清问题生成主要关注流畅性和用户意图对齐，但缺乏对底层语料库的锚定，导致可能生成无法从可用文档中回答的问题

Method: 1) 比较多种检索索引策略；2) 微调大语言模型以充分利用检索上下文并鼓励生成基于证据的问题；3) 应用对比偏好优化，使有检索段落支持的问题优于无依据的替代方案

Result: 在四个基准测试中，RAC相比基线方法表现出显著改进。除了LLM-as-Judge评估外，还引入了基于NLI和数据到文本的新指标来评估问题与上下文的锚定程度，证明该方法能持续提升忠实性

Conclusion: RAC框架通过检索增强和对比偏好优化，成功生成了基于语料库的忠实澄清问题，解决了现有方法缺乏文档依据的问题，为对话搜索系统提供了更可靠的澄清机制

Abstract: Clarification questions help conversational search systems resolve ambiguous or underspecified user queries. While prior work has focused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-Augmented Clarification), a framework for generating corpus-faithful clarification questions. After comparing several indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the generation of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over ungrounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge assessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demonstrate that our approach consistently enhances faithfulness.

</details>


### [11] [Bridging Human Interpretation and Machine Representation: A Landscape of Qualitative Data Analysis in the LLM Era](https://arxiv.org/abs/2601.11739)
*Xinyu Pi,Qisen Yang,Chuong Nguyen,Hua Shen*

Main category: cs.CL

TL;DR: 论文提出了一个4×4框架来分类LLM在质性研究中的输出类型，揭示现有系统偏向低层次意义和低承诺表示，并规划了提升解释性和建模能力的路线图。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在质性研究支持中产生的输出差异很大，从忠实追踪的摘要到理论中介的解释和系统模型都有。为了明确这些差异，需要一个系统性的分类框架来理解LLM在质性研究中的不同应用层次和建模承诺。

Method: 引入一个4×4的景观框架，横跨四个意义建构层次（描述性、分类性、解释性、理论性）和四个建模层次（静态结构、阶段/时间线、因果路径、反馈动态）。将该框架应用于现有的LLM自动化系统进行分析。

Result: 应用该框架分析发现，现有LLM系统存在明显偏向：主要集中在低层次意义建构（描述性、分类性）和低承诺表示（静态结构、阶段/时间线），而缺乏可靠的高层次解释性/理论性推理和动态建模能力。

Conclusion: 基于揭示的差距，提出了一个研究议程：开发能够明确、可选择且可管理的解释性和建模承诺的LLM系统，提升LLM在质性研究中的高级推理和动态建模能力。

Abstract: LLMs are increasingly used to support qualitative research, yet existing systems produce outputs that vary widely--from trace-faithful summaries to theory-mediated explanations and system models. To make these differences explicit, we introduce a 4$\times$4 landscape crossing four levels of meaning-making (descriptive, categorical, interpretive, theoretical) with four levels of modeling (static structure, stages/timelines, causal pathways, feedback dynamics). Applying the landscape to prior LLM-based automation highlights a strong skew toward low-level meaning and low-commitment representations, with few reliable attempts at interpretive/theoretical inference or dynamical modeling. Based on the revealed gap, we outline an agenda for applying and building LLM-systems that make their interpretive and modeling commitments explicit, selectable, and governable.

</details>


### [12] [Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation](https://arxiv.org/abs/2601.11758)
*Arnab Das Utsa*

Main category: cs.CL

TL;DR: 提出基于社交媒体语言的透明焦虑检测方法，通过语言可解释特征建模和跨域验证，实现可靠、可泛化且对关键词鲁棒的焦虑检测


<details>
  <summary>Details</summary>
Motivation: 全球数亿人受焦虑影响，但大规模筛查有限。社交媒体语言提供可扩展检测机会，但现有模型缺乏可解释性、关键词鲁棒性验证和严格用户级数据完整性

Method: 使用Reddit帖子数据集，在精心策划的子版块上训练逻辑回归分类器，进行特征消融、关键词掩码实验、焦虑组与对照组密度差异分析，并使用临床访谈参与者进行外部验证

Result: 模型在移除情感或掩码关键词后仍保持高准确率；使用最少发帖历史的早期检测显著优于随机分类；跨域分析与临床访谈数据高度一致

Conclusion: 透明语言特征可支持可靠、可泛化且对关键词鲁棒的焦虑检测；提出的框架为跨不同在线场景的可解释心理健康筛查提供了可复现基线

Abstract: Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, we trained a logistic regression classifier on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.

</details>


### [13] [Industry-Aligned Granular Topic Modeling](https://arxiv.org/abs/2601.11762)
*Sae Young Moon,Myeongjun Erik Jang,Haoyan Luo,Chunyang Xiao,Antonios Georgiadis,Fran Silavong*

Main category: cs.CL

TL;DR: TIDE框架提出基于大语言模型的粒度主题建模方法，在工业应用中优于现有方法，并提供文档摘要、主题分层等辅助功能


<details>
  <summary>Details</summary>
Motivation: 主题建模在文本挖掘和数据分析中应用广泛，但现有方法在生成粒度主题方面的能力尚未充分探索，而粒度对商业应用具有重要价值

Method: 提出TIDE框架，核心是基于大语言模型的新型粒度主题建模方法，同时提供文档摘要、主题分层（topic parenting）和蒸馏等辅助功能

Result: 在多种公开和真实商业数据集上的实验表明，TIDE的主题建模方法优于现代主题建模方法，辅助组件为工业商业场景提供有价值支持

Conclusion: TIDE框架提供了一种有效的粒度主题建模解决方案，目前正在开源过程中，为商业应用提供深度洞察

Abstract: Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.

</details>


### [14] [Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models](https://arxiv.org/abs/2601.11776)
*Kaituo Zhang,Zhimeng Jiang,Na Zou*

Main category: cs.CL

TL;DR: 提出完全自反射的去毒框架，利用LLMs内在能力进行毒性检测、修正和模型精炼，无需外部模块或数据标注，实现更好的去毒效果和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 当前去毒技术很少利用LLMs内在的自校正和自奖励能力，而是依赖外部模块、人工数据标注或人工干预，这限制了可扩展性和一致性。需要开发能充分利用LLMs内置能力的自去毒方法。

Method: 提出完全自反射去毒框架：1) 毒性信号检测器——内部自识别机制；2) 系统性干预过程将毒性文本转化为非毒性对应文本；3) 迭代过程生成对比去毒数据集用于微调模型，增强安全连贯文本生成能力。

Result: 在DetoxLLM和ParaDetox等基准数据集上的实验表明，该方法在去毒性能上优于最先进方法，同时保持语义保真度。无需人工干预或外部组件，展现了LLMs内在的自去毒能力。

Conclusion: 该方法揭示了LLMs内在的自去毒能力，为减轻有害内容生成提供了一致有效的途径。研究结果强调了真正自调节语言模型的潜力，为更负责任和伦理指导的文本生成系统铺平了道路。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fully self-reflective detoxification framework that harnesses the inherent capacities of LLMs to detect, correct toxic content, and refine LLMs without external modules and data annotation. Specifically, we propose a Toxic Signal Detector --an internal self-identification mechanism, coupled with a systematic intervention process to transform toxic text into its non-toxic counterpart. This iterative procedure yields a contrastive detoxification dataset used to fine-tune the model, enhancing its ability for safe and coherent text generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox show that our method achieves better detoxification performance than state-of-the-art methods while preserving semantic fidelity. By obviating the need for human intervention or external components, this paper reveals the intrinsic self-detoxification ability of LLMs, offering a consistent and effective approach for mitigating harmful content generation. Ultimately, our findings underscore the potential for truly self-regulated language models, paving the way for more responsible and ethically guided text generation systems.

</details>


### [15] [Beyond Tokens: Concept-Level Training Objectives for LLMs](https://arxiv.org/abs/2601.11791)
*Laya Iyer,Pranav Somani,Alice Guo,Dan Jurafsky,Chen Shani*

Main category: cs.CL

TL;DR: 论文提出从传统的下一词预测目标转向概念级预测，通过将同一概念的不同表面形式（如"mom"、"mother"）分组，让语言模型学习更高层次的语义抽象，从而改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 下一词预测目标在词级别操作，会将语义等价但表面形式不同的延续（如"mom" vs "mother"）视为错误，从而惩罚有效的抽象、释义或概念正确的推理路径。这种训练信号与语义正确性之间的不匹配促使需要更高层次表示的学习目标。

Method: 提出从词级预测转向概念级预测，其中概念将同一想法的多个表面形式分组。介绍了将概念监督整合到LLM训练中的各种方法。

Result: 概念感知模型实现了更低的困惑度，在领域转移下具有更好的鲁棒性，并且在多样化的NLP基准测试中比基于NTP的模型表现更强。

Conclusion: 概念级监督作为一种改进的训练信号，能更好地将LLM与人类语义抽象对齐，是比传统下一词预测更优的训练方法。

Abstract: The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., ``mom'' vs. ``mother''). As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., ``mom,'' ``mommy,'' ``mother'' $\rightarrow$ \textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests \textit{concept-level supervision} as an improved training signal that better aligns LLMs with human semantic abstractions.

</details>


### [16] [Don't Start Over: A Cost-Effective Framework for Migrating Personalized Prompts Between LLMs](https://arxiv.org/abs/2601.12034)
*Ziyi Zhao,Chongming Gao,Yang Zhang,Haoyan Liu,Weinan Gan,Huifeng Guo,Yong Liu,Fuli Feng*

Main category: cs.CL

TL;DR: PUMA框架通过轻量级适配器迁移个性化提示，解决大语言模型升级时用户软提示失效问题，减少98%计算成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型个性化通常依赖用户特定的软提示，但当基础模型升级时，这些提示会失效，需要昂贵的全量重新训练。现有方法缺乏高效迁移个性化提示到不兼容模型的解决方案。

Method: 提出Prompt-level User Migration Adapter (PUMA)框架：1) 使用参数高效的适配器桥接语义鸿沟；2) 采用基于组的用户选择策略显著降低训练成本；3) 支持链式和聚合迁移等高级场景。

Result: 在三个大规模数据集上的实验表明：1) 性能匹配甚至超过从头训练；2) 计算成本降低高达98%；3) 在不同模型架构上具有强泛化能力；4) 在链式和聚合迁移场景中表现稳健。

Conclusion: PUMA通过将用户资产与底层模型解耦，为个性化AI的可持续演进提供了实用路径，实现了高效、低成本的个性化提示迁移，支持模型升级时的用户资产保留。

Abstract: Personalization in Large Language Models (LLMs) often relies on user-specific soft prompts. However, these prompts become obsolete when the foundation model is upgraded, necessitating costly, full-scale retraining. To overcome this limitation, we propose the Prompt-level User Migration Adapter (PUMA), a lightweight framework to efficiently migrate personalized prompts across incompatible models. PUMA utilizes a parameter-efficient adapter to bridge the semantic gap, combined with a group-based user selection strategy to significantly reduce training costs. Experiments on three large-scale datasets show our method matches or even surpasses the performance of retraining from scratch, reducing computational cost by up to 98%. The framework demonstrates strong generalization across diverse model architectures and robustness in advanced scenarios like chained and aggregated migrations, offering a practical path for the sustainable evolution of personalized AI by decoupling user assets from the underlying models.

</details>


### [17] [TWeddit : A Dataset of Triggering Stories Predominantly Shared by Women on Reddit](https://arxiv.org/abs/2601.11819)
*Shirlene Rose Bandela,Sanjeev Parthasarathy,Vaibhav Garg*

Main category: cs.CL

TL;DR: TWeddit数据集：一个标注Reddit中女性主要面临的触发经历（如流产、性暴力）的语料库，用于研究创伤内容检测和用户支持


<details>
  <summary>Details</summary>
Motivation: Reddit等平台用户常分享流产、性暴力等创伤经历，但缺乏手动触发警告，现有标注数据集稀缺，需要系统化资源来研究这些敏感内容

Method: 构建TWeddit数据集，收集Reddit上详细叙事（最长40,000字符），标注触发经历类别，进行语言分析和主题建模，比较道德基础表达

Result: TWeddit数据集显示标注故事表达独特主题和道德基础，与未标注内容存在显著差异，为未来研究提供有价值资源

Conclusion: TWeddit填补了Reddit触发经历标注数据集的空白，其语言特征分析表明该数据集适用于广泛研究，包括创伤内容检测和用户支持系统开发

Abstract: Warning: This paper may contain examples and topics that may be disturbing to some readers, especially survivors of miscarriage and sexual violence. People affected by abortion, miscarriage, or sexual violence often share their experiences on social media to express emotions and seek support. On public platforms like Reddit, where users can post long, detailed narratives (up to 40,000 characters), readers may be exposed to distressing content. Although Reddit allows manual trigger warnings, many users omit them due to limited awareness or uncertainty about which categories apply. There is scarcity of datasets on Reddit stories labeled for triggering experiences. We propose a curated Reddit dataset, TWeddit, covering triggering experiences related to issues majorly faced by women. Our linguistic analyses show that annotated stories in TWeddit express distinct topics and moral foundations, making the dataset useful for a wide range of future research.

</details>


### [18] [The Third VoicePrivacy Challenge: Preserving Emotional Expressiveness and Linguistic Content in Voice Anonymization](https://arxiv.org/abs/2601.11846)
*Natalia Tomashenko,Xiaoxiao Miao,Pierre Champion,Sarina Meyer,Michele Panariello,Xin Wang,Nicholas Evans,Emmanuel Vincent,Junichi Yamagishi,Massimiliano Todisco*

Main category: cs.CL

TL;DR: 2024年第三届VoicePrivacy挑战赛总结，聚焦语音匿名化技术，旨在隐藏说话人身份同时保留语言内容和情感状态


<details>
  <summary>Details</summary>
Motivation: 推动语音匿名化技术发展，解决在保护说话人身份隐私的同时保持语音内容（语言信息和情感状态）可用性的挑战

Method: 系统概述挑战框架，包括匿名化任务定义、数据集（开发与评估）、攻击模型、客观评估指标（隐私保护和实用性），描述六个基线匿名化系统并总结参赛者的创新方法

Result: 提供了第三届VoicePrivacy挑战赛的全面分析，包括框架设计、基线系统描述、参与者创新方法总结，以及评估结果分析

Conclusion: 提出关键见解和观察，为未来VoicePrivacy挑战赛设计提供指导，并识别语音匿名化研究的有前景方向

Abstract: We present results and analyses from the third VoicePrivacy Challenge held in 2024, which focuses on advancing voice anonymization technologies. The task was to develop a voice anonymization system for speech data that conceals a speaker's voice identity while preserving linguistic content and emotional state. We provide a systematic overview of the challenge framework, including detailed descriptions of the anonymization task and datasets used for both system development and evaluation. We outline the attack model and objective evaluation metrics for assessing privacy protection (concealing speaker voice identity) and utility (content and emotional state preservation). We describe six baseline anonymization systems and summarize the innovative approaches developed by challenge participants. Finally, we provide key insights and observations to guide the design of future VoicePrivacy challenges and identify promising directions for voice anonymization research.

</details>


### [19] [BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models](https://arxiv.org/abs/2601.12632)
*Kriti Bhattarai,Vipina K. Keloth,Donald Wright,Andrew Loza,Yang Ren,Hua Xu*

Main category: cs.CL

TL;DR: BioPulse-QA是一个评估大语言模型在生物医学领域表现的新基准，包含2280个专家验证的问答对，覆盖药物标签、试验方案和临床指南等新发布文档，旨在解决现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学基准存在多个局限性：依赖静态或过时数据集，无法捕捉动态、上下文丰富的生物医学知识；存在数据泄露风险（与模型预训练语料重叠）；忽视关键维度如语言变异的鲁棒性和人口统计学偏见。需要更全面、动态的评估框架。

Method: 开发BioPulse-QA基准，包含2280个专家验证的问答对，覆盖新发布的生物医学文档（药物标签、试验方案、临床指南）。包含提取式和抽象式问答格式，并引入扰动变体（如释义、拼写错误）。评估GPT-4o、GPT-o1、Gemini-2.0-Flash和LLaMA-3.1 8B Instruct四个在基准文档发布日期前发布的LLM。

Result: GPT-o1在药物标签上获得最高放松F1分数（0.92），Gemini-2.0-Flash次之（0.90）。临床试验是最具挑战性的来源，提取式F1分数低至0.36。模型在释义扰动上的性能差异大于拼写错误，偏见测试显示差异可忽略。

Conclusion: BioPulse-QA提供了一个可扩展且临床相关的框架，用于评估生物医学LLM。基准揭示了模型在不同文档类型上的性能差异，特别是在临床试验领域的挑战。该框架有助于更全面地评估LLM在动态、高风险生物医学环境中的能力。

Abstract: Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.
  Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.
  Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.
  Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.

</details>


### [20] [ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System](https://arxiv.org/abs/2601.11854)
*Yifei Zhang,Hooshang Nayyeri,Rinat Khaziev,Emine Yilmaz,Gokhan Tur,Dilek Hakkani-Tür,Hari Thadakamalla*

Main category: cs.CL

TL;DR: ATOD是一个用于评估任务导向对话系统中智能体行为的基准测试和对话生成框架，包含ATOD-Eval评估框架和基于记忆的评估器


<details>
  <summary>Details</summary>
Motivation: 现有任务导向对话系统基准缺乏对LLM驱动智能体行为的系统性评估支持，如多目标协调、长期推理、异步执行等能力

Method: 提出ATOD基准和合成对话生成管道，创建需要长期推理的丰富注释对话；构建ATOD-Eval评估框架，将智能体维度转化为细粒度指标；开发基于记忆的智能体评估器

Result: ATOD-Eval能够全面评估任务完成度、智能体能力和响应质量；提出的评估器在准确性和效率之间提供了更好的权衡

Conclusion: ATOD填补了高级任务导向对话系统评估的空白，为智能体行为提供了系统性的评估框架

Abstract: Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.

</details>


### [21] [CORE-T: COherent REtrieval of Tables for Text-to-SQL](https://arxiv.org/abs/2601.13111)
*Hassan Soliman,Vivek Gupta,Dan Roth,Iryna Gurevych*

Main category: cs.CL

TL;DR: CORE-T是一个用于多表文本到SQL的免训练框架，通过LLM生成表目的元数据和预计算表兼容性缓存，提高相关表检索的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实中的文本到SQL工作流通常需要连接多个表，但在大规模异构表集合中准确检索相关表成为端到端性能的关键瓶颈。现有方法中，密集检索召回率高但包含大量干扰项，而连接感知方法通常依赖额外假设或推理开销高。

Method: CORE-T框架包含三个核心组件：1) 使用LLM为每个表生成目的元数据；2) 预计算轻量级的表兼容性缓存；3) 推理时先用密集检索获取top-K候选表，再用单次LLM调用选择连贯可连接的表子集，最后通过简单的加法调整步骤恢复强兼容表。

Result: 在Bird、Spider和MMQA数据集上，CORE-T将表选择F1分数提高了最多22.7个百分点，同时检索的表数量减少了最多42%。在多表执行准确率方面，在Bird上提高了最多5.0个百分点，在MMQA上提高了最多6.9个百分点，且比LLM密集型基线使用的token数量减少了4-5倍。

Conclusion: CORE-T通过结合密集检索的高召回率和LLM的语义理解能力，在无需训练的情况下显著提高了多表文本到SQL中相关表检索的准确性和效率，同时大幅降低了计算开销。

Abstract: Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on extra assumptions and/or incur high inference overhead. We propose CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, DR returns top-K candidates; a single LLM call selects a coherent, joinable subset, and a simple additive adjustment step restores strongly compatible tables. Across Bird, Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while retrieving up to 42% fewer tables, improving multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens than LLM-intensive baselines.

</details>


### [22] [CTPD: Cross Tokenizer Preference Distillation](https://arxiv.org/abs/2601.11865)
*Truong Nguyen,Phi Van Dat,Ngan Nguyen,Linh Ngo Van,Trung Le,Thanh Hong Nguyen*

Main category: cs.CL

TL;DR: CTPD是首个在异构分词器模型间进行偏好蒸馏的统一框架，解决了传统方法因分词方案不兼容而无法进行细粒度白盒蒸馏的问题。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏在预训练和指令调优中广泛应用，但在语言模型与人类偏好对齐方面的应用仍待探索，特别是在更现实的跨分词器场景中。教师和学生模型分词方案的不兼容性阻碍了偏好信息的细粒度白盒蒸馏。

Method: 提出了跨分词器偏好蒸馏(CTPD)框架，包含三个关键创新：1)对齐跨度投影，将教师和学生token映射到共享字符级跨度以实现精确监督传递；2)跨分词器适配的token级重要性采样(TIS-DPO)改进信用分配；3)教师锚定参考，允许学生在DPO风格目标中直接利用教师的偏好。

Result: 理论分析将CTPD建立在重要性采样基础上，多个基准测试实验证实了其有效性，相比现有方法取得了显著的性能提升。

Conclusion: CTPD为不同分词方案间的偏好蒸馏提供了一个实用且通用的解决方案，为语言模型更易访问和高效的对齐打开了大门。

Abstract: While knowledge distillation has seen widespread use in pre-training and instruction tuning, its application to aligning language models with human preferences remains underexplored, particularly in the more realistic cross-tokenizer setting. The incompatibility of tokenization schemes between teacher and student models has largely prevented fine-grained, white-box distillation of preference information. To address this gap, we propose Cross-Tokenizer Preference Distillation (CTPD), the first unified framework for transferring human-aligned behavior between models with heterogeneous tokenizers. CTPD introduces three key innovations: (1) Aligned Span Projection, which maps teacher and student tokens to shared character-level spans for precise supervision transfer; (2) a cross-tokenizer adaptation of Token-level Importance Sampling (TIS-DPO) for improved credit assignment; and (3) a Teacher-Anchored Reference, allowing the student to directly leverage the teacher's preferences in a DPO-style objective. Our theoretical analysis grounds CTPD in importance sampling, and experiments across multiple benchmarks confirm its effectiveness, with significant performance gains over existing methods. These results establish CTPD as a practical and general solution for preference distillation across diverse tokenization schemes, opening the door to more accessible and efficient alignment of language models.

</details>


### [23] [Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving](https://arxiv.org/abs/2601.11866)
*Kie Shidara,Preethi Prem,Jonathan Kim,Anna Podlasek,Feng Liu,Ahmed Alaa,Danilo Bernardo*

Main category: cs.CL

TL;DR: 大型语言模型在医学QA基准测试中表现出色，但其临床推理的灵活性存在争议。研究通过mARC基准测试评估多个推理模型，发现强推理模型能更好地避免Einstellung效应陷阱，达到人类水平表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在医学QA基准测试中取得了高准确率，但其临床推理的灵活性一直存在争议。研究旨在探究先进的推理LLMs是否在临床推理中展现出更好的认知灵活性，特别是能否避免Einstellung效应导致的思维僵化。

Method: 使用医学抽象与推理语料库(mARC)作为评估基准，这是一个利用Einstellung效应设计的对抗性医学QA基准。评估了来自OpenAI、Grok、Gemini、Claude和DeepSeek家族的推理模型，测试它们在不同情境下避免过度依赖学习到的启发式模式的能力。

Result: 强推理模型比弱推理模型更频繁地避免了基于Einstellung的陷阱，在mARC上达到了人类水平的性能。在医生最常出错的问题上，前5名模型以高置信度正确回答了55%到70%的问题，表明这些模型可能比人类更不容易受到Einstellung效应的影响。

Conclusion: 强推理模型在医学推理中展现出改进的灵活性，在mARC基准测试上达到了与人类相当的性能水平。这表明先进的推理LLMs能够有效避免临床推理中的认知偏见，具备超越传统医学QA基准测试所衡量的复杂推理能力。

Abstract: Large Language Models (LLMs) have achieved high accuracy on medical question-answer (QA) benchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked whether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We assessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the medicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which utilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts where they become suboptimal. We found that strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC. On questions most commonly missed by physicians, the top 5 performing models answered 55% to 70% correctly with high confidence, indicating that these models may be less susceptible than humans to Einstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility in medical reasoning, achieving performance on par with humans on mARC.

</details>


### [24] [GloCTM: Cross-Lingual Topic Modeling via a Global Context Space](https://arxiv.org/abs/2601.11872)
*Nguyen Tien Phat,Ngo Vu Minh,Linh Van Ngo,Nguyen Thi Ngoc Diep,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: GloCTM是一个新颖的跨语言主题建模框架，通过统一的语义空间强制跨语言主题对齐，利用多语言预训练表示和词汇邻域扩展，显著提升主题连贯性和跨语言对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题模型通常在分离的语言特定空间中学习主题，依赖双语词典等对齐机制，这些机制往往无法捕捉深层的跨语言语义，导致主题空间松散连接。同时，这些方法忽视了多语言预训练表示中丰富的语义信号，限制了细粒度对齐能力。

Method: GloCTM通过在整个模型流程中构建统一的语义空间来强制跨语言主题对齐。方法包括：1）通过跨语言词汇邻域扩展词袋表示；2）使用局部和全局编码器推断主题比例，并通过内部正则化对齐其潜在表示；3）在输出层定义在组合词汇上的全局主题-词分布，结构上同步跨语言主题含义；4）引入中心核对齐损失，将潜在主题空间与多语言上下文嵌入对齐。

Result: 在多个基准测试上的实验表明，GloCTM显著提高了主题连贯性和跨语言对齐效果，优于强基线方法。

Conclusion: GloCTM通过统一的语义空间和深度语义对齐机制，有效解决了跨语言主题建模中的对齐问题，为多语言理解提供了更强大的框架。

Abstract: Cross-lingual topic modeling seeks to uncover coherent and semantically aligned topics across languages - a task central to multilingual understanding. Yet most existing models learn topics in disjoint, language-specific spaces and rely on alignment mechanisms (e.g., bilingual dictionaries) that often fail to capture deep cross-lingual semantics, resulting in loosely connected topic spaces. Moreover, these approaches often overlook the rich semantic signals embedded in multilingual pretrained representations, further limiting their ability to capture fine-grained alignment. We introduce GloCTM (Global Context Space for Cross-Lingual Topic Model), a novel framework that enforces cross-lingual topic alignment through a unified semantic space spanning the entire model pipeline. GloCTM constructs enriched input representations by expanding bag-of-words with cross-lingual lexical neighborhoods, and infers topic proportions using both local and global encoders, with their latent representations aligned through internal regularization. At the output level, the global topic-word distribution, defined over the combined vocabulary, structurally synchronizes topic meanings across languages. To further ground topics in deep semantic space, GloCTM incorporates a Centered Kernel Alignment (CKA) loss that aligns the latent topic space with multilingual contextual embeddings. Experiments across multiple benchmarks demonstrate that GloCTM significantly improves topic coherence and cross-lingual alignment, outperforming strong baselines.

</details>


### [25] [A Systematic Analysis of Chunking Strategies for Reliable Question Answering](https://arxiv.org/abs/2601.14123)
*Sofia Bennani,Charles Moslonka*

Main category: cs.CL

TL;DR: 本文系统评估了文档分块策略对RAG系统可靠性的影响，发现句子分块是最具成本效益的方法，重叠分块无显著益处，且存在"上下文悬崖"现象


<details>
  <summary>Details</summary>
Motivation: 工业实践中RAG系统通常依赖启发式方法进行文档分块，但缺乏对分块选择如何影响系统可靠性的系统性评估。本文旨在填补这一空白，为成本高效的部署提供实证指导。

Method: 在Natural Questions数据集上进行端到端评估，系统性地变化分块方法（token、句子、语义、代码）、分块大小、重叠和上下文长度。采用标准工业设置：SPLADE检索和Mistral-8B生成器。

Result: 发现四个关键结论：(1) 重叠分块无显著益处且增加索引成本；(2) 句子分块是最具成本效益的方法，在~5k tokens内与语义分块效果相当；(3) 存在"上下文悬崖"现象，超过~2.5k tokens后质量下降；(4) 最优上下文长度取决于目标：语义质量在小上下文中达到峰值，精确匹配需要更大的上下文。

Conclusion: 为RAG系统的成本高效部署提供了可操作的指导：推荐使用句子分块而非重叠分块，注意上下文长度的"悬崖效应"，并根据具体目标（语义质量vs精确匹配）选择适当的上下文长度。

Abstract: We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a "context cliff" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).

</details>


### [26] [Faithfulness vs. Safety: Evaluating LLM Behavior Under Counterfactual Medical Evidence](https://arxiv.org/abs/2601.11886)
*Kaijie Mo,Siddhartha Venkatayogi,Chantal Shaib,Ramez Kouzy,Wei Xu,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 研究构建了MedCounterFact反事实医学QA数据集，评估LLMs在面对反事实医学证据时的行为，发现现有模型倾向于不加批判地接受危险或不合理的证据。


<details>
  <summary>Details</summary>
Motivation: 在医学等高风险领域，模型应忠实遵循上下文，但当上下文与模型先验或安全协议不一致时，需要研究LLMs的行为和推理方式，特别是在面对反事实或对抗性医学证据时。

Method: 构建MedCounterFact反事实医学QA数据集，包含临床比较问题（评估治疗效果），将真实医学干预系统性地替换为四种反事实刺激（从未知词汇到有毒物质），并在多个前沿LLMs上进行评估。

Result: 评估显示，面对反事实证据时，现有模型倾向于不加批判地接受这些证据（即使危险或不合理），并提供自信且无保留的答案，表明目前模型在忠实性与安全性之间缺乏明确边界。

Conclusion: 虽然理论上应在忠实性与安全性之间划定界限，但研究发现现有模型尚未建立这样的边界，这对高风险领域应用提出了重要警示。

Abstract: In high-stakes domains like medicine, it may be generally desirable for models to faithfully adhere to the context provided. But what happens if the context does not align with model priors or safety protocols? In this paper, we investigate how LLMs behave and reason when presented with counterfactual or even adversarial medical evidence. We first construct MedCounterFact, a counterfactual medical QA dataset that requires the models to answer clinical comparison questions (i.e., judge the efficacy of certain treatments, with evidence consisting of randomized controlled trials provided as context). In MedCounterFact, real-world medical interventions within the questions and evidence are systematically replaced with four types of counterfactual stimuli, ranging from unknown words to toxic substances. Our evaluation across multiple frontier LLMs on MedCounterFact reveals that in the presence of counterfactual evidence, existing models overwhelmingly accept such "evidence" at face value even when it is dangerous or implausible, and provide confident and uncaveated answers. While it may be prudent to draw a boundary between faithfulness and safety, our findings reveal that there exists no such boundary yet.

</details>


### [27] [PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning](https://arxiv.org/abs/2601.11908)
*Byeongjin Kim,Gyuwan Kim,Seo Yeon Park*

Main category: cs.CL

TL;DR: PPA-Plan是一种针对长上下文推理的主动规划策略，通过识别潜在逻辑陷阱和错误假设，将其转化为负面约束，并在规划生成时明确避免这些约束，从而预防规划失败。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理中存在困难，特别是当相关信息稀疏分布时。现有的规划-执行框架因依赖表层线索而导致规划生成不可靠，规划可能基于错误假设，且一旦形成规划后难以识别问题并进行可靠修订。

Method: PPA-Plan采用主动规划策略：1) 识别潜在逻辑陷阱和错误假设；2) 将其表述为负面约束；3) 在规划生成时明确避免这些约束。这种方法在规划生成前预防失败，而非事后反应性修正。

Result: 在长上下文QA基准测试中，执行PPA-Plan生成的规划持续优于现有的规划-执行方法和直接提示方法。

Conclusion: PPA-Plan通过主动识别和避免潜在逻辑陷阱，显著提高了长上下文推理中规划-执行框架的可靠性和有效性，解决了现有方法中规划生成不可靠和修订困难的问题。

Abstract: Large language models (LLMs) struggle with reasoning over long contexts where relevant information is sparsely distributed. Although plan-and-execute frameworks mitigate this by decomposing tasks into planning and execution, their effectiveness is often limited by unreliable plan generation due to dependence on surface-level cues. Consequently, plans may be based on incorrect assumptions, and once a plan is formed, identifying what went wrong and revising it reliably becomes difficult, limiting the effectiveness of reactive refinement. To address this limitation, we propose PPA-Plan, a proactive planning strategy for long-context reasoning that focuses on preventing such failures before plan generation. PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and conditions plan generation on explicitly avoiding these constraints. Experiments on long-context QA benchmarks show that executing plans generated by PPA-Plan consistently outperforms existing plan-and-execute methods and direct prompting.

</details>


### [28] [LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding](https://arxiv.org/abs/2601.11913)
*Yichen Jiang,Peng Ye,Jiakang Yuan,Chongjun Tu,Lei Bai,Tao Chen*

Main category: cs.CL

TL;DR: 提出LSTM-MAS多智能体系统，通过模拟LSTM架构的层次信息流和门控记忆机制来解决大语言模型处理长上下文的问题，相比现有方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有处理长上下文的方法存在局限性：单LLM方法通常需要减少上下文窗口或优化注意力机制，导致额外计算成本或受限的扩展上下文长度；多智能体框架虽然能缓解这些限制，但仍易受错误累积和幻觉传播的影响

Method: 设计LSTM-MAS多智能体系统，模拟LSTM架构的层次信息流和门控记忆机制。系统采用链式架构组织智能体，每个节点包含：工作智能体（用于片段级理解）、过滤智能体（用于冗余减少）、判断智能体（用于持续错误检测）和管理智能体（用于全局调节信息传播和保留），分别对应LSTM的输入门、遗忘门、恒定误差循环单元和输出门

Result: 相比之前最佳的多智能体方法CoA，在NarrativeQA上提升40.93%，Qasper上提升43.70%，HotpotQA上提升121.57%，MuSiQue上提升33.12%

Conclusion: LSTM-MAS通过受控信息传递和选择性长程依赖建模，能有效避免错误累积和幻觉传播，为长上下文理解提供了有效的多智能体解决方案

Abstract: Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.

</details>


### [29] [Enhancing LLM-Based Data Annotation with Error Decomposition](https://arxiv.org/abs/2601.11920)
*Zhen Xu,Vedant Khatri,Yijun Dai,Xiner Liu,Siyan Li,Xuanming Zhang,Renzhe Yu*

Main category: cs.CL

TL;DR: 本文提出了一种诊断性评估范式，用于分析LLM在主观标注任务中的错误类型，区分模型特定错误与任务固有模糊性，并评估其对下游分析的影响。


<details>
  <summary>Details</summary>
Motivation: LLM在客观标注任务上已达到接近人类的准确率，但在涉及心理构念等主观标注任务中表现不稳定且易出错。现有评估方法将所有错误合并为单一对齐指标，这种简化方法掩盖了不同类型错误对最终分析结论的不同影响。

Method: 提出了一个诊断性评估范式，包含三个核心组件：1) 二维诊断分类法，按来源（模型特定vs任务固有）和类型（边界模糊vs概念误识别）分类LLM标注错误；2) 轻量级人工标注测试，从LLM标注中估计任务固有模糊性；3) 计算方法，按分类法分解观察到的LLM标注错误。该范式在四个教育标注任务上进行了验证。

Result: 验证了该范式的概念有效性和实际效用。理论上，为特定标注任务中过高对齐不现实以及单一对齐指标不足以反映LLM标注质量提供了实证证据。实践上，该范式可作为低成本诊断工具，评估特定任务是否适合LLM标注，并为进一步技术优化提供可操作的见解。

Conclusion: 提出的诊断评估范式能够有效区分LLM标注错误的不同类型，为理解LLM在主观标注任务中的局限性提供了更精细的分析框架，有助于更准确地评估LLM标注的适用性和质量。

Abstract: Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization.

</details>


### [30] [Mapping the maturation of TCM as an adjuvant to radiotherapy](https://arxiv.org/abs/2601.11923)
*P. Bilha Githinji,Aikaterini Melliou,Xi Yuan,Dayan Zhang,Lian Zhang,Zhenglin Chen,Jiansong Ji,Chengying Lv,Jinhao Xu,Peiwu Qin,Dongmei Yu*

Main category: cs.CL

TL;DR: 对2000-2025年69,745篇文献的大规模分析显示，中医药作为放疗辅助的整合肿瘤学领域经历了周期性演变，形成了五个主导主题轴，表明该领域已成熟并可能面临新的转折点，同时存在系统性阳性报告偏倚。


<details>
  <summary>Details</summary>
Motivation: 整合肿瘤学作为范式转变已发展约25年，中医药作为放疗辅助的证据轨迹需要系统梳理。本研究旨在通过大规模文献分析，揭示该领域的演变模式、主题结构和潜在偏倚，为未来发展提供见解。

Method: 对2000-2025年间69,745篇出版物进行大规模分析，采用主题建模工作流程确定稳定的主题结构，识别周期性演变模式，并评估跨出版物类型、主题领域和演变周期的报告偏倚。

Result: 识别出五个主导主题轴：癌症类型、支持性护理、临床终点、机制和方法学；发现该领域呈现定义-构思-测试模式的周期性演变；观察到系统性阳性报告偏倚，且这种偏倚在不同结构驱动因素中具有同质性。

Conclusion: 中医药作为放疗辅助的整合肿瘤学领域已成熟当前研究议程，正处于新突破的边缘。该领域呈现渐进专业化和潜在碎片化，同时存在全系统范围的阳性报告偏倚，需要更批判性的评估方法。

Abstract: The integration of complementary medicine into oncology represents a paradigm shift that has seen to increasing adoption of Traditional Chinese Medicine (TCM) as an adjuvant to radiotherapy. About twenty-five years since the formal institutionalization of integrated oncology, it is opportune to synthesize the trajectory of evidence for TCM as an adjuvant to radiotherapy. Here we conduct a large-scale analysis of 69,745 publications (2000 - 2025), emerging a cyclical evolution defined by coordinated expansion and contraction in publication output, international collaboration, and funding commitments that mirrors a define-ideate-test pattern. Using a theme modeling workflow designed to determine a stable thematic structure of the field, we identify five dominant thematic axes - cancer types, supportive care, clinical endpoints, mechanisms, and methodology - that signal a focus on patient well-being, scientific rigor and mechanistic exploration. Cross-theme integration of TCM is patient-centered and systems-oriented. Together with the emergent cycles of evolution, the thematic structure demonstrates progressive specialization and potential defragmentation of the field or saturation of existing research agenda. The analysis points to a field that has matured its current research agenda and is likely at the cusp of something new. Additionally, the field exhibits positive reporting of findings that is homogeneous across publication types, thematic areas, and the cycles of evolution suggesting a system-wide positive reporting bias agnostic to structural drivers.

</details>


### [31] [Event Detection with a Context-Aware Encoder and LoRA for Improved Performance on Long-Tailed Classes](https://arxiv.org/abs/2601.11932)
*Abdullah Al Monsur,Nitesh Vamshi Bommisetty,Gene Louis Kim*

Main category: cs.CL

TL;DR: 论文研究事件检测中LLM单向解码器架构的局限性，提出使用双向上下文和Macro-F1评估指标，通过LoRA微调提升长尾事件类别的性能


<details>
  <summary>Details</summary>
Motivation: 当前事件检测研究存在两个主要局限：1）仅解码器LLM的单向架构在处理需要双向上下文理解的自然语言理解任务时存在瓶颈；2）传统依赖Micro-F1评分会因偏向多数类别而系统性高估性能，需要更关注模型在长尾事件类型上的表现

Method: 采用句子上下文增强模型性能，使用低秩适应（LoRA）进行微调，特别关注Macro-F1作为评估指标，对比增强模型与标准仅解码器基线模型的性能

Result: 实验表明：1）句子上下文增强的模型性能优于标准仅解码器基线；2）LoRA微调显著提升Macro-F1分数，特别是对仅解码器模型，证明LoRA能有效增强LLM在长尾事件类别上的表现

Conclusion: 事件检测研究需要超越单向LLM架构的限制，采用更全面的评估指标（如Macro-F1），LoRA微调是提升模型在长尾事件类别性能的有效工具

Abstract: The current state of event detection research has two notable re-occurring limitations that we investigate in this study. First, the unidirectional nature of decoder-only LLMs presents a fundamental architectural bottleneck for natural language understanding tasks that depend on rich, bidirectional context. Second, we confront the conventional reliance on Micro-F1 scores in event detection literature, which systematically inflates performance by favoring majority classes. Instead, we focus on Macro-F1 as a more representative measure of a model's ability across the long-tail of event types. Our experiments demonstrate that models enhanced with sentence context achieve superior performance over canonical decoder-only baselines. Using Low-Rank Adaptation (LoRA) during finetuning provides a substantial boost in Macro-F1 scores in particular, especially for the decoder-only models, showing that LoRA can be an effective tool to enhance LLMs' performance on long-tailed event classes.

</details>


### [32] [$\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models](https://arxiv.org/abs/2601.11969)
*Zecheng Tang,Baibei Ji,Ruoxi Sun,Haitian Wang,WangJie You,Zhang Yijun,Wenpeng Zhu,Ji Qi,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: MemoryRewardBench：首个系统研究奖励模型评估长时记忆管理能力的基准，覆盖长上下文理解和长文本生成任务，包含10种不同记忆管理模式的设置，上下文长度从8K到128K token。


<details>
  <summary>Details</summary>
Motivation: 随着工作越来越多地采用以内存为中心的机制以分段方式处理长上下文，有效的内存管理是使大语言模型能够在整个序列中有效传播信息的关键能力之一。因此，利用奖励模型自动可靠地评估内存质量至关重要。

Method: 引入MemoryRewardBench基准，系统研究奖励模型评估长时记忆管理过程的能力。该基准覆盖长上下文理解和长文本生成任务，包含10种不同记忆管理模式的设置，上下文长度从8K到128K token。在13个前沿奖励模型上进行评估。

Result: 评估表明开源模型与专有模型之间的性能差距正在缩小，新一代模型无论参数数量如何都持续优于前代模型。进一步揭示了当前奖励模型在评估LLM内存管理方面的能力和基本局限性。

Conclusion: MemoryRewardBench为系统评估奖励模型在长时记忆管理方面的能力提供了首个基准，揭示了当前模型的进展和局限性，为未来改进提供了重要参考。

Abstract: Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.

</details>


### [33] [Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning](https://arxiv.org/abs/2601.12019)
*Chaowei Zhang,Xiansheng Luo,Zewei Zhang,Yi Zhu,Jipeng Qiang,Longwei Wang*

Main category: cs.CL

TL;DR: 该论文提出SORG框架，利用LLM的奉承倾向生成对立推理对，并开发ORCD模型进行点击诱饵检测，在三个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线内容的广泛传播加剧了对点击诱饵的担忧。虽然大语言模型（LLMs）为解决此问题提供了有前景的途径，但其效果常受奉承倾向（Sycophancy）的阻碍，即倾向于生成符合用户信念而非真实情况的推理，这偏离了指令遵循原则。

Method: 提出SORG框架，利用LLM的奉承倾向生成高质量的对立推理对（同意和不同意推理），无需真实标签。开发ORCD模型，使用三个BERT编码器分别表示新闻标题及其相关推理，通过基于LLM生成可信度分数的软标签指导的对比学习来增强检测鲁棒性。

Result: 在三个基准数据集上的实验评估表明，该方法在点击诱饵检测任务上持续优于LLM提示、微调的小型语言模型以及最先进的点击诱饵检测基线方法。

Conclusion: 该研究提出了一种创新方法，将LLM的奉承倾向从需要消除的缺陷转变为可利用的资源，通过生成对立推理对并构建专门的检测模型，显著提升了点击诱饵检测的性能。

Abstract: The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users' beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.

</details>


### [34] [Preserving Fairness and Safety in Quantized LLMs Through Critical Weight Protection](https://arxiv.org/abs/2601.12033)
*Muhammad Alif Al Hakim,Alfan Farizki Wicaksono,Fajri Koto*

Main category: cs.CL

TL;DR: 量化会降低LLM的公平性和安全性，动态量化比静态量化更稳定，非英语语言的安全性问题更严重，作者提出Critical Weight Protection技术来缓解这些问题


<details>
  <summary>Details</summary>
Motivation: 量化被广泛用于降低大语言模型的计算成本，但其对公平性和安全性的影响，特别是在动态量化和多语言环境中的影响，尚未得到充分研究。需要系统研究量化方法如何影响公平性和安全性，并开发缓解策略。

Method: 1. 系统研究静态和动态量化方法对公平性和安全性的影响；2. 在英语、法语、荷兰语、西班牙语、土耳其语中评估公平性；3. 在英语、韩语、阿拉伯语中评估安全性；4. 提出Critical Weight Protection技术，识别和保护公平性和安全性关键权重。

Result: 1. 量化一致性地降低公平性和安全性；2. 动态量化方法比静态量化更稳定；3. 公平性退化在不同语言间存在差异；4. 安全性退化在非英语环境中尤为显著；5. Critical Weight Protection能有效缓解偏见和安全性退化，无需昂贵的重新训练或对齐。

Conclusion: 量化对LLM的公平性和安全性有负面影响，特别是在多语言环境中。Critical Weight Protection技术提供了一种有效的缓解方法，能够在保持效率的同时维护模型的可靠性，为量化部署提供了实用的解决方案。

Abstract: Quantization is widely adopted to reduce the computational cost of large language models (LLMs); however, its implications for fairness and safety, particularly in dynamic quantization and multilingual contexts, remain underexplored. In this work, we conduct a systematic study of how static and dynamic quantization methods impact fairness and safety across benchmarks measuring intrinsic and extrinsic bias and safety alignment. For fairness, we evaluate English, French, Dutch, Spanish, and Turkish; for safety, we focus on English, Korean, and Arabic. Our findings reveal that quantization consistently degrades fairness and safety, with dynamic methods demonstrating greater stability than static ones. Moreover, fairness degradation varies across languages, while safety deterioration is especially pronounced in non-English settings. To address these risks, we introduce Critical Weight Protection, a novel technique that identifies and preserves fairness- and safety-critical weights during quantization. This approach effectively mitigates bias and safety deterioration without costly retraining or alignment, maintaining trustworthiness while retaining efficiency.

</details>


### [35] [Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation](https://arxiv.org/abs/2601.12061)
*Jinsook Lee,Kirk Vanacore,Zhuqian Zhou,Jeanine Grutter,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 论文提出代码本注入分割方法，将边界决策与下游标注标准结合，评估LLM分割器在对话行为标注中的表现，发现不同分割器各有优劣，需根据下游目标优化选择。


<details>
  <summary>Details</summary>
Motivation: 传统对话行为标注将交际或教学意图局限于单个话语或轮次，导致标注者在底层动作上一致但在片段边界上存在分歧，降低了表面可靠性。需要改进分割方法以提高标注一致性。

Method: 提出代码本注入分割方法，将边界决策条件化于下游标注标准；评估LLM分割器与标准和检索增强基线的对比；引入无金标准评估指标：跨度一致性、区分性和人机分布一致性。

Result: DA感知分割器产生的片段内部一致性优于纯文本基线；LLM擅长创建结构一致的跨度，但基于连贯性的基线在检测对话流全局转变方面更优；两个数据集中没有单一分割器占主导；片段内连贯性的提升常以边界区分性和人机分布一致性为代价。

Conclusion: 分割是重要的设计选择，应根据下游目标优化而非单一性能分数；不同分割方法各有优势，需要权衡片段内连贯性、边界区分性和分布一致性等指标。

Abstract: Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score.

</details>


### [36] [Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset](https://arxiv.org/abs/2601.12068)
*Rowzatul Zannat,Abdullah Al Shafi,Abdul Muntakim*

Main category: cs.CL

TL;DR: 该研究创建了首个全面的孟加拉语症状-疾病数据集（包含758个症状-疾病关系，涵盖85种疾病），并开发了基于症状的疾病预测模型，集成方法达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 针对孟加拉语人群健康信息获取不足的问题，特别是缺乏孟加拉语疾病预测资源，研究旨在填补这一空白，提高孟加拉语社区的医疗可及性。

Method: 1. 构建包含758个独特症状-疾病关系、涵盖85种疾病的孟加拉语数据集；2. 公开数据集以确保透明度和可复现性；3. 评估多种机器学习模型在孟加拉语症状输入下的疾病预测性能；4. 采用软投票和硬投票集成方法结合表现最佳的模型。

Result: 软投票和硬投票集成方法均达到98%的准确率，表现出卓越的鲁棒性和泛化能力，为孟加拉语疾病预测建立了基准性能。

Conclusion: 该研究为孟加拉语疾病预测提供了基础资源，推动了本地化健康信息学和诊断工具的发展，有助于提高孟加拉语社区在早期疾病检测和医疗干预方面的公平获取。

Abstract: Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions.

</details>


### [37] [To Copy or Not to Copy: Copying Is Easier to Induce Than Recall](https://arxiv.org/abs/2601.12075)
*Mehrdad Farahani,Franziska Penzkofer,Richard Johansson*

Main category: cs.CL

TL;DR: 该论文通过提取"仲裁向量"来研究语言模型在检索增强设置中如何权衡参数知识和上下文信息，发现诱导复制比恢复召回更容易实现


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在检索增强设置中如何仲裁存储在权重中的参数知识与提示中的上下文信息，理解模型在这两种知识源之间的选择机制

Method: 从模型激活中提取仲裁向量：在精心设计的数据集上计算残差流质心差异，区分无关上下文（引发参数召回）和相关但错误上下文（引发复制）。将该向量作为加性干预注入选定层和标记跨度，以引导行为在两个方向转变：复制→召回（抑制上下文使用）和召回→复制（诱导模型复制上下文中的任何标记）

Result: 在两个架构（仅解码器和编码器/解码器）和两个开放域QA基准测试中，在适度扩展下观察到一致的行为转变，同时监控准确性和流畅性。机制分析显示不对称性：诱导复制是容易的"再激活"过程，可以在输入的不同位置触发；而恢复召回是更脆弱的"抑制"过程，与对象标记干预紧密相关

Conclusion: 语言模型在参数知识和上下文信息之间的仲裁机制存在不对称性，诱导复制相对容易实现，而抑制上下文使用以恢复参数召回则更为脆弱，这为理解模型在检索增强设置中的行为提供了机制性见解

Abstract: Language models used in retrieval-augmented settings must arbitrate between parametric knowledge stored in their weights and contextual information in the prompt. This work presents a mechanistic study of that choice by extracting an \emph{arbitration vector} from model activations on a curated dataset designed to disentangle (i) irrelevant contexts that elicit parametric recall and (ii) relevant but false contexts that elicit copying. The vector is computed as the residual-stream centroid difference between these regimes across 27 relations, and is injected as an additive intervention at selected layers and token spans to steer behavior in two directions: Copy$\rightarrow$Recall (suppressing context use) and Recall$\rightarrow$Copy (inducing the model to copy any token from the context). Experiments on two architectures (decoder-only and encoder/decoder) and two open-domain QA benchmarks show consistent behavior shifts under moderate scaling while monitoring accuracy and fluency. Mechanistic analyses of attention routing, MLP contributions, and layer-wise probability trajectories reveal an asymmetry: inducing copying is an easy ``reactivation'' process that can be triggered at different locations in the input, while restoring recall is a ``suppression'' process that is more fragile and strongly tied to object-token interventions.

</details>


### [38] [Large language models struggle with ethnographic text annotation](https://arxiv.org/abs/2601.12099)
*Leonardo S. Goodall,Dor Shilton,Daniel A. Mullins,Harvey Whitehouse*

Main category: cs.CL

TL;DR: 评估7个先进大语言模型在567个民族志文本片段上标注121个仪式特征的能力，发现性能有限，无法替代人类专家进行民族志标注


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在自动化文本标注方面的潜力，特别是能否加速跨文化研究，从民族志文本中提取结构化数据

Method: 使用7个最先进的大语言模型，在567个民族志文本片段上评估其对121个仪式特征的标注能力，并与人类编码者的可靠性进行比较

Result: LLM性能有限，远低于可靠自动化标注所需水平；长文本、需要序数区分的特征和模糊概念特别困难；人类编码者可靠性设定了LLM准确性的近似上限；即使在人类编码者能可靠达成一致的特征上，模型表现也低于人类

Conclusion: 大语言模型目前尚无法替代人类专家进行民族志标注，需要进一步改进才能用于跨文化研究的自动化文本分析

Abstract: Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation.

</details>


### [39] [Powerful Training-Free Membership Inference Against Autoregressive Language Models](https://arxiv.org/abs/2601.12104)
*David Ilić,David Stanojević,Kostadin Cvejoski*

Main category: cs.CL

TL;DR: EZ-MIA是一种基于错误区域的成员推理攻击方法，通过分析模型在错误位置的记忆表现，显著提升了检测精度，特别是在低误报率下。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型存在隐私风险，可能记忆并泄露训练数据中的敏感信息。现有成员推理攻击方法检测率有限，特别是在实际隐私审计所需的低误报率阈值下表现不佳。

Method: 提出EZ-MIA攻击方法，基于关键观察：记忆在错误位置表现最强（模型预测错误但仍对训练样本显示较高概率）。引入错误区域分数，测量错误位置相对于预训练参考模型的概率偏移方向性不平衡。该方法仅需每个查询两次前向传播，无需任何模型训练。

Result: 在WikiText与GPT-2上，EZ-MIA在相同条件下检测率比先前最优方法高3.8倍（1%误报率下真阳性率66.3% vs 17.5%），AUC达0.98。在关键的0.1%误报率下，检测率比先前工作高8倍（14.0% vs 1.8%）。在AG News与Llama-2-7B上，检测率高3倍（1%误报率下46.7% vs 15.8%）。

Conclusion: 微调语言模型的隐私风险比先前理解的要大得多，这对隐私审计和部署决策都有重要影响。EZ-MIA为隐私风险评估提供了更有效的工具。

Abstract: Fine-tuned language models pose significant privacy risks, as they may memorize and expose sensitive information from their training data. Membership inference attacks (MIAs) provide a principled framework for auditing these risks, yet existing methods achieve limited detection rates, particularly at the low false-positive thresholds required for practical privacy auditing. We present EZ-MIA, a membership inference attack that exploits a key observation: memorization manifests most strongly at error positions, specifically tokens where the model predicts incorrectly yet still shows elevated probability for training examples. We introduce the Error Zone (EZ) score, which measures the directional imbalance of probability shifts at error positions relative to a pretrained reference model. This principled statistic requires only two forward passes per query and no model training of any kind. On WikiText with GPT-2, EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under identical conditions (66.3% versus 17.5% true positive rate at 1% false positive rate), with near-perfect discrimination (AUC 0.98). At the stringent 0.1% FPR threshold critical for real-world auditing, we achieve 8x higher detection than prior work (14.0% versus 1.8%), requiring no reference model training. These gains extend to larger architectures: on AG News with Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR). These results establish that privacy risks of fine-tuned language models are substantially greater than previously understood, with implications for both privacy auditing and deployment decisions. Code is available at https://github.com/JetBrains-Research/ez-mia.

</details>


### [40] [Bengali Text Classification: An Evaluation of Large Language Model Approaches](https://arxiv.org/abs/2601.12132)
*Md Mahmudul Hoque,Md Mehedi Hassain,Md Hojaifa Tanvir,Rahul Nandy*

Main category: cs.CL

TL;DR: 本研究评估了三种指令调优大语言模型（LLaMA 3.1 8B Instruct、LLaMA 3.2 3B Instruct和Qwen 2.5 7B Instruct）在孟加拉语新闻文章分类任务上的表现，其中Qwen 2.5以72%的准确率表现最佳。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语文本分类是NLP中的重要任务，但与英语相比面临标注数据集和预训练语言模型缺乏的挑战。本研究旨在探索大语言模型在孟加拉语新闻文章分类中的有效性。

Method: 使用来自Kaggle的Prothom Alo（孟加拉国主要报纸）新闻文章数据集，在相同的分类框架下评估三种指令调优大语言模型：LLaMA 3.1 8B Instruct、LLaMA 3.2 3B Instruct和Qwen 2.5 7B Instruct。

Result: Qwen 2.5 7B Instruct获得了最高的分类准确率72%，在"体育"类别表现尤为突出。LLaMA 3.1和LLaMA 3.2分别获得53%和56%的准确率。

Conclusion: 研究结果表明，尽管孟加拉语NLP资源稀缺，大语言模型在孟加拉语文本分类中仍然有效。未来研究将探索更多模型、解决类别不平衡问题并改进微调方法以提升分类性能。

Abstract: Bengali text classification is a Significant task in natural language processing (NLP), where text is categorized into predefined labels. Unlike English, Bengali faces challenges due to the lack of extensive annotated datasets and pre-trained language models. This study explores the effectiveness of large language models (LLMs) in classifying Bengali newspaper articles. The dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for this task under the same classification framework. Among the evaluated models, Qwen 2.5 achieved the highest classification accuracy of 72%, showing particular strength in the "Sports" category. In comparison, LLaMA 3.1 and LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings highlight the effectiveness of LLMs in Bengali text classification, despite the scarcity of resources for Bengali NLP. Future research will focus on exploring additional models, addressing class imbalance issues, and refining fine-tuning approaches to improve classification performance.

</details>


### [41] [Analyzing Cancer Patients' Experiences with Embedding-based Topic Modeling and LLMs](https://arxiv.org/abs/2601.12154)
*Teodor-Călin Ionescu,Lifeng Han,Jan Heijdra Suasnabar,Anne Stiggelbout,Suzan Verberne*

Main category: cs.CL

TL;DR: 研究评估BERTopic和Top2Vec在癌症患者访谈数据上的主题建模性能，结合GPT-4进行主题标注，发现BERTopic配合BioClinicalBERT嵌入模型能有效提取临床相关主题，为患者导向的医疗实践提供支持。


<details>
  <summary>Details</summary>
Motivation: 从患者叙事数据中提取有意义的主题，为更以患者为中心的医疗实践提供洞察。当前需要有效的方法来分析大量患者访谈文本，以支持临床决策和医疗流程改进。

Method: 使用BERTopic和Top2Vec两种神经主题建模方法分析13个癌症患者访谈（132,722词）。采用相似的预处理、分块和聚类配置进行公平比较，然后使用GPT-4进行主题标注。通过人工评估（关注连贯性、清晰度和相关性）比较模型性能。选择表现更好的BERTopic，进一步测试三种临床导向嵌入模型，最终使用最佳模型设置分析完整数据集。

Result: BERTopic表现优于Top2Vec。在临床嵌入模型中，BioClinicalBERT产生最一致的结果，提高了主题精确度和可解释性。全局分析揭示两个主导主题："癌症护理管理中的协调与沟通"和"癌症治疗旅程中的患者决策"。尽管数据是机器翻译且无临床专业人员参与评估，但结果表明该流程能为临床医生提供有用反馈。

Conclusion: 神经主题建模（特别是BERTopic）能有效从患者访谈中提取临床相关主题，支持更高效的文档导航并增强患者声音在医疗工作流程中的作用。该流程有潜力为患者导向的医疗实践提供实用工具。

Abstract: This study investigates the use of neural topic modeling and LLMs to uncover meaningful themes from patient storytelling data, to offer insights that could contribute to more patient-oriented healthcare practices. We analyze a collection of transcribed interviews with cancer patients (132,722 words in 13 interviews). We first evaluate BERTopic and Top2Vec for individual interview summarization by using similar preprocessing, chunking, and clustering configurations to ensure a fair comparison on Keyword Extraction. LLMs (GPT4) are then used for the next step topic labeling. Their outputs for a single interview (I0) are rated through a small-scale human evaluation, focusing on {coherence}, {clarity}, and {relevance}. Based on the preliminary results and evaluation, BERTopic shows stronger performance and is selected for further experimentation using three {clinically oriented embedding} models. We then analyzed the full interview collection with the best model setting. Results show that domain-specific embeddings improved topic \textit{precision} and \textit{interpretability}, with BioClinicalBERT producing the most consistent results across transcripts. The global analysis of the full dataset of 13 interviews, using the BioClinicalBERT embedding model, reveals the most dominant topics throughout all 13 interviews, namely ``Coordination and Communication in Cancer Care Management" and ``Patient Decision-Making in Cancer Treatment Journey''. Although the interviews are machine translations from Dutch to English, and clinical professionals are not involved in this evaluation, the findings suggest that neural topic modeling, particularly BERTopic, can help provide useful feedback to clinicians from patient interviews. This pipeline could support more efficient document navigation and strengthen the role of patients' voices in healthcare workflows.

</details>


### [42] [Tolerance Principle and Small Language Model Learning](https://arxiv.org/abs/2601.12179)
*Adam E. Friedman,Stevan Harnad,Rushen Shi*

Main category: cs.CL

TL;DR: 该研究测试了BabyBERTa模型在人工语法学习中的表现，发现其学习动态与人类婴儿不同，不遵循Yang的容忍原则。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索Transformer语言模型在有限训练数据下学习抽象语法规则的能力，并与人类婴儿的快速语法学习能力进行比较。人类婴儿能从少量示例中学习抽象语法规则，即使存在例外情况，而现代语言模型需要大量训练数据。Yang的容忍原则为规则可学习性提供了精确阈值，本研究旨在测试语言模型是否遵循这一原则。

Method: 使用BabyBERTa（专为小数据集优化的Transformer模型）在人工语法上进行训练。训练集在以下维度变化：1) 数据集大小；2) 独特句子类型数量；3) 规则遵循示例与例外示例的比例。通过控制这些变量来测试模型在不同条件下的语法规则泛化能力。

Result: 研究发现BabyBERTa的学习动态与人类婴儿不同，不遵循Yang的容忍原则。模型在语法规则学习方面表现出与人类婴儿不同的模式，未能复制人类婴儿从少量示例中学习抽象语法规则的能力。

Conclusion: Transformer语言模型在有限数据下的语法学习机制与人类婴儿不同，不遵循容忍原则。这表明当前语言模型的学习机制与人类语言习得存在本质差异，需要进一步研究来理解这些差异并改进模型的小样本学习能力。

Abstract: Modern language models like GPT-3, BERT, and LLaMA require massive training data, yet with sufficient training they reliably learn to distinguish grammatical from ungrammatical sentences. Children aged as young as 14 months already have the capacity to learn abstract grammar rules from very few exemplars, even in the presence of non-rule-following exceptions. Yang's (2016) Tolerance Principle defines a precise threshold for how many exceptions a rule can tolerate and still be learnable. The present study explored the minimal amount and quality of training data necessary for rules to be generalized by a transformer-based language model to test the predictions of the Tolerance Principle. We trained BabyBERTa (Huebner et al. 2021), a transformer model optimized for small datasets, on artificial grammars. The training sets varied in size, number of unique sentence types, and proportion of rule-following versus exception exemplars. We found that, unlike human infants, BabyBERTa's learning dynamics do not align with the Tolerance Principle.

</details>


### [43] [CTC-DID: CTC-Based Arabic dialect identification for streaming applications](https://arxiv.org/abs/2601.12199)
*Muhammad Umar Farooq,Oscar Saz*

Main category: cs.CL

TL;DR: 提出基于CTC损失的方言识别方法，将方言识别任务建模为有限词汇的ASR系统，在低资源阿拉伯方言识别任务中表现优于微调的Whisper和ECAPA-TDNN模型


<details>
  <summary>Details</summary>
Motivation: 传统方言识别方法在处理低资源场景和短语音时存在局限性，需要一种更鲁棒且适用于实时应用的解决方案。CTC损失在ASR中的成功应用为方言识别提供了新的思路。

Method: 提出CTC-DID方法，将方言识别任务框架化为有限词汇的ASR系统，将方言标签视为语音序列的标签序列。训练时使用语言无关启发式方法或预训练ASR模型估计方言标签在转录中的重复次数。基于自监督学习构建模型。

Result: 在低资源阿拉伯方言识别任务中，CTC-DID模型在有限数据集上训练后，性能优于微调的Whisper和ECAPA-TDNN模型。在Casablanca数据集上的零样本评估中也表现更优，对短语音更鲁棒，且易于适配流式实时应用。

Conclusion: CTC-DID方法为方言识别提供了一种有效的解决方案，特别适用于低资源场景和实时应用，展现了CTC损失框架在方言识别任务中的潜力。

Abstract: This paper proposes a Dialect Identification (DID) approach inspired by the Connectionist Temporal Classification (CTC) loss function as used in Automatic Speech Recognition (ASR). CTC-DID frames the dialect identification task as a limited-vocabulary ASR system, where dialect tags are treated as a sequence of labels for a given utterance. For training, the repetition of dialect tags in transcriptions is estimated either using a proposed Language-Agnostic Heuristic (LAH) approach or a pre-trained ASR model. The method is evaluated on the low-resource Arabic Dialect Identification (ADI) task, with experimental results demonstrating that an SSL-based CTC-DID model, trained on a limited dataset, outperforms both fine-tuned Whisper and ECAPA-TDNN models. Notably, CTC-DID also surpasses these models in zero-shot evaluation on the Casablanca dataset. The proposed approach is found to be more robust to shorter utterances and is shown to be easily adaptable for streaming, real-time applications, with minimal performance degradation.

</details>


### [44] [CoReflect: Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement](https://arxiv.org/abs/2601.12208)
*Yunzhe Li,Richie Yueqi Feng,Tianxin Wei,Chin-Chia Hsu*

Main category: cs.CL

TL;DR: CoReflect：通过协同进化模拟和反思性评估准则优化，实现对话系统多轮评估的自适应迭代方法


<details>
  <summary>Details</summary>
Motivation: 传统对话系统评估方法依赖人工定义的评估准则和固定对话上下文，这种静态方法覆盖范围有限，无法捕捉对话模型多样化的涌现行为，难以适应快速发展的对话模型能力

Method: 提出CoReflect框架，将对话模拟和评估统一为自适应迭代过程：1）对话规划器生成结构化模板指导用户模拟器进行多样化目标导向对话；2）反思分析器处理对话，识别系统性行为模式并自动优化评估准则；3）通过协同进化循环将分析结果反馈给规划器更新对话模板

Result: 该方法最小化人工干预，提供可扩展的自优化方法，使评估协议能够适应对话模型的快速发展，测试用例复杂度和评估准则诊断精度同步提升

Conclusion: CoReflect通过协同进化模拟和反思性评估准则优化，解决了多轮对话系统评估的根本性挑战，为对话模型评估提供了自适应、可扩展的框架

Abstract: Evaluating conversational systems in multi-turn settings remains a fundamental challenge. Conventional pipelines typically rely on manually defined rubrics and fixed conversational context$-$a static approach that limits coverage and fails to capture the diverse, emergent behaviors of dialogue models. To address this, we introduce CoReflect (Conversational Evaluation via Co-Evolutionary Simulation and Reflective Rubric Refinement), which unifies dialogue simulation and evaluation into an adaptive, iterative process. CoReflect employs a conversation planner that generates structured templates to guide a user simulator through diverse, goal-directed dialogues. Subsequently, a reflective analyzer processes these dialogues to identify systematic behavioral patterns and automatically refine the evaluation rubrics. Crucially, the insights from the conversation analysis are fed back into the planner to update conversation templates for subsequent iterations. This co-evolution loop ensures that the complexity of test cases and the diagnostic precision of rubrics improve in tandem. By minimizing human intervention, CoReflect provides a scalable and self-refining methodology that allows evaluation protocols to adapt alongside the rapidly advancing capabilities of dialogue models.

</details>


### [45] [Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models](https://arxiv.org/abs/2601.12247)
*Miao Li,Hanyang Jiang,Sikai Chen,Hengyu Fu,Yuhang Cai,Baihe Huang,Tinghan Ye,Xuanzhou Chen,Pascal Van Hentenryck*

Main category: cs.CL

TL;DR: PVF是一种无需训练的文本生成范式，通过规划-验证-填充机制，在扩散语言模型中实现高效并行解码，减少65%的函数评估次数。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型的解码策略往往是被动的，未能充分利用全局双向上下文来指导生成轨迹，导致效率低下。需要一种能主动规划全局结构的方法来提高解码效率。

Method: 提出Plan-Verify-Fill（PVF）范式：1）主动构建分层骨架，优先选择高影响力的语义锚点；2）采用验证协议实现实用结构停止，当进一步思考收益递减时停止；3）基于量化验证进行规划，无需额外训练。

Result: 在LLaDA-8B-Instruct和Dream-7B-Instruct上的评估显示，PVF相比基于置信度的并行解码，在基准数据集上减少了高达65%的函数评估次数（NFE），在保持准确性的同时实现了更高的效率。

Conclusion: PVF为扩散语言模型提供了一种有效的训练无关解码范式，通过主动规划和验证机制显著提升了生成效率，为DLMs的实际应用开辟了新途径。

Abstract: Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.

</details>


### [46] [Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers](https://arxiv.org/abs/2601.12263)
*Yixuan Du,Chenxiao Yu,Haoyan Xu,Ziyi Wang,Yue Zhao,Xiyang Hu*

Main category: cs.CL

TL;DR: 论文揭示视觉语言模型在商品搜索中的多模态排名攻击漏洞，提出MGEO框架通过联合优化图像扰动和文本后缀来不公平地提升目标产品排名


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在现代检索和推荐系统中广泛应用，但其在竞争性排名场景下对抗操纵的鲁棒性尚未充分探索。本文旨在揭示VLM在商品搜索中的多模态排名攻击漏洞

Method: 提出多模态生成引擎优化（MGEO）框架，采用交替梯度优化策略，联合优化不可感知的图像扰动和流畅的文本后缀，利用VLM内部的深度跨模态耦合

Result: 在真实世界数据集上使用最先进模型的实验表明，协调攻击显著优于仅文本和仅图像的基线方法，证实多模态协同可以被武器化以破坏搜索排名完整性

Conclusion: 多模态协同作为VLM的优势，可能被武器化以在不触发传统内容过滤器的情况下损害搜索排名的完整性，揭示了VLM在对抗性排名场景中的关键漏洞

Abstract: Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.

</details>


### [47] [Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models](https://arxiv.org/abs/2601.12269)
*Xucong Hu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 通过MCMC采样和退火技术，从基础语言模型中恢复强大的心理理论能力，无需额外训练或权重更新


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型通常被认为是下一个词预测器，主要优化表面合理性而非保持正确的潜在状态表示，因此被认为在心理理论任务上表现不佳。虽然后训练方法可以改进心理理论性能，但研究旨在直接从基础模型中恢复这种能力，无需额外权重更新或验证。

Method: 基于Karan & Du (2025)的power-sampling方法，使用马尔可夫链蒙特卡洛从自回归语言模型的序列级概率分布中采样。进一步引入退火技术，将温度分布从高温逐渐转移到低温，显著改进了固定温度power sampling的心理理论性能。

Result: 研究表明，通过采样优化方法可以从语言模型中提取潜在能力而无需重新训练。退火技术的加入显著提升了心理理论任务的表现，超过了固定温度的power sampling方法。

Conclusion: 基于采样的优化方法为从语言模型中提取潜在能力提供了一种强大途径，无需重新训练。这挑战了自回归语言模型无法有效处理心理理论任务的传统观点，表明通过适当的采样策略可以恢复模型的深层推理能力。

Abstract: Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.

</details>


### [48] [Conversational Context Classification: A Representation Engineering Approach](https://arxiv.org/abs/2601.12286)
*Jonathan Pan*

Main category: cs.CL

TL;DR: 该论文探索使用表示工程和一类支持向量机在LLM内部状态中识别特定上下文子空间，以检测对话中的离题响应


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，需要有效保障其操作安全，特别是检测其生成离题响应（话题转移、事实错误或幻觉）的能力。传统异常检测方法难以直接应用于上下文语义分析。

Method: 采用表示工程和一类支持向量机方法，在LLM内部状态中识别与特定上下文相关的子空间。通过训练OCSVM建立隐藏状态潜在空间的边界，并在Llama和Qwen模型上进行评估，识别与感兴趣上下文强关联的最优层。

Result: 评估结果显示，该方法在识别特定上下文子空间方面表现出良好效果，能够有效检测对话线程是否在特定上下文范围内。

Conclusion: 该方法不仅可用于检测对话的上下文一致性，还为更好地解释大型语言模型的研究做出了贡献。

Abstract: The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.

</details>


### [49] [A Scalable Entity-Based Framework for Auditing Bias in LLMs](https://arxiv.org/abs/2601.12374)
*Akram Elbouanani,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 论文提出了一个使用命名实体作为探针的可扩展偏见审计框架，通过合成数据重现自然文本中的偏见模式，进行了迄今为止最大的偏见审计（19亿数据点），揭示了LLMs中的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型偏见评估方法在生态效度和统计控制之间存在权衡：人工提示不能反映真实使用场景，而自然任务缺乏规模和严谨性。需要一种既能保持生态效度又能进行大规模分析的偏见审计方法。

Method: 引入基于命名实体的可扩展偏见审计框架，使用合成数据可靠地重现自然文本中的偏见模式。通过实体作为探针，测量模型行为中的结构性差异，涵盖多种实体类型、任务、语言、模型和提示策略。

Result: 进行了迄今为止最大的偏见审计（19亿数据点），发现系统性偏见：模型惩罚右翼政客、偏爱左翼政客、偏好西方和富裕国家而非全球南方、偏爱西方公司、惩罚国防和制药行业公司。指令微调减少偏见，但模型规模增大会放大偏见，中文或俄文提示不会减弱西方偏好。

Conclusion: 大语言模型在高风险应用部署前应进行严格审计，因为存在系统性偏见，且模型规模增大会放大偏见，语言提示策略不能有效减轻西方中心主义偏好。

Abstract: Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.

</details>


### [50] [LR-DWM: Efficient Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.12376)
*Ofek Raban,Ethan Fetaya,Gal Chechik*

Main category: cs.CL

TL;DR: 本文提出了一种名为LR-DWM的水印方法，专门针对扩散语言模型，通过利用左右邻居token来嵌入水印信号，实现了高效的水印嵌入和检测。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法主要针对自回归语言模型，依赖token的顺序生成。扩散语言模型采用非顺序迭代去噪生成文本，需要大幅修改现有水印方法。最近的工作通过反转过程实现水印，但存在显著的计算或内存开销。

Method: 提出左右扩散水印方法，当左右邻居token可用时，基于这两个邻居来偏置生成的token。该方法在运行时和内存开销上接近未加水印的基线扩散语言模型。

Result: LR-DWM在标准评估设置下实现了可靠的统计检测，同时保持了接近未加水印基线模型的运行效率和内存使用。实验结果表明扩散语言模型可以高效地加水印，实现高可检测性且计算和内存开销可忽略。

Conclusion: 扩散语言模型可以通过LR-DWM方法高效地添加水印，该方法利用左右邻居token嵌入水印信号，在保持模型性能的同时实现了可靠的水印检测。

Abstract: Watermarking (WM) is a critical mechanism for detecting and attributing AI-generated content. Current WM methods for Large Language Models (LLMs) are predominantly tailored for autoregressive (AR) models: They rely on tokens being generated sequentially, and embed stable signals within the generated sequence based on the previously sampled text. Diffusion Language Models (DLMs) generate text via non-sequential iterative denoising, which requires significant modification to use WM methods designed for AR models. Recent work proposed to watermark DLMs by inverting the process when needed, but suffers significant computational or memory overhead. We introduce Left-Right Diffusion Watermarking (LR-DWM), a scheme that biases the generated token based on both left and right neighbors, when they are available. LR-DWM incurs minimal runtime and memory overhead, remaining close to the non-watermarked baseline DLM while enabling reliable statistical detection under standard evaluation settings. Our results demonstrate that DLMs can be watermarked efficiently, achieving high detectability with negligible computational and memory overhead.

</details>


### [51] [NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages](https://arxiv.org/abs/2601.12389)
*Lakshya Tomar,Vinayak Abrol,Puneet Agarwal*

Main category: cs.CL

TL;DR: NADIR是一种新型非自回归架构，针对多语言音译等局部依赖任务，在保持准确性的同时实现13倍加速，平衡了自回归模型的高延迟和非自回归模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 许多序列到序列任务（如多语言音译、代码重构、语法修正等）主要依赖局部依赖关系，自回归模型虽然准确但推理延迟高，而非自回归模型速度快但存在幻觉和长度控制问题。需要在准确性和速度之间找到平衡。

Method: 提出NADIR架构，结合差分Transformer和专家混合机制，能够建模复杂字符映射而无需序列依赖。该架构专门针对多语言音译任务设计，特别是印度语言。

Result: 相比最先进的自回归基线，NADIR实现超过13倍加速。字符错误率为15.78%（自回归模型为14.44%，标准非自回归模型为21.88%）。显著减少各类错误：重复错误减少49.53%，替换错误减少24.45%，省略错误减少32.92%，插入错误减少16.87%。

Conclusion: NADIR为构建快速可靠的非自回归系统提供了实用蓝图，有效弥合了自回归模型的准确性与实时大规模部署需求之间的差距，特别适用于依赖局部关系的序列任务。

Abstract: In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.

</details>


### [52] [Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification](https://arxiv.org/abs/2601.12419)
*Mahammad Namazov,Tomáš Koref,Ivan Habernal*

Main category: cs.CL

TL;DR: 比较分析框架评估法律领域LLM可解释性技术，发现模型预测理由与法律专家存在显著差异


<details>
  <summary>Details</summary>
Motivation: 法律领域应用大型语言模型需要信任和透明度，但现有可解释性技术中哪种最适合法律结果预测仍不明确

Method: 提出模型无关的可解释性技术比较分析框架，采用两种理由提取方法，通过忠实度（标准化充分性和全面性指标）和合理性（法律专家评估）进行评估

Result: 模型预测违规的理由与法律专家的理由存在显著差异，尽管定量分析结果和下游分类性能表现良好

Conclusion: 需要更深入理解法律领域LLM的可解释性，模型预测理由与专家判断存在差异，LLM-as-a-Judge方法的可行性需要进一步评估

Abstract: Interpretability is critical for applications of large language models in the legal domain which requires trust and transparency. While some studies develop task-specific approaches, other use the classification model's parameters to explain the decisions. However, which technique explains the legal outcome prediction best remains an open question. To address this challenge, we propose a comparative analysis framework for model-agnostic interpretability techniques. Among these, we employ two rationale extraction methods, which justify outcomes with human-interpretable and concise text fragments (i.e., rationales) from the given input text. We conduct comparison by evaluating faithfulness-via normalized sufficiency and comprehensiveness metrics along with plausibility-by asking legal experts to evaluate extracted rationales. We further assess the feasibility of LLM-as-a-Judge using legal expert evaluation results. We show that the model's "reasons" for predicting a violation differ substantially from those of legal experts, despite highly promising quantitative analysis results and reasonable downstream classification performance. The source code of our experiments is publicly available at https://github.com/trusthlt/IntEval.

</details>


### [53] [System-Mediated Attention Imbalances Make Vision-Language Models Say Yes](https://arxiv.org/abs/2601.12430)
*Tsan Tsai Chan,Varsha Suresh,Anisha Saha,Michael Hahn,Vera Demberg*

Main category: cs.CL

TL;DR: 该研究提出系统注意力失衡是VLM幻觉的关键因素，通过因果性重新分配系统注意力到图像和文本输入，有效抑制了yes-bias幻觉


<details>
  <summary>Details</summary>
Motivation: 现有缓解VLM幻觉的方法通常偏向图像中心解释，过度强调增加图像注意力而忽视其他模态的作用。本研究旨在评估更全面的系统介导解释，将注意力失衡归因于功能冗余的系统权重减少了图像和文本输入的注意力

Method: 采用系统介导的解释框架，通过因果性重新分配注意力从系统模态到图像和文本输入，评估这种方法对yes-bias（VLM常见幻觉形式）的抑制效果

Result: 因果性重新分配系统注意力到图像和文本输入显著抑制了yes-bias，通常优于现有方法。证据表明系统介导的注意力失衡通过鼓励对粗略输入表示的默认依赖而导致yes-bias

Conclusion: 系统注意力是VLM幻觉的关键因素，并具有作为缓解杠杆的潜力。系统介导的注意力失衡导致对粗略表示的依赖，这在某些任务中有效但不适合其他任务

Abstract: Vision-language model (VLM) hallucination is commonly linked to imbalanced allocation of attention across input modalities: system, image and text. However, existing mitigation strategies tend towards an image-centric interpretation of these imbalances, often prioritising increased image attention while giving less consideration to the roles of the other modalities. In this study, we evaluate a more holistic, system-mediated account, which attributes these imbalances to functionally redundant system weights that reduce attention to image and textual inputs. We show that this framework offers a useful empirical perspective on the yes-bias, a common form of hallucination in which VLMs indiscriminately respond 'yes'. Causally redistributing attention from the system modality to image and textual inputs substantially suppresses this bias, often outperforming existing approaches. We further present evidence suggesting that system-mediated attention imbalances contribute to the yes-bias by encouraging a default reliance on coarse input representations, which are effective for some tasks but ill-suited to others. Taken together, these findings firmly establish system attention as a key factor in VLM hallucination and highlight its potential as a lever for mitigation.

</details>


### [54] [Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping](https://arxiv.org/abs/2601.12465)
*Miao Peng,Weizhou Shen,Nuo Chen,Chenliang Li,Ming Yan,Jia Li*

Main category: cs.CL

TL;DR: 提出DeepReasonQA框架和LongPAS方法，解决长上下文推理中RLVR性能下降问题，通过知识图谱驱动合成高难度多跳QA对，并进行细粒度信用分配，显著提升长上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR在短上下文推理中有效，但在长上下文场景中性能下降，存在"almost-there"现象（轨迹大部分正确但最后一步失败）。这归因于：1）长上下文QA数据缺乏高推理密度，无法推动LLM超越简单定位进行复杂多跳推理；2）长上下文RL训练中，对部分正确但结果错误的轨迹进行无差别惩罚，导致有价值学习信号丢失。

Method: 提出DeepReasonQA：基于知识图谱的可控合成框架，构建具有内在推理链的高难度多跳长上下文QA对。提出LongPAS（Long-context Process Advantage Shaping）：通过有效性和相关性两个维度评估推理步骤，进行细粒度信用分配，从"almost-there"轨迹中捕获关键学习信号。

Result: 在三个长上下文推理基准测试中，该方法显著优于RLVR基线，与前沿LLM性能相当但使用更少参数。进一步分析证实了方法在增强长上下文推理能力的同时保持RL训练稳定性的有效性。

Conclusion: 通过DeepReasonQA提供高质量训练数据和LongPAS进行细粒度信用分配，成功解决了长上下文推理中RLVR的性能瓶颈，实现了对"almost-there"现象的有效处理，显著提升了LLM的长上下文推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the "almost-there" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from "almost-there" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.

</details>


### [55] [Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty](https://arxiv.org/abs/2601.12471)
*Sravanthi Machcha,Sushrita Yerra,Sahil Gupta,Aishwarya Sahoo,Sharmin Sultana,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: MedAbstain：用于医学多选题回答中弃权能力的统一基准，发现即使高精度LLM也常无法在不确定时弃权，提供显式弃权选项比输入扰动更有效提升安全性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估过于强调准确性，但在现实世界和安全关键应用中，模型在不确定时能够弃权的能力对于可信部署同样重要。医学多选题回答作为离散选择场景，可推广到智能体行动选择，需要系统评估LLM的弃权能力。

Method: 提出MedAbstain基准和评估协议，整合了共形预测、对抗性问题扰动和显式弃权选项。系统评估了开源和闭源LLM，分析模型在不确定时弃权的能力。

Result: 即使最先进的高精度LLM也经常无法在不确定时弃权。提供显式弃权选项能持续增加模型不确定性和更安全的弃权行为，效果远超过输入扰动。扩大模型规模或使用高级提示技术带来的改善有限。

Conclusion: 弃权机制对于可信LLM部署具有核心作用，为高风险应用中的安全性改进提供了实用指导。研究强调了在LLM评估中超越准确性、重视弃权能力的重要性。

Abstract: Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.

</details>


### [56] [Capability-Aware Early-Stage Research Idea Evaluation](https://arxiv.org/abs/2601.12473)
*Renlong Jie,Chen Chu,Zhen Wang*

Main category: cs.CL

TL;DR: 提出基于作者信息和研究想法的能力感知框架，预测论文接受率和评分，无需完整文本或实验结果


<details>
  <summary>Details</summary>
Motivation: 在概念阶段（投入大量资源前）预测研究想法的结果，对于优化科学资源分配和研究规划具有重要意义。现有方法主要依赖完整稿件或同行评审，而本研究旨在仅使用作者信息和研究想法进行预测

Method: 提出能力感知框架，通过三路transformer架构整合作者信息、（推断的）能力呈现和研究想法，采用灵活融合机制。引入两阶段架构学习给定作者信息和想法的能力表示

Result: 实验表明，该方法显著优于基于bert-base和bert-large微调的单路模型，能力预测显著提高了最终模型的预测准确性

Conclusion: 所提方法可应用于早期研究结果预测和科学资源分配，为研究规划提供有价值的工具

Abstract: Predicting the outcomes of research ideas at their conceptual stage (i.e. before significant resources are committed) holds great potential for optimizing scientific resource allocation and research planning. While existing methods rely heavily on finished manuscripts or peer reviews, we propose a novel capability-aware framework that predicts paper acceptance and ratings using only author information and research ideas, without requiring full text or experimental results. Our approach integrates author information, (inferred) capability presentation, and research ideas through a three-way transformer architecture with flexible fusion mechanisms. We also introduce a two-stage architecture for learning the capability representation given the author information and idea. Experiments show that our method significantly outperform the single-way models by finetuning bert-base and bert-large, and the capability predicting significantly increase the predictive accuracy of the final model. The proposed method can be applied in both early-stage research outcome prediction and scientific resource allocation.

</details>


### [57] [DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity](https://arxiv.org/abs/2601.12505)
*Ashish Raj Shekhar,Shiven Agarwal,Priyanuj Bordoloi,Yash Shah,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: DoPE是一种文档层防御框架，通过在PDF/HTML考试文档中嵌入语义诱饵来防止和检测MLLM作弊，利用渲染-解析差异实现模型无关的防御。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLM)能够直接处理考试文档，威胁传统评估和学术诚信。现有防御方法依赖一次性分类器，缺乏有效的文档层防御机制。

Method: 提出DoPE框架，包含FewSoRT-Q生成问题级语义诱饵，FewSoRT-D将其封装到水印文档中。利用渲染-解析差异，在作者阶段对考试文档进行检测和预防。

Result: 在Integrity-Bench基准(1826个考试)上评估：对OpenAI和Anthropic的黑盒MLLM，检测率91.4%(假阳性率8.7%)，96.3%的尝试被阻止或诱导诱饵对齐失败。

Conclusion: DoPE提供有效的文档层防御，无需依赖传统一次性分类器。释放基准、工具包和评估代码，促进学术诚信文档层防御的可重复研究。

Abstract: Multimodal Large Language Models (MLLMs) can directly consume exam documents, threatening conventional assessments and academic integrity. We present DoPE (Decoy-Oriented Perturbation Encapsulation), a document-layer defense framework that embeds semantic decoys into PDF/HTML assessments to exploit render-parse discrepancies in MLLM pipelines. By instrumenting exams at authoring time, DoPE provides model-agnostic prevention (stop or confound automated solving) and detection (flag blind AI reliance) without relying on conventional one-shot classifiers. We formalize prevention and detection tasks, and introduce FewSoRT-Q, an LLM-guided pipeline that generates question-level semantic decoys and FewSoRT-D to encapsulate them into watermarked documents. We evaluate on Integrity-Bench, a novel benchmark of 1826 exams (PDF+HTML) derived from public QA datasets and OpenCourseWare. Against black-box MLLMs from OpenAI and Anthropic, DoPE yields strong empirical gains: a 91.4% detection rate at an 8.7% false-positive rate using an LLM-as-Judge verifier, and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. We release Integrity-Bench, our toolkit, and evaluation code to enable reproducible study of document-layer defenses for academic integrity.

</details>


### [58] [Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning](https://arxiv.org/abs/2601.12535)
*Ahmed Attia,Alham Fikri*

Main category: cs.CL

TL;DR: 该论文提出了一种基于自监督强化学习的低资源机器翻译微调方法，使用NLLB模型通过往返翻译进行引导，在多种低资源语言上取得了翻译质量的提升。


<details>
  <summary>Details</summary>
Motivation: 随着低资源语言平行数据的收集，低资源机器翻译受到越来越多的关注，但许多潜在的改进方法仍有待探索。研究者希望开发一种能够有效利用现有预训练模型知识并持续自我改进的低资源翻译方法。

Method: 采用自监督强化学习微调方法，使用NLLB模型家族进行往返翻译引导：先将英语翻译成目标低资源语言，再翻译回英语。使用chrF++和BLEU的组合作为重建英语句子的奖励函数。在NLLB-MD数据集上评估了600M和1.3B参数的NLLB模型。

Result: 在Central Aymara、Friulian、Wolof和Russian等语言上观察到了一致的改进。对翻译输出的定性检查表明，翻译的流畅性和语义保真度都有所提高。该方法显示出随着模型规模增大而进一步获益的潜力。

Conclusion: 提出的自监督强化学习微调方法能够有效提升低资源机器翻译质量，并且该方法可以从模型规模扩展中进一步获益，使模型能够更好地利用预训练知识并持续自我改进。

Abstract: Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.

</details>


### [59] [Benchmarking Concept-Spilling Across Languages in LLMs](https://arxiv.org/abs/2601.12549)
*Ilia Badanin,Daniil Dzenhaliou,Imanol Schlag*

Main category: cs.CL

TL;DR: 该论文提出了一种评估多语言大语言模型语义鲁棒性的比较框架，通过系统测量模型处理跨语言多义词的能力，揭示了语言溢出现象，并建立了模型性能的排序系统。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型虽然表现出显著的跨语言能力，但经常表现出对其他语言表示的系统性偏见，导致在生成非英语内容时出现语义干扰，这种现象被定义为"语言溢出"。需要一种系统方法来评估多语言语义鲁棒性。

Method: 提出了一种新颖的比较评估框架，通过结构化意义生成任务系统测量模型处理多义词的能力。使用100个高多义性英语单词构建基准，在9种语言中评估开放和闭源多语言LLM。该方法提供相对性能度量：当需要生成恰好五个意义时，模型会从主导语言中借用意义，但语义更强的模型在生成序列后期才这样做。

Result: 研究发现不同模型和语言之间存在显著的语义鲁棒性差异。语义更强的模型在失败前能从目标语言产生更多真实意义，而较弱的模型在序列早期就转向主导语言意义。这提供了一个无需确定错误源因果归因的原则性模型排序系统。

Conclusion: 该研究为多语言语义评估贡献了一个可扩展的比较基准和严格的验证流程，这些是开发更语言平衡的AI系统的关键工具。该方法能够系统评估和比较多语言LLM的语义鲁棒性。

Abstract: Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.

</details>


### [60] [Evaluating Contextually Mediated Factual Recall in Multilingual Large Language Models](https://arxiv.org/abs/2601.12555)
*Yihong Liu,Bingyu Xiong,Hinrich Schütze*

Main category: cs.CL

TL;DR: 该研究探讨了LLMs在上下文中介下的多语言事实回忆能力，发现上下文中介会降低事实回忆效果，大模型对此更稳健，而真实姓名的影响不系统。


<details>
  <summary>Details</summary>
Motivation: 现有事实回忆评估主要评估孤立的事实检索，但在自然语言使用中，事实通常通过上下文间接访问。需要研究LLMs在目标实体嵌入自然语境而非显式查询时的多语言事实回忆能力。

Method: 构建受控提示，在保留底层事实的同时通过上下文句子引入指称中介。使用合成姓名和真实姓名跨语言比较，以区分上下文效应与姓名特定关联。在五种语言中评估多个模型家族。

Result: 上下文中介持续降低事实回忆效果，不同关系间存在显著差异。更大模型对上下文中介更稳健，相对于直接查询的性能差距减小。真实姓名和姓名来源的影响混合且不系统。

Conclusion: 研究发现多语言LLMs在孤立事实回忆和上下文依赖语言理解之间存在差距，上下文中介会损害事实回忆，尽管大模型对此更稳健。

Abstract: Large language models (LLMs) can recall a wide range of factual knowledge across languages. However, existing factual recall evaluations primarily assess fact retrieval in isolation, where the queried entity is explicitly named and the fact is requested directly. In natural language use, facts are often accessed through context, where the relevant entity is introduced only indirectly. In this work, we study contextually mediated factual recall, asking whether LLMs can reliably retrieve factual knowledge when the target entity is embedded in a naturalistic context rather than queried explicitly, across languages. We construct controlled prompts that preserve the underlying fact while introducing referential mediation through contextual sentences. To disentangle contextual effects from name-specific associations, we further compare performance using synthetic names and real names across languages. Evaluating multiple model families in five languages, we find that contextual mediation consistently degrades factual recall, with substantial variation across relations. Larger models are more robust to contextual mediation, exhibiting a reduced performance gap relative to direct queries, while the effect of real names and name origin is mixed and unsystematic. These findings highlight a gap between isolated factual recall and context-dependent language understanding in multilingual LLMs.

</details>


### [61] [Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems](https://arxiv.org/abs/2601.12618)
*Elham Tajik,Conrad Borchers,Bahar Shahrokhian,Sebastian Simon,Ali Keramati,Sonika Pal,Sreecharan Sankaranarayanan*

Main category: cs.CL

TL;DR: 该研究提出利用LLM多智能体系统的推理轨迹作为过程数据，通过余弦相似度量化智能体间的分歧，将其转化为有意义的分析信号，以增强质性编码的解释实践和方法严谨性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，全自动和人机协作的工作流程成为学习分析中质性数据分析的有前景方法，但指导此类工作流程的方法学标准仍然有限。研究者需要新的方法来增强质性编码的解释实践和方法严谨性。

Method: 提出将LLM多智能体系统生成的推理轨迹作为过程数据，应用余弦相似度来系统检测、量化和解释智能体间的分歧。分析了近10,000个人类辅导对话片段编码的智能体对，将分歧重新定义为有意义的分析信号。

Result: LLM智能体的语义推理相似度能够稳健地区分共识与分歧，并与人类编码可靠性相关。基于该指标的质性分析揭示了编码内的细微教学子功能以及概念编码本细化的机会。

Conclusion: 通过整合量化相似度指标与质性审查，该方法有潜力通过揭示解释模糊性来改进和加速编码过程中评估者间信度的建立，特别是在LLM与人类协作时。推理轨迹分歧代表了推进教育研究方法严谨性和解释深度的有价值新型分析信号。

Abstract: Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents' semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.

</details>


### [62] [Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift](https://arxiv.org/abs/2601.12639)
*Daniel Vennemeyer,Punya Syon Pandey,Phan Anh Duong,Michael Umeokoli,Samuel Ratnam*

Main category: cs.CL

TL;DR: 不同微调目标在安全性与能力权衡上存在系统性差异：小规模训练时安全性相似但能力不同，大规模训练时监督和偏好优化会显著降低对抗鲁棒性，而ORPO和KL正则化能有效缓解这一问题


<details>
  <summary>Details</summary>
Motivation: 尽管在良性数据上微调LLMs仍可能损害对齐性和对抗鲁棒性，但微调目标在塑造这些安全结果中的具体作用尚未得到充分研究。本文旨在系统比较不同微调目标如何影响模型的安全性与能力权衡

Method: 在控制数据、领域、架构和优化的条件下，对六种微调目标进行对比：监督微调、直接偏好优化、条件微调、接种提示、几率比偏好优化和KL正则化微调。实验涵盖封闭式推理和开放式生成任务

Result: 微调目标选择导致安全-能力前沿的系统性、规模依赖性变化。小训练预算下各目标鲁棒性相似但能力不同；大预算下目标差异显著：监督和偏好优化使能力提升与对抗脆弱性增加紧密耦合，而ORPO和KL正则化能大幅缓解两者

Conclusion: 微调目标在小规模训练中对安全性影响不大，但随着训练规模增加，成为对抗鲁棒性和潜在角色稳定性的主要驱动因素。约束学习信号的目标（特别是ORPO和KL正则化）能有效平衡安全性与能力

Abstract: Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.

</details>


### [63] [Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?](https://arxiv.org/abs/2601.12648)
*Nafiz Imtiaz Khan,Kylie Cleland,Vladimir Filkov,Roger Eric Goldman*

Main category: cs.CL

TL;DR: LLMs can effectively自动化介入放射学程序病例日志记录，直接从自由文本报告中提取结构化信息，显著减少学员的文书负担并提高一致性。


<details>
  <summary>Details</summary>
Motivation: 程序病例日志是放射学培训的核心要求，但手动完成耗时且容易不一致。需要自动化解决方案来减轻学员的文书负担并提高日志记录的一致性。

Method: 评估多个本地和商业LLM在指令提示和思维链提示下，从414份由9名住院医师在2018-2024年间撰写的介入放射学报告中提取结构化程序信息。使用敏感性、特异性、F1分数以及推理延迟和代币效率来评估模型性能。

Result: 本地和商业模型都实现了强大的提取性能，最佳F1分数接近0.87。不同模型在速度和成本之间存在权衡。自动化有潜力显著减少学员的文书负担并提高病例日志的一致性。

Conclusion: LLMs在医学教育中辅助文档记录的可行性得到证实。研究结果支持AI在医疗培训中的实际应用，但需要在不同机构和临床工作流程中进行进一步验证。

Abstract: Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.

</details>


### [64] [Augmenting Question Answering with A Hybrid RAG Approach](https://arxiv.org/abs/2601.12658)
*Tianyi Yang,Nashrah Haque,Vaishnave Jonnalagadda,Yuya Jeremy Ong,Zhehui Chen,Yanzhao Wu,Lei Yu,Divyesh Jadav,Wenqi Wei*

Main category: cs.CL

TL;DR: SSRAG是一种混合架构，通过集成查询增强、智能路由和结构化检索机制来提升问答质量，相比标准RAG在多个数据集上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在检索上下文相关信息方面存在困难，导致答案不完整或次优，需要改进检索过程以提升问答质量。

Method: 提出结构化语义RAG（SSRAG）混合架构，整合查询增强、智能路由和结构化检索机制，结合向量和图检索技术并进行上下文统一。

Result: 在TruthfulQA、SQuAD和WikiQA三个QA数据集上，对五个大型语言模型进行广泛评估，证明该方法相比标准RAG能持续提升响应质量。

Conclusion: SSRAG通过改进检索过程和增强上下文基础，提高了答案准确性和信息丰富度，为问答任务提供了更有效的RAG解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.

</details>


### [65] [UbuntuGuard: A Culturally-Grounded Policy Benchmark for Equitable AI Safety in African Languages](https://arxiv.org/abs/2601.12696)
*Tassallah Abdullahi,Macton Mgonzo,Mardiyyah Oduwole,Paul Okewunmi,Abraham Owodunni,Ritambhara Singh,Carsten Eickhoff*

Main category: cs.CL

TL;DR: UbuntuGuard：首个基于政策的非洲语言安全基准，通过领域专家构建对抗性查询，评估13个模型在非洲低资源语言上的安全性能，发现现有英语基准高估多语言安全性，跨语言迁移不足，动态模型仍难以完全本地化非洲语境。


<details>
  <summary>Details</summary>
Motivation: 当前监护人模型主要面向西方中心主义和高资源语言，导致非洲低资源语言面临演化危害、跨语言安全失效和文化错位问题。现有安全分类僵化，无法适应多样化的语言和社会文化背景，需要灵活、运行时可执行的政策和基准来反映本地规范、危害场景和文化期望。

Method: 引入UbuntuGuard基准：1）由155名敏感领域（包括医疗保健）的领域专家编写对抗性查询；2）从这些专家构建的查询中推导出上下文特定的安全政策和参考响应，捕捉文化基础的风险信号；3）评估13个模型，包括6个通用LLM和7个监护人模型（静态、动态和多语言三种变体）。

Result: 评估发现：1）现有英语中心基准高估了真实世界的多语言安全性；2）跨语言迁移提供部分但不充分的覆盖；3）动态模型虽然在推理时能更好地利用政策，但仍难以完全本地化非洲语言语境。这些发现凸显了开发多语言、文化基础安全基准的紧迫性。

Conclusion: 需要多语言、文化基础的安全基准来支持低资源语言可靠且公平的监护人模型开发。UbuntuGuard作为首个非洲政策安全基准，为评估模型在非洲语言环境中的安全性能提供了重要工具，揭示了当前模型在文化适应性和本地化方面的不足。

Abstract: Current guardian models are predominantly Western-centric and optimized for high-resource languages, leaving low-resource African languages vulnerable to evolving harms, cross-lingual safety failures, and cultural misalignment. Moreover, most guardian models rely on rigid, predefined safety categories that fail to generalize across diverse linguistic and sociocultural contexts. Robust safety, therefore, requires flexible, runtime-enforceable policies and benchmarks that reflect local norms, harm scenarios, and cultural expectations. We introduce UbuntuGuard, the first African policy-based safety benchmark built from adversarial queries authored by 155 domain experts across sensitive fields, including healthcare. From these expert-crafted queries, we derive context-specific safety policies and reference responses that capture culturally grounded risk signals, enabling policy-aligned evaluation of guardian models. We evaluate 13 models, comprising six general-purpose LLMs and seven guardian models across three distinct variants: static, dynamic, and multilingual. Our findings reveal that existing English-centric benchmarks overestimate real-world multilingual safety, cross-lingual transfer provides partial but insufficient coverage, and dynamic models, while better equipped to leverage policies at inference time, still struggle to fully localize African-language contexts. These findings highlight the urgent need for multilingual, culturally grounded safety benchmarks to enable the development of reliable and equitable guardian models for low-resource languages. Our code can be found online.\footnote{Code repository available at https://github.com/hemhemoh/UbuntuGuard.

</details>


### [66] [A Shared Geometry of Difficulty in Multilingual Language Models](https://arxiv.org/abs/2601.12731)
*Stefano Civelli,Pietro Bernardelle,Nicolò Brunello,Gianluca Demartini*

Main category: cs.CL

TL;DR: LLMs在问题难度预测中表现出两阶段表示过程：浅层表示形成语言无关的难度信号，深层表示则转化为语言特定的难度评估


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中问题难度预测的多语言几何特性，探索难度信号在不同语言间的泛化能力

Method: 使用Easy2Hard基准的AMC子集，翻译成21种语言，在LLM内部表示上训练线性探针，分析浅层（早期层）和深层（后期层）表示的功能差异

Result: 深层表示探针在相同语言内准确率高但跨语言泛化差；浅层表示探针在相同语言内性能较低但跨语言泛化显著更好

Conclusion: LLMs首先形成语言无关的问题难度表示，随后转化为语言特定的难度评估，这与LLM解释性研究中发现的抽象概念空间先于语言特定输出的模式一致

Abstract: Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.

</details>


### [67] [Towards Robust Process Reward Modeling via Noise-aware Learning](https://arxiv.org/abs/2601.12748)
*Bin Xie,Bingbing Xu,Xueyun Tian,Yilin Chen,Huawei Shen*

Main category: cs.CL

TL;DR: 提出NAIT框架解决过程奖励模型中的噪声监督问题，通过标签修正和噪声感知迭代训练提升步骤正确性判别能力


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型依赖昂贵的过程级监督，而蒙特卡洛估计方法产生策略依赖的奖励，导致标签噪声（包括错误奖励正确步骤和惩罚正确步骤）

Method: 两阶段框架：1) 标签阶段引入反思感知的标签修正机制，使用LLM作为裁判检测与当前推理步骤相关的反思和自我纠正行为；2) 训练阶段提出噪声感知迭代训练框架，使PRM基于自身置信度逐步精炼噪声标签

Result: 方法显著提升步骤级正确性判别能力，在噪声监督训练的PRM上实现高达27%的绝对平均F1增益

Conclusion: 提出的NAIT框架有效缓解过程奖励模型中的噪声监督问题，通过结合反思感知标签修正和噪声感知迭代训练，显著提升推理步骤正确性评估的准确性

Abstract: Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \underline{\textbf{N}}oise-\underline{\textbf{A}}ware \underline{\textbf{I}}terative \underline{\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\% absolute gain in average F1 over PRMs trained with noisy supervision.

</details>


### [68] [VISPA: Pluralistic Alignment via Automatic Value Selection and Activation](https://arxiv.org/abs/2601.12758)
*Shenyan Zheng,Jiayou Zhong,Anudeex Shetty,Heng Ji,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: VISPA是一个无需训练的多元化对齐框架，通过动态选择和内部模型激活引导实现对价值表达的直接控制，使语言模型输出能够反映多样化的观点而非单一偏好。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在高风险领域应用增加，需要其输出反映多样化的人类观点而非单一平均偏好。现有方法存在价值范围有限或仅依赖提示级干预的问题，缺乏价值控制和代表性。

Method: VISPA采用无需训练的多元化对齐框架，通过动态选择和内部模型激活引导直接控制价值表达。该方法不依赖模型训练，而是通过内部激活机制实现价值导向。

Result: 在涵盖多个模型和评估设置的广泛实证研究中，VISPA在医疗保健及其他领域的各种多元化对齐模式中均表现优异。进一步分析表明VISPA能够适应不同的引导初始化、模型和/或价值观。

Conclusion: 多元化对齐可以通过内部激活机制实现，这为构建能够服务所有人的语言模型提供了一条可扩展的路径。

Abstract: As large language models are increasingly used in high-stakes domains, it is essential that their outputs reflect not average} human preference, rather range of varying perspectives. Achieving such pluralism, however, remains challenging. Existing approaches consider limited values or rely on prompt-level interventions, lacking value control and representation. To address this, we introduce VISPA, a training-free pluralistic alignment framework, that enables direct control over value expression by dynamic selection and internal model activation steering. Across extensive empirical studies spanning multiple models and evaluation settings, we show VISPA is performant across all pluralistic alignment modes in healthcare and beyond. Further analysis reveals VISPA is adaptable with different steering initiations, model, and/or values. These results suggest that pluralistic alignment can be achieved through internal activation mechanisms, offering a scalable path toward language models that serves all.

</details>


### [69] [Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?](https://arxiv.org/abs/2601.12812)
*Sushant Kumar Ray,Gautam Siddharth Kashyap,Sahil Tripathi,Nipun Joshi,Vijay Govindarajan,Rafiq Ali,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: MEDASSESS-X是一个面向临床问答部署的框架，通过推理时对齐而非监督微调来提升LLMs性能，解决了"专业化谬误"。


<details>
  <summary>Details</summary>
Motivation: 当前临床问答系统过度依赖领域特定微调，存在覆盖范围窄、重训练成本高、适应性有限等问题，且存在"专业化谬误"——认为专业医疗LLMs必然优于通用模型。

Method: 提出MEDASSESS-X框架，采用推理时对齐方法，使用轻量级引导向量来调整模型激活，使其朝向医学一致推理，无需更新模型权重或领域特定重训练。

Result: MEDASSESS-X在所有LLM家族中均带来一致提升：准确率最高提升+6%，事实一致性提升+7%，安全错误率降低达50%，稳定了通用和专业医疗LLMs的性能。

Conclusion: 推理时对齐方法能有效解决临床问答中的专业化谬误，提供了一种更灵活、成本效益更高的部署方案，无需依赖领域特定微调。

Abstract: Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.

</details>


### [70] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: 提出JurisMMA框架用于法律判决预测，通过任务分解和流程标准化处理多指控、证据多样等挑战，并构建包含10万+中文司法记录的JurisMM多模态数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 传统法律判决预测方法依赖统计分析或基于角色的模拟，在处理多指控、多样证据时面临挑战且缺乏适应性，需要更有效的框架来处理复杂的法律案件预测任务。

Method: 提出JurisMMA框架，将审判任务有效分解、流程标准化并组织为不同阶段；构建包含10万+近期中文司法记录的JurisMM大型数据集，包含文本和多模态视频-文本数据。

Result: 在JurisMM数据集和基准LawBench上的实验验证了框架的有效性，结果表明该框架不仅适用于法律判决预测，也为更广泛的法律应用提供了新视角。

Conclusion: JurisMMA框架通过任务分解和流程标准化有效解决了传统法律判决预测方法的局限性，结合大型多模态数据集为未来法律方法和数据集的发展提供了新方向。

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


### [71] [Rapport du Projet de Recherche TRAIMA](https://arxiv.org/abs/2601.12844)
*Julie Rançon,Jean-François Cerisier,Emilie Remond,Aurélien Nguyen,Andrew Peterson,Ladjel Bellatreche*

Main category: cs.CL

TL;DR: TRAIMA项目研究如何利用机器学习自动处理教育环境中的多模态互动，解决当前人工分析耗时且难以扩展的问题，重点关注课堂解释性和协作性序列的多模态特征。


<details>
  <summary>Details</summary>
Motivation: 当前教育互动研究中，语言、副语言和非语言数据的分析完全依赖人工处理，极其耗时且难以规模化。项目旨在探索机器学习方法如何帮助分类和识别这类多模态互动，特别是在法语作为外语和母语教学环境中。

Method: 项目采用多模态分析方法，结合话语分析和互动语言学理论，将解释性话语精确定义为三部结构（开场、解释核心、结束）。基于INTER-EXPLIC和EXPLIC-LEXIC语料库进行实证研究，重点分析教师手势、韵律特征等功能性角色。同时系统评估现有转录规范，建立与机器学习兼容的方法框架。

Result: 项目展示了转录实践中不可避免的变异性和解释性维度，建立了多模态教学互动自动处理的严格方法框架。确定了与机器学习兼容的转录规范、标注类别和分析单元，强调了理论明确性和研究者反思性的必要性。

Conclusion: TRAIMA项目并未开发完全可操作的自动化系统，而是为多模态教学互动的自动处理建立了严谨的方法框架。项目为未来跨学科研究奠定了基础，连接了教学法、话语分析、多模态研究和教育人工智能领域。

Abstract: The TRAIMA project (TRaitement Automatique des Interactions Multimodales en Apprentissage), conducted between March 2019 and June 2020, investigates the potential of automatic processing of multimodal interactions in educational settings. The project addresses a central methodological challenge in educational and interactional research: the analysis of verbal, paraverbal, and non-verbal data is currently carried out manually, making it extremely time-consuming and difficult to scale. TRAIMA explores how machine learning approaches could contribute to the categorisation and classification of such interactions. The project focuses specifically on explanatory and collaborative sequences occurring in classroom interactions, particularly in French as a Foreign Language (FLE) and French as a First Language (FLM) contexts. These sequences are analysed as inherently multimodal phenomena, combining spoken language with prosody, gestures, posture, gaze, and spatial positioning. A key theoretical contribution of the project is the precise linguistic and interactional definition of explanatory discourse as a tripartite sequence (opening, explanatory core, closure), drawing on discourse analysis and interactional linguistics. A substantial part of the research is devoted to the methodological foundations of transcription, which constitute a critical bottleneck for any form of automation. The report provides a detailed state of the art of existing transcription conventions (ICOR, Mondada, GARS, VALIBEL, Ferr{é}), highlighting their respective strengths and limitations when applied to multimodal classroom data. Through comparative analyses of manually transcribed sequences, the project demonstrates the inevitable variability and interpretative dimension of transcription practices, depending on theoretical positioning and analytical goals. Empirical work is based on several corpora, notably the INTER-EXPLIC corpus (approximately 30 hours of classroom interaction) and the EXPLIC-LEXIC corpus, which serve both as testing grounds for manual annotation and as reference datasets for future automation. Particular attention is paid to teacher gestures (kin{é}sic and proxemic resources), prosodic features, and their functional role in meaning construction and learner comprehension. The project also highlights the strategic role of the Techn{é}LAB platform, which provides advanced multimodal data capture (multi-camera video, synchronized audio, eye-tracking, digital interaction traces) and constitutes both a research infrastructure and a test environment for the development of automated tools. In conclusion, TRAIMA does not aim to deliver a fully operational automated system, but rather to establish a rigorous methodological framework for the automatic processing of multimodal pedagogical interactions. The project identifies transcription conventions, annotation categories, and analytical units that are compatible with machine learning approaches, while emphasizing the need for theoretical explicitness and researcher reflexivity. TRAIMA thus lays the groundwork for future interdisciplinary research at the intersection of didactics, discourse analysis, multimodality, and artificial intelligence in education.

</details>


### [72] [Race, Ethnicity and Their Implication on Bias in Large Language Models](https://arxiv.org/abs/2601.12868)
*Shiyue Hu,Ruizhe Li,Yanjun Gao*

Main category: cs.CL

TL;DR: 该论文通过可解释性方法研究LLM中种族和族裔信息的内部表示机制，发现人口统计信息分布在不同神经元中，相同人口统计线索可能引发不同行为，干预能减少偏见但效果有限。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗等高风险领域应用时，种族和族裔信息可能被显式或隐式处理，现有研究主要记录结果层面的差异，缺乏对这些效应内部机制的理解。

Method: 使用两个公开数据集（毒性生成和临床叙事理解任务），分析三个开源模型，采用可复现的可解释性流程，结合探测、神经元级归因和针对性干预方法。

Result: 人口统计信息分布在不同内部单元中，存在显著的跨模型差异；某些单元编码了预训练中的敏感或刻板印象关联；相同人口统计线索可能引发不同行为；抑制相关神经元能减少偏见但仍有显著残留效应。

Conclusion: 干预主要改变行为而非表示，表明需要更系统的缓解方法；研究揭示了LLM中人口统计信息表示的复杂性，为理解偏见机制提供了新视角。

Abstract: Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.

</details>


### [73] [From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.12904)
*Jiahao Wang,Weiyu Xie,Mingxing Zhang,Boxing Zhang,Jianwei Dong,Yuening Zhu,Chen Lin,Jinqi Tang,Yaochen Han,Zhiyuan Ai,Xianglin Chen,Yongwei Wu,Congfeng Jiang*

Main category: cs.CL

TL;DR: FusionRAG是一个新颖的推理框架，通过在预处理阶段将相关文本块信息嵌入到每个块中，并在在线重处理阶段重新计算模型关注的token的KV缓存，从而在保持生成质量的同时提高RAG效率。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）虽然能减少大语言模型的幻觉，但增加了提示长度，导致计算成本增加和首token时间（TTFT）延长。现有解决方案试图重用检索块的预处理KV缓存来加速RAG，但缺乏跨块上下文信息导致生成质量显著下降，使得KV缓存重用的潜在好处无法充分发挥。

Method: FusionRAG采用两阶段优化：1）离线预处理阶段：将其他相关文本块的信息嵌入到每个块中；2）在线重处理阶段：重新计算模型关注的token的KV缓存。这种方法在保持上下文信息的同时实现KV缓存的有效重用。

Result: 实验表明，FusionRAG在相同重计算比例下显著提高生成质量。通过重计算少于15%的token，FusionRAG相比基线方法实现了高达70%的归一化F1分数提升，相比完全注意力机制将TTFT减少了2.66-9.39倍。

Conclusion: FusionRAG通过创新的两阶段处理框架，在保持生成质量的同时有效提高了RAG系统的效率，实现了生成质量与计算效率之间的更好平衡，为KV缓存重用提供了实用解决方案。

Abstract: Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.

</details>


### [74] [Gated Differentiable Working Memory for Long-Context Language Modeling](https://arxiv.org/abs/2601.12906)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Yuyao Ge,Baolong Bi,Jiayu Yao,Jun Wan,Ziling Yin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: Gdwm提出了一种基于门控可微分工作内存的测试时自适应框架，通过上下文效用评估选择性整合关键信息，相比均匀写入策略减少4倍梯度步数


<details>
  <summary>Details</summary>
Motivation: 长上下文对Transformer的挑战：注意力分数在数千个token上被稀释，关键信息在中间丢失，模型在推理时难以适应新模式。现有测试时自适应方法使用均匀写入策略，浪费计算资源在低效用区域，且在语义异构上下文上梯度方差高。

Method: 将测试时自适应重构为预算约束下的内存整合问题，提出Gdwm框架，引入写入控制器门控整合过程。控制器估计上下文效用（衡量长距离上下文依赖的信息论指标），据此分配梯度步数，同时保持全局覆盖。

Result: 在ZeroSCROLLS和LongBench v2上的实验表明，Gdwm使用比均匀基线少4倍的梯度步数，达到相当或更优的性能，建立了测试时自适应的新效率-性能帕累托前沿。

Conclusion: Gdwm通过选择性整合高上下文效用区域，有效解决了长上下文Transformer的注意力稀释和关键信息丢失问题，显著提升了测试时自适应的计算效率。

Abstract: Long contexts challenge transformers: attention scores dilute across thousands of tokens, critical information is often lost in the middle, and models struggle to adapt to novel patterns at inference time. Recent work on test-time adaptation addresses this by maintaining a form of working memory -- transient parameters updated on the current context -- but existing approaches rely on uniform write policies that waste computation on low-utility regions and suffer from high gradient variance across semantically heterogeneous contexts. In this work, we reframe test-time adaptation as a budget-constrained memory consolidation problem, focusing on which parts of the context should be consolidated into working memory under limited computation. We propose Gdwm (Gated Differentiable Working Memory), a framework that introduces a write controller to gate the consolidation process. The controller estimates Contextual Utility, an information-theoretic measure of long-range contextual dependence, and allocates gradient steps accordingly while maintaining global coverage. Experiments on ZeroSCROLLS and LongBench v2 demonstrate that Gdwm achieves comparable or superior performance with 4$\times$ fewer gradient steps than uniform baselines, establishing a new efficiency-performance Pareto frontier for test-time adaptation.

</details>


### [75] [SciCoQA: Quality Assurance for Scientific Paper--Code Alignment](https://arxiv.org/abs/2601.12910)
*Tim Baumgärtner,Iryna Gurevych*

Main category: cs.CL

TL;DR: SciCoQA是一个用于检测科学出版物与其代码库之间差异的数据集，包含真实和合成的论文-代码差异，涵盖多个计算科学领域，现有LLM在检测真实差异方面表现有限。


<details>
  <summary>Details</summary>
Motivation: 确保科学出版物与其实现代码之间的一致性对于可重复性至关重要，当前缺乏系统检测论文与代码差异的数据集和方法。

Method: 从GitHub问题和可重复性论文构建真实差异数据集，提出合成数据生成方法扩展数据集规模，详细分析差异类型和类别，评估21个LLM在检测差异方面的性能。

Result: 构建了包含611个论文-代码差异的数据集（81个真实，530个合成），涵盖AI、物理、定量生物学等多个领域。评估显示现有LLM检测差异困难，特别是涉及省略细节、长上下文和预训练语料外数据的情况。最佳模型GPT-5仅能检测45.7%的真实差异。

Conclusion: SciCoQA揭示了科学出版物与代码实现之间普遍存在差异，现有LLM在检测这些差异方面能力有限，特别是在复杂场景下，需要进一步研究改进科学可重复性验证方法。

Abstract: We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\% of real-world paper-code discrepancies.

</details>


### [76] [Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs](https://arxiv.org/abs/2601.12921)
*Adimulya Kartiyasa,Bao Gia Cao,Boyang Li*

Main category: cs.CL

TL;DR: 提出了IndoSoSci数据集和一种将印尼文化知识注入大语言模型的有效方法，通过提取文化事实并使用LLM生成的假设文档作为查询进行检索增强生成，在IndoCulture基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对印尼文化的理解有限，而本土社会科学期刊作为重要的文化知识来源被忽视。这些期刊包含大量从本土视角出发的文化研究，是改善LLMs文化理解能力的宝贵资源。

Method: 1) 从151个开源印尼社会科学期刊创建IndoSoSci文本数据集；2) 提取与印尼文化相关的事实；3) 使用LLM生成的假设文档作为查询进行检索增强生成(RAG)；4) 将IndoSoSci与印尼维基百科结合。

Result: 提出的方法在IndoCulture基准上显著优于多个强基线模型。结合IndoSoSci和印尼维基百科，在IndoCulture基准上达到了新的最先进准确率。

Conclusion: 印尼社会科学期刊是改善LLMs印尼文化理解的重要资源，提出的RAG方法能有效注入文化知识，为LLMs的文化理解能力提升提供了有效途径。

Abstract: Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.

</details>


### [77] [A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits](https://arxiv.org/abs/2601.12945)
*Miao Xie,Siguang Chen,Chunli Lv*

Main category: cs.CL

TL;DR: 这是第一篇系统综述大语言模型与多臂老虎机双向交互的论文，探讨两者在组件层面的相互增强关系。


<details>
  <summary>Details</summary>
Motivation: 大语言模型已成为强大的语言理解和生成系统，而多臂老虎机算法为不确定性下的自适应决策提供了原则性框架。目前缺乏对这两个领域交叉潜力的系统性综述，特别是组件层面的双向交互分析。

Method: 采用系统性文献综述方法，从组件层面分析LLM与MAB的双向交互。具体包括：1) 分析MAB算法如何解决LLM的关键挑战（从预训练到检索增强生成和个性化）；2) 探讨LLM如何增强MAB系统（重新定义臂定义和环境建模等核心组件）。

Result: 识别了现有LLM增强的老虎机系统和老虎机增强的LLM系统，分析了它们的设计、方法论和性能。总结了关键挑战和代表性发现，并创建了GitHub仓库索引相关文献。

Conclusion: LLM与MAB的双向交互具有显著潜力：MAB算法能有效解决LLM的关键挑战，而LLM能增强MAB系统的决策能力。该综述为未来研究提供了指导框架，并建立了开源资源库促进该交叉领域的发展。

Abstract: Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.

</details>


### [78] [Trustworthy Data-driven Chronological Age Estimation from Panoramic Dental Images](https://arxiv.org/abs/2601.12960)
*Ainhoa Vivel-Couso,Nicolás Vila-Blanco,María J. Carreira,Alberto Bugarín-Diz,Inmaculada Tomás,Jose M. Alonso-Moral*

Main category: cs.CL

TL;DR: 提出一个结合不透明和透明方法的牙科年龄估计系统，通过自然语言生成模块为临床医生提供可理解的文本解释，提高AI在医疗中的透明度。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医疗保健中的应用虽然能实现个性化护理，但由于模型不透明性引发了信任问题。需要提高AI系统的透明度，特别是在牙科年龄估计这样的临床应用中。

Method: 开发了一个从全景图像进行牙科年龄估计的系统，结合了不透明和透明方法，通过自然语言生成模块产生临床医生友好的文本解释。解释生成采用基于规则的方法，并与牙科专家合作设计。使用问卷调查由牙科专家手动验证生成解释的质量。

Result: 牙科专家在五个维度上的平均评分为4.77±0.12（满分5分），表现出色。按照ALTAI清单进行的可信度自我评估在七个维度上得分为4.40±0.27（满分5分）。

Conclusion: 提出的系统成功提高了牙科年龄估计的透明度，通过自然语言解释增强了临床医生的信任。专家验证和ALTAI评估表明该系统在透明度和可信度方面表现良好，为医疗AI系统的可解释性提供了实用解决方案。

Abstract: Integrating deep learning into healthcare enables personalized care but raises trust issues due to model opacity. To improve transparency, we propose a system for dental age estimation from panoramic images that combines an opaque and a transparent method within a natural language generation (NLG) module. This module produces clinician-friendly textual explanations about the age estimations, designed with dental experts through a rule-based approach. Following the best practices in the field, the quality of the generated explanations was manually validated by dental experts using a questionnaire. The results showed a strong performance, since the experts rated 4.77+/-0.12 (out of 5) on average across the five dimensions considered. We also performed a trustworthy self-assessment procedure following the ALTAI checklist, in which it scored 4.40+/-0.27 (out of 5) across seven dimensions of the AI Trustworthiness Assessment List.

</details>


### [79] [Pardon? Evaluating Conversational Repair in Large Audio-Language Models](https://arxiv.org/abs/2601.12973)
*Shuanghong Huang,Jinlei Xu,Youchao Zhou,Yanghao Zhou,Xuan Zhao,Chong Feng,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 论文提出了修复感知评估框架，用于评估大型音频语言模型在可回答与不可回答音频输入下的表现，并引入EAR评分来联合评估任务能力和修复行为


<details>
  <summary>Details</summary>
Motivation: 现有大型音频语言模型评估主要关注答案准确性和声学扰动鲁棒性，但假设语音输入在语义上总是可回答的。然而现实交互中常出现信息缺失导致不可回答的情况，需要评估模型识别不可回答性并启动适当对话修复的能力

Method: 1) 引入修复感知评估设置，明确区分可回答和不可回答音频输入；2) 将可回答性定义为输入本身的属性；3) 使用语义-声学掩蔽协议构建配对评估条件；4) 提出EAR评分，这是一个非补偿性指标，联合评估可回答条件下的任务能力和不可回答条件下的修复行为

Result: 在两个语音问答基准测试中，实验显示答案准确性和对话可靠性之间存在一致差距：许多模型在输入可回答时表现良好，但大多数无法识别语义不可回答性并启动适当的对话修复

Conclusion: 研究揭示了当前以准确性为中心的评估实践的局限性，并激励开发将不可回答输入视为修复和持续交互线索的可靠性评估方法

Abstract: Large Audio-Language Models (LALMs) have demonstrated strong performance in spoken question answering (QA), with existing evaluations primarily focusing on answer accuracy and robustness to acoustic perturbations. However, such evaluations implicitly assume that spoken inputs remain semantically answerable, an assumption that often fails in real-world interaction when essential information is missing. In this work, we introduce a repair-aware evaluation setting that explicitly distinguishes between answerable and unanswerable audio inputs. We define answerability as a property of the input itself and construct paired evaluation conditions using a semantic-acoustic masking protocol. Based on this setting, we propose the Evaluability Awareness and Repair (EAR) score, a non-compensatory metric that jointly evaluates task competence under answerable conditions and repair behavior under unanswerable conditions. Experiments on two spoken QA benchmarks across diverse LALMs reveal a consistent gap between answer accuracy and conversational reliability: while many models perform well when inputs are answerable, most fail to recognize semantic unanswerability and initiate appropriate conversational repair. These findings expose a limitation of prevailing accuracy-centric evaluation practices and motivate reliability assessments that treat unanswerable inputs as cues for repair and continued interaction.

</details>


### [80] [ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation](https://arxiv.org/abs/2601.12983)
*Jesus-German Ortiz-Barajas,Jonathan Tonglet,Vivek Gupta,Iryna Gurevych*

Main category: cs.CL

TL;DR: ChartAttack框架评估多模态大语言模型生成误导性图表的风险，通过注入误导元素降低图表问答准确性


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs被广泛用于从数据表自动生成图表，虽然提高了数据分析效率，但也带来了新的滥用风险，需要评估这些模型被用于大规模生成误导性图表的可能性

Method: 提出ChartAttack框架，通过向图表设计中注入误导元素来诱导对底层数据的错误解读；创建AttackViz数据集，包含标注了有效误导元素及其诱导错误答案的图表规范-QA对

Result: 实验显示ChartAttack显著降低MLLM阅读器的QA性能：域内设置平均降低19.6个百分点，跨域设置平均降低14.9个百分点；人类研究显示参与者面对误导性图表时准确性平均下降20.2个百分点

Conclusion: 研究结果强调了在MLLM图表生成系统的设计、评估和部署中迫切需要加强鲁棒性和安全性考虑

Abstract: Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.

</details>


### [81] [Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.12995)
*Runxuan Liu,Xianhao Ou,Xinyan Ma,Jiyuan Wang,Jiafeng Liang,Jiaqi Li,Tao He,Zheng Chu,Rongchuan Mu,Zekun Wang,Baoxin Wang,Dayong Wu,Ming Liu,Shijin Wang,Guoping Hu,Bing Qin*

Main category: cs.CL

TL;DR: 提出图推理范式(GRP)和过程感知分层裁剪组相对策略优化(PASC-GRPO)，通过结构化图表示替代非结构化文本推理，解决现有强化学习验证奖励方法中的计算瓶颈、监督粒度粗、奖励攻击等问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的推理主要生成纯文本，对非结构化数据进行语义评估会在训练中产生计算瓶颈。尽管有RLVR优化，现有方法仍存在监督粒度粗、奖励攻击、训练成本高和泛化性差等问题。

Method: 提出图推理范式(GRP)，实现结构化和符号化推理，通过带步骤级认知标签的图结构表示。在此基础上设计PASC-GRPO：1)用结构化评估替代语义评估；2)通过图结构结果奖励实现过程感知验证；3)通过分层裁剪优势估计缓解奖励攻击。

Result: 实验证明在数学推理和代码生成任务上取得显著改进。数据、模型和代码将后续发布。

Conclusion: GRP和PASC-GRPO通过结构化推理表示和过程感知优化，有效解决了当前RLVR方法中的关键问题，提升了LLMs的推理能力。

Abstract: Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.

</details>


### [82] [Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context](https://arxiv.org/abs/2601.13018)
*Ghislain Dorian Tchuente Mondjo*

Main category: cs.CL

TL;DR: 提出BiAtt-BiRNN-HateXplain模型，通过双向注意力机制和双向RNN层改进仇恨言论检测的可解释性，减少注意力变异性和无意识偏见


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测模型存在注意力变异问题，导致解释不一致、预测不稳定和学习困难。黑盒模型缺乏透明度，而HateXplain基准的多任务方法虽然同时学习解释和分类，但注意力预测变化较大。需要更稳定、可解释且能处理序列数据的模型。

Method: 提出BiAtt-BiRNN-HateXplain模型：1) 使用双向注意力机制改进可解释性；2) 通过BiRNN层考虑输入数据的序列特性；3) 采用多任务学习同时进行解释和分类；4) 相比复杂的LLMs更易解释且透明

Result: 在HateXplain数据上的实验结果显示：1) 检测性能明显提升；2) 可解释性改善；3) 无意识偏见减少；4) 解决了注意力变异问题

Conclusion: BiAtt-BiRNN-HateXplain模型通过结合双向注意力和BiRNN，有效提高了仇恨言论检测的可解释性和稳定性，减少了注意力变异和无意识偏见，为透明可靠的仇恨言论检测提供了解决方案

Abstract: Technological advances in the Internet and online social networks have brought many benefits to humanity. At the same time, this growth has led to an increase in hate speech, the main global threat. To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities. The experimental results on HateXplain data show a clear improvement in detection performance, explainability and a reduction in unintentional bias.

</details>


### [83] [Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses](https://arxiv.org/abs/2601.13024)
*Chongyuan Dai,Yaling Shen,Jinpeng Hu,Zihan Gao,Jia Li,Yishun Jiang,Yaxiong Wang,Liu Liu,Zongyuan Ge*

Main category: cs.CL

TL;DR: CEDAR是一个多模态基准测试，专注于评估大语言模型在文化特异性情感理解方面的能力，通过捕捉不同文化背景下情感反应的差异来弥补现有文化对齐评估的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文化对齐评估主要关注地理事实、社会习俗等陈述性知识，未能捕捉不同文化背景下主观情感解释的差异。文化作为人类情感处理的基本决定因素，深刻影响个体对情感刺激的感知和解释，因此需要专门评估模型在文化特异性情感理解方面的能力。

Method: 提出CEDAR基准测试构建方法：1）利用LLM生成的临时标签识别产生跨文化情感差异的实例；2）通过严格的人工评估获得可靠的真实标注；3）构建包含10,962个实例的数据集，涵盖7种语言和14种细粒度情感类别，每种语言包含400个多模态样本和1,166个纯文本样本。

Result: 对17个代表性多语言模型的全面评估显示：语言一致性与文化对齐之间存在分离现象，表明当前模型在文化基础情感理解方面仍面临重大挑战，即使模型在语言层面表现一致，也难以准确捕捉文化特异性的情感反应。

Conclusion: 文化基础的情感理解是当前大语言模型面临的重要挑战，需要专门设计的评估基准来测试模型在不同文化背景下解释情感刺激的能力。CEDAR基准填补了这一空白，为评估和提升模型的文化对齐能力提供了重要工具。

Abstract: Culture serves as a fundamental determinant of human affective processing and profoundly shapes how individuals perceive and interpret emotional stimuli. Despite this intrinsic link extant evaluations regarding cultural alignment within Large Language Models primarily prioritize declarative knowledge such as geographical facts or established societal customs. These benchmarks remain insufficient to capture the subjective interpretative variance inherent to diverse sociocultural lenses. To address this limitation, we introduce CEDAR, a multimodal benchmark constructed entirely from scenarios capturing Culturally \underline{\textsc{E}}licited \underline{\textsc{D}}istinct \underline{\textsc{A}}ffective \underline{\textsc{R}}esponses. To construct CEDAR, we implement a novel pipeline that leverages LLM-generated provisional labels to isolate instances yielding cross-cultural emotional distinctions, and subsequently derives reliable ground-truth annotations through rigorous human evaluation. The resulting benchmark comprises 10,962 instances across seven languages and 14 fine-grained emotion categories, with each language including 400 multimodal and 1,166 text-only samples. Comprehensive evaluations of 17 representative multilingual models reveal a dissociation between language consistency and cultural alignment, demonstrating that culturally grounded affective understanding remains a significant challenge for current models.

</details>


### [84] [SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification](https://arxiv.org/abs/2601.13035)
*Xu Xiaodan,Hu Xiaolin*

Main category: cs.CL

TL;DR: SASA框架通过分离注意力机制和语义感知对比学习提升知识图谱三元组分类性能，在FB15k-237和YAGO3-10数据集上分别实现+5.9%和+3.4%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的三元组分类方法存在两个关键挑战：1）忽略知识图谱不同组件间的有效语义交互；2）采用单一二元分类训练目标导致语义表示学习不充分。

Method: 提出SASA框架：1）分离注意力机制将三元组编码为解耦的上下文表示并通过更有效的交互方式融合；2）语义感知分层对比学习作为辅助训练目标，包含局部和全局两个层次的对比学习。

Result: 在两个基准数据集上显著超越现有最优方法：FB15k-237准确率提升+5.9%，YAGO3-10准确率提升+3.4%。

Conclusion: SASA通过分离注意力机制增强语义交互，结合语义感知对比学习提升判别能力，有效解决了现有三元组分类方法的局限性，实现了显著的性能提升。

Abstract: Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\% on FB15k-237 and +3.4\% on YAGO3-10.

</details>


### [85] [Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition](https://arxiv.org/abs/2601.13044)
*Warit Sirichotedumrong,Adisai Na-Thalang,Potsawee Manakul,Pittawat Taveekitworachai,Sittipong Sripaisarnmongkol,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: Typhoon ASR Real-time是一个115M参数的FastConformer-Transducer模型，用于低延迟泰语语音识别，通过严格的文本规范化实现与Whisper Large-v3相当的准确性，同时计算成本降低45倍，并发布了标准化的泰语ASR基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有大型编码器-解码器模型（如Whisper）在离线转录中表现良好，但由于高延迟不适用于流式应用。泰语ASR领域缺乏高效的流式解决方案，且存在预训练模型主导离线架构的问题，需要填补这一关键空白。

Method: 1. 采用115M参数的FastConformer-Transducer架构实现低延迟流式识别；2. 设计严格的文本规范化流程，解决泰语转录中的系统歧义（包括上下文相关的数字发音和重复标记）；3. 引入两阶段课程学习方法进行伊桑（东北）方言适应，同时保持中部泰语性能；4. 发布Typhoon ASR Benchmark，提供遵循泰语语言学规范的人工标注数据集和标准化评估协议。

Result: 紧凑模型相比Whisper Large-v3实现45倍计算成本降低，同时保持相当的准确性。文本规范化流程解决了泰语转录中的系统歧义，创建了一致的训练目标。两阶段课程学习方法成功实现了伊桑方言适应而不损害中部泰语性能。

Conclusion: 严格的文本规范化可以与模型缩放产生同等影响，使得紧凑模型在保持准确性的同时大幅降低计算成本。Typhoon ASR Real-time填补了泰语ASR领域高效流式解决方案的空白，并通过发布标准化基准数据集解决了可复现性挑战。

Abstract: Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.

</details>


### [86] [Profiling German Text Simplification with Interpretable Model-Fingerprints](https://arxiv.org/abs/2601.13050)
*Lars Klöser,Mika Beele,Bodo Kraft*

Main category: cs.CL

TL;DR: 本文提出了Simplification Profiler工具包，用于生成文本简化模型的多维可解释指纹，通过模型行为特征而非传统人工评分来诊断模型性能，特别适用于数据稀缺的多语言场景。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对大型语言模型文本简化行为的全面、高效、可复现的诊断工具。特别是在多语言场景下，数据稀缺问题在创建适应不同目标群体的灵活模型时更为突出，需要新的评估范式。

Method: 开发Simplification Profiler诊断工具包，生成简化文本的多维可解释指纹。通过聚合多个简化结果形成模型指纹，采用元评估方法测试指纹的描述能力，使用线性分类器验证不同模型配置的可区分性。

Result: Profiler能够区分提示策略的高层行为变化和提示工程的细粒度调整（包括few-shot示例）。完整特征集达到71.9%的分类F1分数，比简单基线提高超过48个百分点。

Conclusion: Simplification Profiler为开发者提供了细粒度、可操作的分析工具，有助于构建更有效、真正自适应的文本简化系统，特别是在数据稀缺的多语言环境中。

Abstract: While Large Language Models (LLMs) produce highly nuanced text simplifications, developers currently lack tools for a holistic, efficient, and reproducible diagnosis of their behavior. This paper introduces the Simplification Profiler, a diagnostic toolkit that generates a multidimensional, interpretable fingerprint of simplified texts. Multiple aggregated simplifications of a model result in a model's fingerprint. This novel evaluation paradigm is particularly vital for languages, where the data scarcity problem is magnified when creating flexible models for diverse target groups rather than a single, fixed simplification style. We propose that measuring a model's unique behavioral signature is more relevant in this context as an alternative to correlating metrics with human preferences. We operationalize this with a practical meta-evaluation of our fingerprints' descriptive power, which bypasses the need for large, human-rated datasets. This test measures if a simple linear classifier can reliably identify various model configurations by their created simplifications, confirming that our metrics are sensitive to a model's specific characteristics. The Profiler can distinguish high-level behavioral variations between prompting strategies and fine-grained changes from prompt engineering, including few-shot examples. Our complete feature set achieves classification F1-scores up to 71.9 %, improving upon simple baselines by over 48 percentage points. The Simplification Profiler thus offers developers a granular, actionable analysis to build more effective and truly adaptive text simplification systems.

</details>


### [87] [Alexandria: A Multi-Domain Dialectal Arabic Machine Translation Dataset for Culturally Inclusive and Linguistically Diverse LLMs](https://arxiv.org/abs/2601.13099)
*Abdellah El Mekki,Samar M. Magdy,Houdaifa Atou,Ruwa AbuHweidi,Baraah Qawasmeh,Omer Nacar,Thikra Al-hibiri,Razan Saadie,Hamzah Alsayadi,Nadia Ghezaiel Hammouda,Alshima Alkhazimi,Aya Hamod,Al-Yas Al-Ghafri,Wesam El-Sayed,Asila Al sharji,Mohamad Ballout,Anas Belfathi,Karim Ghaddar,Serry Sibaee,Alaa Aoun,Areej Asiri,Lina Abureesh,Ahlam Bashiti,Majdal Yousef,Abdulaziz Hafiz,Yehdih Mohamed,Emira Hamedtou,Brakehe Brahim,Rahaf Alhamouri,Youssef Nafea,Aya El Aatar,Walid Al-Dhabyani,Emhemed Hamed,Sara Shatnawi,Fakhraddin Alwajih,Khalid Elkhidir,Ashwag Alasmari,Abdurrahman Gerrio,Omar Alshahri,AbdelRahim A. Elmadany,Ismail Berrada,Amir Azad Adli Alkathiri,Fadi A Zaraket,Mustafa Jarrar,Yahya Mohamed El Hadj,Hassan Alhuzali,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: Alexandria是一个大规模、社区驱动、人工翻译的数据集，旨在解决阿拉伯语方言机器翻译的泛化问题，覆盖13个阿拉伯国家和11个高影响力领域，提供城市级别的方言细粒度标注。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语是高度双言现象的语言，日常交流主要使用地区方言而非现代标准阿拉伯语。然而，现有机器翻译系统对方言输入泛化能力差，限制了数百万使用者的实用性。

Method: 构建Alexandria数据集：1）覆盖13个阿拉伯国家和11个高影响力领域；2）提供城市来源元数据，超越粗粒度区域标签；3）包含多轮对话场景，标注说话者-受话者性别配置；4）总计107K样本。

Result: Alexandria数据集既可作为训练资源，也可作为评估机器翻译和大语言模型的严格基准。通过自动和人工评估阿拉伯语感知的LLMs，揭示了当前在跨阿拉伯语方言和次方言翻译方面的能力与持续挑战。

Conclusion: Alexandria数据集填补了阿拉伯语方言翻译资源的空白，提供了前所未有的方言细粒度，能够支持机器翻译和大语言模型在阿拉伯语方言处理方面的研究和应用，同时暴露了该领域仍存在的显著挑战。

Abstract: Arabic is a highly diglossic language where most daily communication occurs in regional dialects rather than Modern Standard Arabic. Despite this, machine translation (MT) systems often generalize poorly to dialectal input, limiting their utility for millions of speakers. We introduce \textbf{Alexandria}, a large-scale, community-driven, human-translated dataset designed to bridge this gap. Alexandria covers 13 Arab countries and 11 high-impact domains, including health, education, and agriculture. Unlike previous resources, Alexandria provides unprecedented granularity by associating contributions with city-of-origin metadata, capturing authentic local varieties beyond coarse regional labels. The dataset consists of multi-turn conversational scenarios annotated with speaker-addressee gender configurations, enabling the study of gender-conditioned variation in dialectal use. Comprising 107K total samples, Alexandria serves as both a training resource and a rigorous benchmark for evaluating MT and Large Language Models (LLMs). Our automatic and human evaluation of Arabic-aware LLMs benchmarks current capabilities in translating across diverse Arabic dialects and sub-dialects, while exposing significant persistent challenges.

</details>


### [88] [Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification](https://arxiv.org/abs/2601.13105)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: 该研究结合LoRA微调大语言模型与RAG框架，实现了英语双及物构式的自动识别，在BNC标注数据上取得了显著优于原生模型和纯理论RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决英语双及物构式的自动识别问题，探索结合参数高效微调（LoRA）与检索增强生成（RAG）的方法来提升识别准确率，弥补传统方法在语义理解上的不足。

Method: 采用LoRA微调Qwen3-8B大语言模型，结合RAG框架构建检索增强系统。在BNC语料库的标注数据上进行二元分类任务，对比了原生Qwen3-MAX模型、纯理论RAG系统和LoRA微调模型的性能。

Result: LoRA微调的Qwen3-8B模型在双及物构式识别任务上显著优于原生Qwen3-MAX模型和纯理论RAG系统。错误分析表明，微调使模型从表层形式匹配转向基于语义的更深层理解。

Conclusion: LoRA微调与RAG框架的有效结合能够显著提升英语双及物构式的自动识别性能，微调过程促使模型从形式匹配转向语义理解，为构式语法研究提供了有效的计算工具。

Abstract: This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.

</details>


### [89] [Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains](https://arxiv.org/abs/2601.13137)
*Yuan Gao,Zhigang Liu,Xinyu Yao,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 提出对抗对齐框架VC-LLM，通过持续预训练、指令微调和对抗训练增强大语言模型在敏感领域（种族、社会、政治）的价值一致性


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，在敏感领域中存在的偏见和价值不一致问题逐渐凸显，特别是在种族、社会和政治方面，需要解决这些模型的价值对齐问题

Method: 提出对抗对齐框架：1）持续预训练；2）指令微调；3）对抗训练（使用Attacker生成争议性查询，Actor生成价值一致的响应，Critic过滤并确保响应质量）。训练了专门针对敏感领域的价值一致大语言模型VC-LLM，并构建了中英文双语评估数据集

Result: 实验结果表明，VC-LLM在中英文测试中都优于现有主流模型，验证了该方法的有效性。论文包含具有冒犯性或有害性质的LLM示例

Conclusion: 提出的对抗对齐框架能有效增强大语言模型在敏感领域的价值一致性，VC-LLM在价值对齐方面表现优于现有模型，为敏感领域的大语言模型价值对齐提供了有效解决方案

Abstract: With the wide application of large language models (LLMs), the problems of bias and value inconsistency in sensitive domains have gradually emerged, especially in terms of race, society and politics. In this paper, we propose an adversarial alignment framework, which enhances the value consistency of the model in sensitive domains through continued pre-training, instruction fine-tuning and adversarial training. In adversarial training, we use the Attacker to generate controversial queries, the Actor to generate responses with value consistency, and the Critic to filter and ensure response quality. Furthermore, we train a Value-Consistent Large Language Model, VC-LLM, for sensitive domains, and construct a bilingual evaluation dataset in Chinese and English. The experimental results show that VC-LLM performs better than the existing mainstream models in both Chinese and English tests, verifying the effectiveness of the method. Warning: This paper contains examples of LLMs that are offensive or harmful in nature.

</details>


### [90] [Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages](https://arxiv.org/abs/2601.13178)
*Joseph Gatto,Parker Seegmiller,Timothy Burdick,Philip Resnik,Roshnik Rahat,Sarah DeLozier,Sarah M. Preum*

Main category: cs.CL

TL;DR: 提出首个大规模公开数据集PMR-Bench，用于研究异步门诊门户消息的医疗分诊任务，将患者消息分诊视为成对推理问题，通过LLM训练实现医疗紧急程度的比较排序。


<details>
  <summary>Details</summary>
Motivation: 医疗分诊是根据医疗需求分配资源和优先处理患者的关键任务，但缺乏大规模公开数据集来研究异步门诊门户消息的分诊问题，需要开发能够模拟真实世界医疗分诊场景的基准测试。

Method: 将患者消息分诊建模为成对推理问题，训练LLM进行"哪个消息更紧急"的头部对比；开发PMR-Bench基准，包含1569条独特消息和2000+高质量测试对；提出自动数据标注策略为LLM提供领域指导；训练UrgentReward（使用Bradley-Terry目标）和UrgentSFT（使用下一词预测目标）两类模型。

Result: UrgentSFT在PMR-Bench上表现最佳，UrgentReward在低资源设置中显示独特优势；UrgentSFT-8B和UrgentReward-8B相比现成的8B模型在收件箱排序指标上分别提升15和16个百分点。

Conclusion: 成功开发了首个大规模公开医疗分诊数据集PMR-Bench，证明了将患者消息分诊作为成对推理问题的有效性，为医疗分诊研究提供了重要基准和实用方法。

Abstract: Medical triage is the task of allocating medical resources and prioritizing patients based on medical need. This paper introduces the first large-scale public dataset for studying medical triage in the context of asynchronous outpatient portal messages. Our novel task formulation views patient message triage as a pairwise inference problem, where we train LLMs to choose `"which message is more medically urgent" in a head-to-head tournament-style re-sort of a physician's inbox. Our novel benchmark PMR-Bench contains 1569 unique messages and 2,000+ high-quality test pairs for pairwise medical urgency assessment alongside a scalable training data generation pipeline. PMR-Bench includes samples that contain both unstructured patient-written messages alongside real electronic health record (EHR) data, emulating a real-world medical triage scenario.
  We develop a novel automated data annotation strategy to provide LLMs with in-domain guidance on this task. The resulting data is used to train two model classes, UrgentReward and UrgentSFT, leveraging Bradley-Terry and next token prediction objective, respectively to perform pairwise urgency classification. We find that UrgentSFT achieves top performance on PMR-Bench, with UrgentReward showing distinct advantages in low-resource settings. For example, UrgentSFT-8B and UrgentReward-8B provide a 15- and 16-point boost, respectively, on inbox sorting metrics over off-the-shelf 8B models. Paper resources can be found at https://tinyurl.com/Patient-Message-Triage

</details>


### [91] [OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand](https://arxiv.org/abs/2601.13183)
*Sergio Servantez,Sarah B. Lawsky,Rajiv Jain,Daniel W. Linna,Kristian Hammond*

Main category: cs.CL

TL;DR: OpenExempt是一个用于法律推理诊断评估的框架和基准，通过专家构建的美国破产法法规符号表示动态生成自然语言推理任务，支持细粒度控制任务复杂度，包含9,765个样本的基准测试揭示了语言模型在长推理路径和混淆语句下的性能陡降。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准存在局限性：静态问答对只能提供性能快照，将复杂行为压缩为单一准确率指标；在复杂、规则密集的法律领域，现有基准构建成本高且难以隔离特定失败模式。需要一种能够进行诊断性评估、精细控制任务复杂度的法律推理评估框架。

Method: 提出OpenExempt框架，使用专家构建的美国破产法法规符号表示，动态生成大量自然语言推理任务及其机器可计算解；用户可细粒度控制任务复杂度和范围，单独测试特定推理技能；基于此框架构建OpenExempt基准，包含9,765个样本，分布在9个评估套件中。

Result: 对13个多样化语言模型的实验显示：在较长推理路径和存在混淆语句的情况下，模型性能出现显著陡降；基准能够有效诊断模型在复杂法律推理中的具体失败模式；框架和基准已公开发布以支持推理系统研究。

Conclusion: OpenExempt框架和基准为法律推理提供了有效的诊断评估工具，能够动态生成任务、精细控制复杂度，揭示了语言模型在复杂推理场景下的性能局限性，有助于理解和改进下一代推理系统。

Abstract: Reasoning benchmarks have played a crucial role in the progress of language models. Yet rigorous evaluation remains a significant challenge as static question-answer pairs provide only a snapshot of performance, compressing complex behavior into a single accuracy metric. This limitation is especially true in complex, rule-bound domains such as law, where existing benchmarks are costly to build and ill suited for isolating specific failure modes. To address this, we introduce OpenExempt, a framework and benchmark for diagnostic evaluation of legal reasoning. The OpenExempt Framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate a large space of natural language reasoning tasks and their machine-computable solutions on demand. This gives users fine-grained control over task complexity and scope, allowing individual reasoning skills to be probed in isolation. Using this system, we construct the OpenExempt Benchmark, a diagnostic benchmark for legal reasoning with 9,765 samples across nine evaluation suites designed to carefully probe model capabilities. Experiments on 13 diverse language models reveal sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements. We release the framework and benchmark publicly to support research aimed at understanding and improving the next generation of reasoning systems.

</details>


### [92] [Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation](https://arxiv.org/abs/2601.13228)
*Tianqi Du,Lizhe Fang,Weijie Yang,Chenheng Zhang,Zeming Wei,Yifei Wang,Yisen Wang*

Main category: cs.CL

TL;DR: A3框架将自回归建模扩展为任意顺序、任意子集的预测，结合了自回归模型的深度建模优势和扩散模型的灵活生成能力


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然支持任意顺序生成和双向条件化，但在建模深度、样本质量和稳定性方面不如自回归模型。需要一种既能保持自回归模型优势又能获得扩散模型灵活性的统一框架。

Method: 提出A3框架，将标准自回归分解扩展到任意标记组和生成顺序。采用双流注意力架构和渐进适应策略，将预训练的自回归模型转换为支持任意顺序预测的模型。

Result: 在问答、常识推理和故事填充任务上的实验表明，A3在保持灵活解码的同时，性能优于基于扩散的模型。

Conclusion: A3提供了一个统一的方法，实现了灵活、高效且新颖的语言建模范式，结合了自回归模型的概率严谨性和扩散模型的生成灵活性。

Abstract: Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.

</details>


### [93] [Aligning Agentic World Models via Knowledgeable Experience Learning](https://arxiv.org/abs/2601.13247)
*Baochang Ren,Yunzhi Yao,Rui Sun,Shuofei Qiao,Ningyu Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: WorldMind框架通过构建符号化世界知识库，利用环境反馈解决LLMs物理幻觉问题，实现物理可行性和任务最优性的统一


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在模态断层：拥有丰富语义知识但缺乏程序性基础来尊重物理世界的不变法则，导致产生逻辑合理但物理上不可执行的计划（物理幻觉）。现有对齐策略主要依赖资源密集的训练或微调，试图将动态环境规则压缩到静态模型参数中，但这种参数化封装本质上是僵化的，难以适应物理动态的开放可变性

Method: 引入WorldMind框架，自主构建符号化世界知识库，通过综合环境反馈来统一过程经验（通过预测误差强制物理可行性）和目标经验（通过成功轨迹指导任务最优性）

Result: 在EB-ALFRED和EB-Habitat上的实验表明，WorldMind相比基线方法取得了优越性能，并展现出显著的跨模型和跨环境可迁移性

Conclusion: WorldMind通过构建符号化世界知识库有效解决了LLMs的物理幻觉问题，实现了物理可行性和任务最优性的统一，且具有良好的可迁移性，避免了传统参数化方法的僵化问题

Abstract: Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.

</details>


### [94] [Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph](https://arxiv.org/abs/2601.13251)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: 提出大规模语义聚类系统，解决神经嵌入无法区分同义词和反义词的盲点，通过三路语义关系判别器和软到硬聚类算法生成290万个高精度语义簇。


<details>
  <summary>Details</summary>
Motivation: 神经嵌入存在显著盲点：无法可靠区分同义词和反义词，导致提高相似度阈值时仍会将反义词归为一类。现有方法在形态丰富和低资源语言中同义词数据库稀疏，需要更精确的语义聚类系统。

Method: 1) 构建包含843,000个概念对的数据集，涵盖同义、反义和同下位关系，使用Gemini 2.5-Flash LLM增强并用人编词典验证；2) 提出三路语义关系判别器，实现90%宏F1分数；3) 开发软到硬聚类算法，采用拓扑感知的两阶段扩展-剪枝过程与拓扑投票，防止语义漂移和错误传递链。

Result: 系统处理1500万个词汇项，评估5.2亿个潜在关系，最终生成290万个高精度语义簇。三路判别器达到90%宏F1分数，软到硬聚类算法有效防止语义漂移并解决多义性问题。

Conclusion: 该系统解决了神经嵌入区分同义词和反义词的根本问题，通过专门的关系判别和拓扑感知聚类生成高质量语义资源，特别适用于形态丰富和低资源语言的语义搜索和检索增强生成。

Abstract: Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.

</details>


### [95] [A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus](https://arxiv.org/abs/2601.13253)
*Ebubekir Tosun,Mehmet Emin Buldur,Özay Ezerceli,Mahmoud ElHussieni*

Main category: cs.CL

TL;DR: 提出一种混合方法生成低资源语言的大规模语义关系数据集，以土耳其语为例创建了包含84.3万对语义关系的语料库，成本仅65美元


<details>
  <summary>Details</summary>
Motivation: 解决土耳其语等低资源语言中语义关系数据稀缺的问题，现有资源规模有限且成本高昂

Method: 三阶段混合方法：1) FastText嵌入+凝聚聚类识别语义簇；2) Gemini 2.5-Flash自动分类语义关系；3) 整合词典资源。涵盖同义词、反义词、同下位词三种关系类型

Result: 创建了84.3万对土耳其语语义关系数据集，规模是现有资源的10倍，成本仅65美元。下游任务验证：嵌入模型达到90% top-1检索准确率，分类模型达到90% F1-macro分数

Conclusion: 该方法可扩展地解决了土耳其语NLP中的数据稀缺问题，适用于其他低资源语言，数据集和模型已公开

Abstract: We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.

</details>


### [96] [Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models](https://arxiv.org/abs/2601.13260)
*Sawsan Alqahtani,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 该论文主张将分词视为核心建模决策而非预处理步骤，提出上下文感知框架，强调分词器与模型的协同设计，以实现更公平、高效、适应性强的语言技术。


<details>
  <summary>Details</summary>
Motivation: 当前分词方法（如BPE）存在三个主要问题：1）与语言结构错位，2）放大偏见，3）在不同语言和领域中浪费容量。这些问题源于分词被视为技术后处理而非核心设计决策。

Method: 提出上下文感知框架，将分词器与模型协同设计，考虑语言学、领域和部署因素。强调标准化评估和透明报告，使分词选择可问责、可比较。

Result: 论文提出了理论框架而非具体实验结果，但论证了将分词作为核心设计问题可以带来更公平、高效、适应性强的语言技术。

Conclusion: 分词应被视为核心建模决策而非预处理步骤。通过上下文感知框架、标准化评估和透明报告，可以实现更公平、高效、适应性强的语言技术。

Abstract: Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.

</details>


### [97] [Unlearning in LLMs: Methods, Evaluation, and Open Challenges](https://arxiv.org/abs/2601.13264)
*Tyler Lizzo,Larry Heck*

Main category: cs.CL

TL;DR: 关于大语言模型机器遗忘方法的综述，系统分类现有技术并分析评估体系，为开发可靠遗忘技术提供路线图


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理任务中取得显著成功，但其广泛部署引发了隐私、版权、安全和偏见等紧迫问题。机器遗忘作为一种有前景的范式，可以在不完全重新训练的情况下有选择地从训练模型中移除知识或数据

Method: 提供结构化综述，将现有遗忘方法分类为数据中心、参数中心、架构中心、混合及其他策略；回顾评估生态系统，包括基准测试、指标和数据集；分析关键挑战和开放问题

Result: 系统梳理了大语言模型机器遗忘的研究现状，建立了方法分类框架和评估体系，识别了当前研究的主要挑战和未来方向

Conclusion: 通过综合当前进展和突出开放方向，本文旨在为开发大语言模型中可靠和负责任的遗忘技术提供路线图

Abstract: Large language models (LLMs) have achieved remarkable success across natural language processing tasks, yet their widespread deployment raises pressing concerns around privacy, copyright, security, and bias. Machine unlearning has emerged as a promising paradigm for selectively removing knowledge or data from trained models without full retraining. In this survey, we provide a structured overview of unlearning methods for LLMs, categorizing existing approaches into data-centric, parameter-centric, architecture-centric, hybrid, and other strategies. We also review the evaluation ecosystem, including benchmarks, metrics, and datasets designed to measure forgetting effectiveness, knowledge retention, and robustness. Finally, we outline key challenges and open problems, such as scalable efficiency, formal guarantees, cross-language and multimodal unlearning, and robustness against adversarial relearning. By synthesizing current progress and highlighting open directions, this paper aims to serve as a roadmap for developing reliable and responsible unlearning techniques in large language models.

</details>


### [98] [A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification](https://arxiv.org/abs/2601.13288)
*Gonzalo Ariel Meyoyan,Luciano Del Corro*

Main category: cs.CL

TL;DR: 论文提出在LLM服务中复用计算的方法：通过训练轻量级探针在LLM隐藏状态上进行分类预测，避免使用独立的分类模型，从而降低延迟、VRAM占用和操作复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前生产级LLM系统通常使用独立模型进行安全和分类任务，这增加了延迟、VRAM占用和操作复杂性。作者希望复用LLM服务已经支付的计算成本，在生成的前向传播过程中同时完成分类任务。

Method: 提出基于表示选择的分类框架，而非固定token或层。引入两阶段聚合器：(1) 在每层内汇总token，(2) 跨层聚合形成单一表示进行分类。具体实现包括直接池化、100K参数评分注意力门控和最多35M可训练参数的下采样多头自注意力探针。

Result: 在安全和情感基准测试中，该方法优于仅使用logit的复用方法（如MULI），与更大的任务特定基线模型竞争性相当，同时保持了接近服务延迟，避免了独立防护模型管道的VRAM和延迟成本。

Conclusion: 通过复用LLM隐藏状态训练轻量级探针进行同步分类，能够在保持低延迟的同时有效替代独立的分类模型，为生产级LLM系统提供了更高效的分类解决方案。

Abstract: Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.

</details>


### [99] [OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference](https://arxiv.org/abs/2601.13300)
*Yow-Fu Liou,Yu-Chien Tang,Yu-Hsiang Liu,An-Zi Yen*

Main category: cs.CL

TL;DR: 本文提出选项注入攻击方法，通过在多项选择题中插入包含误导性指令的额外选项，系统评估大语言模型对指令干扰的脆弱性，并构建了包含3000个问题的OI-Bench基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大语言模型的决策不仅受界面因素影响，还会被社会线索、框架效应和指令等定向信号所干扰。需要系统评估模型对这类指令干扰的鲁棒性，特别是在标准化选择界面中的表现。

Method: 提出选项注入攻击方法，在多项选择题回答界面中增加包含误导性指令的额外选项。构建OI-Bench基准测试，包含3000个问题，涵盖知识、推理和常识任务，使用16种指令类型（社会遵从、奖励框架、威胁框架、指令干扰）。评估12个大语言模型，分析攻击成功率、行为响应，并研究从推理时提示到训练后对齐等多种缓解策略。

Result: 实验结果显示大语言模型存在显著脆弱性，不同模型表现出异质性鲁棒性。选项注入攻击能有效揭示模型对指令干扰的敏感性，为系统评估模型在基于选择的界面中对指令干扰的鲁棒性提供了有效工具。

Conclusion: 选项注入方法结合了选择界面操纵和基于指令的干扰，能够系统评估模型易感性。OI-Bench基准测试支持对大语言模型在基于选择的界面中对指令干扰的鲁棒性进行更系统评估，揭示了模型安全性和鲁棒性方面的重要问题。

Abstract: Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.

</details>


### [100] [Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse](https://arxiv.org/abs/2601.13317)
*Samantha Sudhoff,Pranav Perumal,Zhaoqing Wu,Tunazzina Islam*

Main category: cs.CL

TL;DR: 提出一个可解释的主题发现框架，比较Meta付费广告和Bluesky公共帖子中的气候话语差异，发现平台激励机制影响叙事结构、立场对齐和时间响应性


<details>
  <summary>Details</summary>
Motivation: 现有研究通常孤立分析不同平台的气候话语，限制了区分机构信息与公众表达的能力。付费广告生态系统激励定向战略说服，而公共社交媒体平台主要承载有机的用户驱动话语，需要比较分析这两种结构不同的环境

Method: 提出可解释的端到端主题发现和分配框架：通过语义相似性聚类文本，利用大语言模型生成简洁、人类可解释的主题标签。评估诱导主题质量（与传统主题建模基线比较，使用人类判断和LLM评估器），通过下游立场预测和主题引导检索任务验证语义连贯性。应用于2024年7月至2025年9月的Meta付费广告和Bluesky公共帖子数据

Result: 平台层面的激励机制反映在气候叙事的主题结构、立场对齐和时间响应性中。付费气候信息与公共气候话语存在系统性差异，主题流行度在重大政治事件周围发生变化

Conclusion: 虽然实证分析聚焦气候传播，但提出的框架旨在支持跨异构传播环境的比较叙事分析。平台激励机制塑造了气候话语的特征，区分付费战略说服与有机公众表达对于理解气候传播动态至关重要

Abstract: Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.

</details>


### [101] [Reducing Tokenization Premiums for Low-Resource Languages](https://arxiv.org/abs/2601.13328)
*Geoffrey Churchill,Steven Skiena*

Main category: cs.CL

TL;DR: 分析流行语言模型的tokenizer设计及其对低资源语言的影响，提出通过后处理增加词汇表来减少低资源语言的tokenization premium


<details>
  <summary>Details</summary>
Motivation: 低资源语言在现代语言模型中存在显著的tokenization premium（相对于英语需要更多token编码相同内容），这导致API成本增加、能耗上升和有效上下文窗口减少

Method: 分析10个流行语言模型的tokenizer设计，提出后处理机制：通过向预训练模型的词汇表中添加多字符组合的单一token来压缩tokenization premium

Result: 在12种低资源语言上应用该方法，证明原始输入和压缩输入在Llama 3.2 1B模型中具有相似的最终隐藏状态

Conclusion: 通过后处理增加词汇表可以有效减少低资源语言的tokenization premium，同时保持模型性能，为解决低资源语言在语言模型中的效率问题提供了实用方案

Abstract: Relative to English, low-resource languages suffer from substantial tokenization premiums in modern LMs, meaning that it generally requires several times as many tokens to encode a sentence in a low-resource language than to encode the analogous sentence in English. This tokenization premium results in increased API and energy costs and reduced effective context windows for these languages. In this paper we analyze the tokenizers of ten popular LMs to better understand their designs and per-language tokenization premiums. We also propose a mechanism to reduce tokenization premiums in pre-trained models, by post-hoc additions to the token vocabulary that coalesce multi-token characters into single tokens. We apply this methodology to 12 low-resource languages, demonstrating that the original and compressed inputs often have similar last hidden states when run through the Llama 3.2 1B model.

</details>


### [102] [RegCheck: A tool for automating comparisons between study registrations and papers](https://arxiv.org/abs/2601.13330)
*Jamie Cummins,Beth Clarke,Ian Hussey,Malte Elson*

Main category: cs.CL

TL;DR: RegCheck是一个模块化的LLM辅助工具，帮助研究人员、审稿人和编辑跨科学领域比较研究注册与对应论文，保持人类专业判断在循环中，并生成可共享的报告。


<details>
  <summary>Details</summary>
Motivation: 尽管预先指定研究活动（注册）对科学透明度和严谨性有益，但注册经常未被检查，降低了其有效性。手动检查注册与论文耗时耗力，需要跨格式仔细阅读和跨领域专业知识。AI的出现为促进这一活动提供了新可能。

Method: RegCheck是一个模块化的LLM辅助工具，设计用于比较研究注册与对应论文。它保持人类专业判断在循环中：(i) 用户决定比较哪些特征；(ii) 向用户呈现每个特征最相关的文本，促进而非替代人类差异判断。工具生成带有唯一RegCheck ID的可共享报告，便于其他用户验证。

Result: RegCheck被设计为跨科学领域、注册和出版格式的适应性工具。论文概述了其动机、工作流程和设计原则，并讨论了其作为可扩展基础设施的潜力，提供了一个示例用例。

Conclusion: RegCheck作为LLM辅助工具，通过促进而非替代人类判断，帮助解决研究注册检查的挑战，为可重复科学提供了可扩展的基础设施。

Abstract: Across the social and medical sciences, researchers recognize that specifying planned research activities (i.e., 'registration') prior to the commencement of research has benefits for both the transparency and rigour of science. Despite this, evidence suggests that study registrations frequently go unexamined, minimizing their effectiveness. In a way this is no surprise: manually checking registrations against papers is labour- and time-intensive, requiring careful reading across formats and expertise across domains. The advent of AI unlocks new possibilities in facilitating this activity. We present RegCheck, a modular LLM-assisted tool designed to help researchers, reviewers, and editors from across scientific disciplines compare study registrations with their corresponding papers. Importantly, RegCheck keeps human expertise and judgement in the loop by (i) ensuring that users are the ones who determine which features should be compared, and (ii) presenting the most relevant text associated with each feature to the user, facilitating (rather than replacing) human discrepancy judgements. RegCheck also generates shareable reports with unique RegCheck IDs, enabling them to be easily shared and verified by other users. RegCheck is designed to be adaptable across scientific domains, as well as registration and publication formats. In this paper we provide an overview of the motivation, workflow, and design principles of RegCheck, and we discuss its potential as an extensible infrastructure for reproducible science with an example use case.

</details>


### [103] [AfroScope: A Framework for Studying the Linguistic Landscape of Africa](https://arxiv.org/abs/2601.13346)
*Sang Yun Kwon,AbdelRahim Elmadany,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: AfroScope是一个统一的非洲语言识别框架，包含覆盖713种非洲语言的数据集和模型套件，通过分层分类方法改进对高度相似语言的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别方法在非洲语言覆盖范围有限，且难以区分密切相关的语言变体，这影响了下游NLP应用的可靠性。需要开发更全面的非洲语言识别系统来支持非洲语言景观的大规模测量。

Method: 提出AfroScope统一框架，包括AfroScope-Data（覆盖713种非洲语言的数据集）和AfroScope-Models（模型套件）。针对高度相似的语言，采用分层分类方法，利用专门针对29种密切相关或地理邻近语言的Mirror-Serengeti嵌入模型。

Result: 分层分类方法在高度相似语言子集上比最佳基础模型提高了4.55的宏观F1分数。分析了跨语言迁移和领域效应，为构建稳健的非洲语言识别系统提供指导。

Conclusion: AfroScope框架显著扩展了非洲语言识别覆盖范围，改进了对密切相关的语言变体的区分能力，为大规模测量非洲数字文本中的语言景观提供了使能技术。数据集和模型已公开发布。

Abstract: Language Identification (LID) is the task of determining the language of a given text and is a fundamental preprocessing step that affects the reliability of downstream NLP applications. While recent work has expanded LID coverage for African languages, existing approaches remain limited in (i) the number of supported languages and (ii) their ability to make fine-grained distinctions among closely related varieties. We introduce AfroScope, a unified framework for African LID that includes AfroScope-Data, a dataset covering 713 African languages, and AfroScope-Models, a suite of strong LID models with broad language coverage. To better distinguish highly confusable languages, we propose a hierarchical classification approach that leverages Mirror-Serengeti, a specialized embedding model targeting 29 closely related or geographically proximate languages. This approach improves macro F1 by 4.55 on this confusable subset compared to our best base model. Finally, we analyze cross linguistic transfer and domain effects, offering guidance for building robust African LID systems. We position African LID as an enabling technology for large scale measurement of Africas linguistic landscape in digital text and release AfroScope-Data and AfroScope-Models publicly.

</details>


### [104] [LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction](https://arxiv.org/abs/2601.13352)
*Yuxing Lu,J. Ben Tamo,Weichen Zhao,Nan Sun,Yishan Zhong,Wenqi Shi,Jinzhuo Wang,May D. Wang*

Main category: cs.CL

TL;DR: LLM-as-RNN将冻结的LLM转换为循环预测器，通过自然语言记忆实现可更新的隐藏状态，在固定token预算下进行在线学习，无需参数更新


<details>
  <summary>Details</summary>
Motivation: 标准LLM推理使用不可变的上下文历史，一旦在生成步骤t出错，模型缺乏可更新的记忆机制来改进步骤t+1的预测。需要一种无需参数更新的在线学习方法。

Method: 提出LLM-as-RNN框架，将冻结LLM转变为循环预测器，隐藏状态表示为自然语言记忆（结构化系统提示摘要）。通过反馈驱动的文本重写在每个时间步更新状态，实现无参数更新的学习。

Result: 在医疗、气象和金融三个序列基准测试中，LLM-as-RNN显著优于零样本、完整历史和MemPrompt基线，平均预测准确率提高6.5%，同时产生可解释的人类可读学习轨迹。

Conclusion: LLM-as-RNN通过自然语言记忆实现可更新的隐藏状态，使冻结LLM能够进行在线学习，纠正错误并保留任务相关模式，在固定token预算下优于传统上下文累积方法。

Abstract: Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.

</details>


### [105] [Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection](https://arxiv.org/abs/2601.13359)
*Asen Dotsinski,Panagiotis Eustratiadis*

Main category: cs.CL

TL;DR: 提出了一种名为"sockpuppetting"的简单越狱方法，通过在模型输出开头插入接受序列（如"Sure, here is how to..."）来攻击开源大语言模型，无需优化且代码量极少。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型能力增强，保护它们免受恶意提示攻击和理解可能的攻击向量变得日益重要。现有自动化越狱方法如GCG需要大量计算资源和专业知识，需要更简单有效的攻击方法。

Method: 提出"sockpuppetting"方法：在模型输出开头插入接受序列（如"Sure, here is how to..."），然后让模型完成响应。该方法只需单行代码，无需优化。还探索了混合方法：在助手消息块内优化对抗后缀而非用户提示。

Result: sockpuppetting在Qwen3-8B上比GCG攻击成功率提高80%（逐提示比较）。混合方法在Llama-3.1-8B上比GCG攻击成功率提高64%（提示无关设置）。

Conclusion: sockpuppetting是一种低成本有效攻击方法，可供非专业攻击者使用，突显了开源模型需要防御输出前缀注入攻击的重要性。

Abstract: As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce "sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., "Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.

</details>


### [106] [Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.13368)
*Zhenjiang Mao,Anirudhh Venkat*

Main category: cs.CL

TL;DR: 提出一种结合步骤间注意力分析和隐藏置信度机制的新方法，用于评估大语言模型推理过程中的不确定性，解决传统方法忽略时间置信度传播导致整体置信度高估的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理任务上表现优异，但评估答案不确定性仍具挑战性。现有方法分析长推理序列时，通常通过过滤无关标记和检查邻近标记/句子间的潜在连接，但忽略了置信度的时间传播，导致早期步骤置信度很低时整体置信度仍被高估。

Method: 提出新颖方法：1) 结合步骤间注意力分析跨步骤的语义相关性；2) 针对长序列响应引入隐藏置信度机制，保留历史置信度信息，并与逐步置信度结合产生更准确的总体估计。

Result: 在GAOKAO数学基准和CLadder因果推理数据集上，使用主流开源大语言模型进行评估。该方法在预测质量和校准之间实现了更优平衡，在负对数似然和期望校准误差指标上均表现出色，优于现有最先进方法。

Conclusion: 通过分析步骤间注意力并引入隐藏置信度机制，能够更准确地评估大语言模型推理过程中的不确定性，有效解决置信度时间传播被忽略的问题，提升模型输出的可靠性和安全性。

Abstract: As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.

</details>


### [107] [Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction](https://arxiv.org/abs/2601.13388)
*Sasha Ronaghi,Prerit Choudhary,David H Rehkopf,Bryant Lin*

Main category: cs.CL

TL;DR: 利用大型语言模型从糖尿病患者生活故事中提取结构化社会健康决定因素，并评估其对糖尿病控制的预测价值


<details>
  <summary>Details</summary>
Motivation: 社会健康决定因素在2型糖尿病管理中至关重要，但电子健康记录和风险预测模型中往往缺乏这些信息。传统的结构化筛查工具缺乏灵活性，无法捕捉患者经历的复杂性。需要一种方法将非结构化患者叙述转化为可操作的临床见解。

Method: 收集65名65岁以上2型糖尿病患者的非结构化访谈，使用检索增强生成的大型语言模型分析叙述，生成定性临床摘要和结构化SDOH评分。将这些评分与传统实验室生物标志物结合，输入线性和树基机器学习模型进行风险预测。同时评估LLM直接从访谈文本预测糖尿病控制水平的能力。

Result: LLM能够从患者生活故事中提取结构化SDOH信息，并将其转化为临床可用的见解。LLM仅从访谈文本预测糖尿病控制水平（低、中、高）的准确率达到60%。结构化SDOH评分可与传统生物标志物结合，增强风险预测模型的性能。

Conclusion: 大型语言模型能够将非结构化SDOH相关数据转化为结构化见解，为增强临床风险模型和决策制定提供可扩展的方法。这种方法弥补了传统结构化工具的不足，使患者叙述能够整合到常规风险预测工作流程中。

Abstract: Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.

</details>


### [108] [Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks](https://arxiv.org/abs/2601.13392)
*Shlok Shelat,Jay Raval,Souvik Roy,Manas Gaur*

Main category: cs.CL

TL;DR: LLMs在确定性有限自动机(DFA)构建任务上表现良好，但在未见问题中准确率显著下降，暴露了形式推理能力的根本性差距


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在形式语言任务中的表现是否反映真正的符号推理能力，还是仅仅对熟悉结构的模式匹配，通过DFA构建任务来评估其形式推理能力

Method: 引入一个DFA构建基准测试，包含事实知识问题、公开来源的已见构造问题，以及两种未见问题：手工制作的多约束交互实例和通过Arden定理系统生成的问题；评估多种提示策略（直接、思维链、思维树）和三阶段提示协议

Result: 模型在事实问题上达到完美准确率，在已见任务上达到84-90%准确率，但在未见问题上准确率急剧下降30-64%；错误源于对语言约束的系统性误解、Kleene星号语义的错误处理以及全局一致性的失败；三阶段提示协议能修正浅层错误但无法可靠解决全局不一致或结构缺陷的自动机

Conclusion: LLMs在生成语法上合理的DFA与语义正确的形式推理能力之间存在根本性差距，错误在所有提示策略中持续存在，表明当前LLMs的形式推理能力有限

Abstract: Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.

</details>


### [109] [Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models](https://arxiv.org/abs/2601.13433)
*Priyanka Mary Mammen,Emil Joswin,Shankar Venkitachalam*

Main category: cs.CL

TL;DR: 语言模型在推理任务中会受到建议和认可的影响，但认可来源的可信度影响尚未充分研究。研究发现模型对高权威来源的错误认可表现出系统性偏见，导致准确性下降和错误答案置信度增加。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明语言模型在推理任务上的表现会受到建议、提示和认可的影响，但认可来源可信度的影响尚未得到充分探索。本研究旨在探究语言模型是否基于认可提供者的感知专业知识表现出系统性偏见。

Method: 在数学、法律和医学推理4个数据集上评估了11个模型，使用代表每个领域四个专业知识水平的人物角色。通过不同权威程度的认可来源测试模型对错误/误导性认可的反应。

Result: 结果显示，随着来源专业知识的增加，模型对错误/误导性认可的敏感性增强。高权威来源不仅导致准确性下降，还增加了对错误答案的置信度。这种权威偏见在模型中具有机制性编码。

Conclusion: 语言模型存在权威偏见，但可以通过引导模型远离这种偏见来改善性能，即使在专家给出误导性认可的情况下也能提高表现。这揭示了模型对权威来源的系统性偏见及其可修正性。

Abstract: Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.

</details>


### [110] [MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization](https://arxiv.org/abs/2601.13437)
*Adriana-Valentina Costache,Daria-Nicoleta Dragomir,Silviu-Florin Gheorghe,Eduard Poesina,Paul Irofti,Radu Tudor Ionescu*

Main category: cs.CL

TL;DR: 该论文提出了首个多语言开放集学习与发现（MOSLD）基准，用于文本主题分类，包含12种语言的96万数据样本，并提出了一个集成多阶段的新框架来持续发现和学习新类别。


<details>
  <summary>Details</summary>
Motivation: 开放集学习与发现（OSLD）是一个具有挑战性的机器学习任务，其中测试时可能出现来自新（未知）类别的样本。虽然零样本学习在文本分类中已被广泛研究，但开放集学习与发现对于文本领域来说是一个相对较新的设置，缺乏相应的基准。

Method: 1）通过重新整理现有数据集和收集新闻领域新数据样本，构建了首个多语言开放集学习与发现基准（MOSLD），包含12种语言的960K数据样本；2）提出了一个新颖的OSLD框架，集成多个阶段来持续发现和学习新类别。

Result: 评估了包括作者提出的模型在内的多种语言模型，获得了可作为未来工作参考的结果。基准已公开发布在GitHub上。

Conclusion: 该研究填补了文本领域开放集学习与发现基准的空白，为未来研究提供了多语言基准和参考结果，推动了OSLD在文本分类领域的发展。

Abstract: Open-set learning and discovery (OSLD) is a challenging machine learning task in which samples from new (unknown) classes can appear at test time. It can be seen as a generalization of zero-shot learning, where the new classes are not known a priori, hence involving the active discovery of new classes. While zero-shot learning has been extensively studied in text classification, especially with the emergence of pre-trained language models, open-set learning and discovery is a comparatively new setup for the text domain. To this end, we introduce the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. To construct the benchmark, we (i) rearrange existing datasets and (ii) collect new data samples from the news domain. Moreover, we propose a novel framework for the OSLD task, which integrates multiple stages to continuously discover and learn new classes. We evaluate several language models, including our own, to obtain results that can be used as reference for future work. We release our benchmark at https://github.com/Adriana19Valentina/MOSLD-Bench.

</details>


### [111] [Anonpsy: A Graph-Based Framework for Structure-Preserving De-identification of Psychiatric Narratives](https://arxiv.org/abs/2601.13503)
*Kyung Ho Lim,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: Anonpsy是一个用于精神病学叙事去识别的框架，通过图引导的语义重写方法，在保持临床诊断保真度的同时降低再识别风险。


<details>
  <summary>Details</summary>
Motivation: 精神病学叙事不仅包含明确的身份标识符，还通过嵌入临床结构中的独特生活事件编码患者身份。现有的去识别方法（如PHI掩码和基于LLM的合成重写）在文本层面操作，对哪些语义元素被保留或修改的控制有限。

Method: Anonpsy将去识别任务重新定义为图引导的语义重写：1) 将每个叙事转换为编码临床实体、时间锚点和类型关系的语义图；2) 应用图约束的扰动，修改识别性上下文同时保留临床必需结构；3) 通过图条件LLM生成重新生成文本。

Result: 在90个临床医生撰写的精神病学案例叙事评估中，Anonpsy在保持诊断保真度的同时，在专家、语义和GPT-5基评估下实现了持续低的再识别风险。与强大的仅LLM重写基线相比，Anonpsy产生了显著更低的语义相似性和可识别性。

Conclusion: 显式结构表示与约束生成相结合，为精神病学叙事的去识别提供了有效方法。图引导的语义重写框架在保护患者隐私的同时保持了临床实用性。

Abstract: Psychiatric narratives encode patient identity not only through explicit identifiers but also through idiosyncratic life events embedded in their clinical structure. Existing de-identification approaches, including PHI masking and LLM-based synthetic rewriting, operate at the text level and offer limited control over which semantic elements are preserved or altered. We introduce Anonpsy, a de-identification framework that reformulates the task as graph-guided semantic rewriting. Anonpsy (1) converts each narrative into a semantic graph encoding clinical entities, temporal anchors, and typed relations; (2) applies graph-constrained perturbations that modify identifying context while preserving clinically essential structure; and (3) regenerates text via graph-conditioned LLM generation. Evaluated on 90 clinician-authored psychiatric case narratives, Anonpsy preserves diagnostic fidelity while achieving consistently low re-identification risk under expert, semantic, and GPT-5-based evaluations. Compared with a strong LLM-only rewriting baseline, Anonpsy yields substantially lower semantic similarity and identifiability. These results demonstrate that explicit structural representations combined with constrained generation provide an effective approach to de-identification for psychiatric narratives.

</details>


### [112] [When Wording Steers the Evaluation: Framing Bias in LLM judges](https://arxiv.org/abs/2601.13537)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Minwoo Lee,Kyomin Jung*

Main category: cs.CL

TL;DR: 研究发现大语言模型在评估任务中存在显著的框架效应，即提示词的微小变化会显著影响模型判断，这暴露了当前LLM评估系统的结构性偏差


<details>
  <summary>Details</summary>
Motivation: 大语言模型的回答会因提示词措辞而变化，但这种框架偏差对LLM评估任务的影响尚未充分研究。心理学中的框架效应表明，问题的表述方式会影响决策，作者希望系统研究这种效应在LLM评估中的表现。

Method: 采用心理学中的框架效应概念，设计对称提示词（谓词肯定和谓词否定结构），在四个高风险评估任务中系统研究提示词框架如何影响模型判断。使用14个不同的LLM作为评估者进行分析。

Result: 研究发现框架效应显著影响模型输出，不同模型家族表现出明显的接受或拒绝倾向。这表明框架偏差是当前LLM评估系统的结构性特征。

Conclusion: 框架偏差是LLM评估系统的重要问题，需要开发框架感知的评估协议来确保评估的稳定性和公正性。

Abstract: Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.

</details>


### [113] [HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations](https://arxiv.org/abs/2601.13547)
*Yujia Hu,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: HateXScore：一个用于评估仇恨言论检测模型解释质量的四组件度量套件，旨在揭示标准指标无法发现的解释性失败和标注不一致问题


<details>
  <summary>Details</summary>
Motivation: 当前仇恨言论检测评估框架很少评估文本被判定为仇恨言论的原因，缺乏对模型解释质量的系统性评估方法，需要诊断工具来揭示标准指标无法发现的解释性失败和标注不一致问题

Method: 提出HateXScore四组件度量套件：1)结论明确性评估；2)引用跨度的忠实性和因果基础；3)受保护群体识别（可配置策略）；4)这些元素间的逻辑一致性。在六个不同的仇恨言论数据集上进行评估

Result: HateXScore作为诊断补充工具，能够揭示准确率或F1分数等标准指标无法发现的解释性失败和标注不一致问题。人工评估显示与HateXScore有很强的一致性，验证了其作为可信透明内容审核实用工具的有效性

Conclusion: HateXScore是一个有效的度量套件，用于评估仇恨言论检测模型的解释质量，为可信和透明的审核系统提供了实用工具，能够诊断标准指标无法发现的解释性问题

Abstract: Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.
  \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}

</details>


### [114] [Comparing Without Saying: A Dataset and Benchmark for Implicit Comparative Opinion Mining from Same-User Reviews](https://arxiv.org/abs/2601.13575)
*Thanh-Lam T. Nguyen,Ngoc-Quang Le,Quoc-Trung Phu,Thi-Phuong Le,Ngoc-Huyen Pham,Phuong-Nguyen Nguyen,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: SUDO是一个用于隐式比较观点挖掘的新数据集，包含4,150个标注的评论对，支持在没有显式比较线索的情况下推断用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有比较观点挖掘研究主要关注显式比较表达，但在真实评论中不常见，而隐式比较（用户在不同评论中表达偏好）尚未得到充分探索。

Method: 提出了SUDO数据集，包含4,150个标注的评论对（15,191个句子），采用双层结构捕捉方面级提及和评论级偏好。使用两种基线架构进行基准测试：传统机器学习方法和语言模型方法。

Result: 实验结果表明，语言模型基线优于传统机器学习方法，但整体性能仍然中等，揭示了任务的固有难度。

Conclusion: SUDO是一个具有挑战性和价值的基准数据集，为未来隐式比较观点挖掘研究提供了重要资源。

Abstract: Existing studies on comparative opinion mining have mainly focused on explicit comparative expressions, which are uncommon in real-world reviews. This leaves implicit comparisons - here users express preferences across separate reviews - largely underexplored. We introduce SUDO, a novel dataset for implicit comparative opinion mining from same-user reviews, allowing reliable inference of user preferences even without explicit comparative cues. SUDO comprises 4,150 annotated review pairs (15,191 sentences) with a bi-level structure capturing aspect-level mentions and review-level preferences. We benchmark this task using two baseline architectures: traditional machine learning- and language model-based baselines. Experimental results show that while the latter outperforms the former, overall performance remains moderate, revealing the inherent difficulty of the task and establishing SUDO as a challenging and valuable benchmark for future research.

</details>


### [115] [CauScientist: Teaching LLMs to Respect Data for Causal Discovery](https://arxiv.org/abs/2601.13614)
*Bo Peng,Sirui Chen,Lei Xu,Chaochao Lu*

Main category: cs.CL

TL;DR: CauScientist是一个结合LLM假设生成与概率统计验证的因果发现框架，通过混合初始化、迭代优化和错误记忆机制，显著提升因果图构建性能


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法存在局限性：纯数据驱动方法受统计不可区分性和建模假设影响，而基于LLM的方法要么忽略统计证据，要么引入未经验证的先验知识可能误导结果

Method: 提出CauScientist协作框架，将LLM作为假设生成的"数据科学家"，概率统计作为严格"验证者"。采用混合初始化选择优质起始图，通过LLM提议修改并由统计标准验证进行迭代优化，维护错误记忆引导高效搜索空间

Result: 实验表明CauScientist显著优于纯数据驱动基线，F1分数提升达53.8%，召回率从35.0%提升至100.0%。在37节点图上，相比Qwen3-32B将结构汉明距离(SHD)降低44.0%

Conclusion: CauScientist通过LLM与统计方法的协同工作，有效解决了现有因果发现方法的局限性，在复杂图结构上表现出优越性能，为可靠因果发现提供了新范式

Abstract: Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating "data scientists" with probabilistic statistics as rigorous "verifiers". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.

</details>


### [116] [Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models](https://arxiv.org/abs/2601.13630)
*Zhaopeng Zhang,Pengcheng Sun,Lan Zhang,Chen Tang,Jiewei Lai,Yunhao Wang,Hui Jin*

Main category: cs.CL

TL;DR: AAAC是一种无需训练的多类权限控制框架，利用激活空间几何规律，通过权限锚点重定向查询激活，显著降低权限违规率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识库问答中可能无意中超越用户权限范围泄露敏感信息，难以满足细粒度访问控制需求

Method: 基于激活空间几何规律性，提出AAAC框架：从离线样本集构建权限锚点库，通过多锚点引导机制将查询激活重定向到授权区域

Result: 在三个LLM家族上实验显示，AAAC将权限违规率降低高达86.5%，提示攻击成功率降低90.7%，推理开销小且提升响应可用性

Conclusion: AAAC通过激活空间锚点控制实现了有效的多类权限管理，无需微调即可显著减少权限违规，为知识库QA的安全部署提供解决方案

Abstract: Large language models (LLMs) are increasingly deployed over knowledge bases for efficient knowledge retrieval and question answering. However, LLMs can inadvertently answer beyond a user's permission scope, leaking sensitive content, thus making it difficult to deploy knowledge-base QA under fine-grained access control requirements. In this work, we identify a geometric regularity in intermediate activations: for the same query, representations induced by different permission scopes cluster distinctly and are readily separable. Building on this separability, we propose Activation-space Anchored Access Control (AAAC), a training-free framework for multi-class permission control. AAAC constructs an anchor bank, with one permission anchor per class, from a small offline sample set and requires no fine-tuning. At inference time, a multi-anchor steering mechanism redirects each query's activations toward the anchor-defined authorized region associated with the current user, thereby suppressing over-privileged generations by design. Finally, extensive experiments across three LLM families demonstrate that AAAC reduces permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while improving response usability with minor inference overhead compared to baselines.

</details>


### [117] [Towards Token-Level Text Anomaly Detection](https://arxiv.org/abs/2601.13644)
*Yang Cao,Bicheng Yu,Sikun Yang,Ming Liu,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出token级异常检测新范式，实现文本内部异常的精确定位，超越传统文档级检测方法


<details>
  <summary>Details</summary>
Motivation: 现有文本异常检测方法（如垃圾邮件过滤、假新闻检测）仅限于文档级分析，无法识别文本中哪些具体部分存在异常，需要更细粒度的检测能力

Method: 1. 形式化定义文档级和token级文本异常；2. 提出统一的多层级检测框架；3. 收集并标注三个包含token级标签的基准数据集（垃圾邮件、评论、语法错误）

Result: 实验结果表明，该框架在性能上优于其他6个基线方法，为文本异常精确定位开辟了新可能性

Conclusion: 提出的token级异常检测范式填补了细粒度文本异常定位的空白，相关代码和数据已开源，推动了该研究方向的发展

Abstract: Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on https://github.com/charles-cao/TokenCore.

</details>


### [118] [Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge](https://arxiv.org/abs/2601.13649)
*Xiaolin Zhou,Zheng Luo,Yicheng Gao,Qixuan Chen,Xiyang Hu,Yue Zhao,Ruishan Liu*

Main category: cs.CL

TL;DR: 该论文研究LLM作为评判者时的语言偏见问题，发现同语言评判中存在语言家族间的性能差异，跨语言评判中模型偏好英语答案，且语言偏见不能完全由困惑度偏见解释。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM作为评判者的应用日益广泛，但先前研究表明LLM评判存在多种偏见，其中语言偏见尤为突出。该研究旨在深入探究LLM作为评判者在成对比较中的两种语言偏见类型，并分析其与困惑度偏见的关系。

Method: 研究采用实验方法分析LLM作为评判者的语言偏见：1) 同语言评判中，比较不同语言家族间的性能差异；2) 跨语言评判中，分析模型对不同语言答案的偏好；3) 通过相关性分析探究语言偏见与困惑度偏见的关系。

Result: 研究发现：1) 同语言评判中，欧洲语言表现优于非洲语言，且文化相关主题中偏见更明显；2) 跨语言评判中，大多数模型偏好英语答案，且答案语言比问题语言对偏好影响更大；3) 语言偏见与困惑度偏见仅有轻微相关性，不能完全由困惑度解释。

Conclusion: LLM作为评判者存在显著的语言偏见，包括同语言评判中的性能差异和跨语言评判中的语言偏好。这些偏见不能完全归因于困惑度偏见，表明需要更深入理解LLM评判机制并开发缓解策略。

Abstract: Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.

</details>


### [119] [Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation](https://arxiv.org/abs/2601.13658)
*Arthur Amalvy,Hen-Hsen Huang*

Main category: cs.CL

TL;DR: 提出基于未来事实的合成评估数据集，解决时序知识图谱抽取任务中数据稀缺和评估数据污染问题


<details>
  <summary>Details</summary>
Motivation: 时序知识图谱抽取任务中，现有数据集稀缺且存在评估数据污染问题，可能导致LLM性能被高估

Method: 采用两步法：1) 时序知识图谱预测生成未来四元组；2) LLM进行四元组到文本生成，创建语义对齐的文本描述

Result: 构建了包含4.2K未来四元组及对应文本描述的数据集，实验显示LLM在新数据集上性能下降，验证了数据污染问题

Conclusion: 提出的合成评估数据集消除了数据污染，为时序知识图谱抽取提供了长期、无污染的基准测试框架

Abstract: The automatic extraction of information is important for populating large web knowledge bases such as Wikidata. The temporal version of that task, temporal knowledge graph extraction (TKGE), involves extracting temporally grounded facts from text, represented as semantic quadruples (subject, relation, object, timestamp). Many recent systems take advantage of large language models (LLMs), which are becoming a new cornerstone of the web due to their performance on many tasks across the natural language processing (NLP) field. Despite the importance of TKGE, existing datasets for training and evaluation remain scarce, and contamination of evaluation data is an unaddressed issue, potentially inflating LLMs' perceived performance due to overlaps between training and evaluation sets. To mitigate these challenges, we propose a novel synthetic evaluation dataset constructed from predicted future, previously unseen temporal facts, thereby eliminating contamination and enabling robust and unbiased benchmarking. Our dataset creation involves a two-step approach: (1) Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema; (2) LLMs perform quadruple-to-text generation, creating semantically aligned textual descriptions. We benchmark Extract, Define and Canonicalize (EDC), a state-of-the-art LLM-based extraction framework, demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts. We publicly release our dataset consisting of 4.2K future quadruples and corresponding textual descriptions, along with the generation methodology, enabling continuous creation of unlimited future temporal datasets to serve as long-term, contamination-free benchmarks for TKGE.

</details>


### [120] [Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.13659)
*Chunlei Meng,Ziyang Zhou,Lucas He,Xiaojing Du,Chun Ouyang,Zhongxue Gan*

Main category: cs.CL

TL;DR: TSDA通过时空解耦-对齐-重耦合框架解决多模态情感分析中的时空异质性问题，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法基于模态不变/特定分解或复杂融合，但仍依赖时空混合建模，忽略了时空异质性，导致时空信息不对称和性能受限

Method: 提出TSDA框架：1) 将每个模态显式解耦为时间动态和空间结构上下文；2) 因子一致跨模态对齐：时间特征仅与跨模态时间特征对齐，空间特征仅与空间特征对齐；3) 因子特定监督和去相关正则化减少跨因子泄漏；4) 门控重耦合模块将对齐后的流重耦合用于任务

Result: 广泛实验表明TSDA优于基线方法，消融分析证实了设计的必要性和可解释性

Conclusion: 通过显式时空解耦和对齐策略，TSDA有效解决了多模态情感分析中的时空异质性问题，实现了性能提升

Abstract: Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.

</details>


### [121] [CommunityBench: Benchmarking Community-Level Alignment across Diverse Groups and Tasks](https://arxiv.org/abs/2601.13669)
*Jiayu Lin,Zhongyu Wei*

Main category: cs.CL

TL;DR: 该论文提出社区级对齐作为LLM对齐的中间路径，构建了首个大规模社区级对齐评估基准CommunityBench，发现当前LLM在建模社区特定偏好方面能力有限，但社区级对齐为可扩展的多元对齐提供了有前景的方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐策略存在两个极端：一是假设普世价值集的一刀切方法，这会边缘化少数群体规范；二是为每个个体定制模型的个体级方法，成本过高。作者认识到人类社会由具有高度组内价值对齐的社会集群组成，因此提出社区级对齐作为中间方案。

Method: 引入CommunityBench——首个大规模社区级对齐评估基准，基于共同身份和共同纽带理论构建了四个任务。使用该基准对各种基础模型进行全面评估，并研究社区级对齐在促进个体建模方面的潜力。

Result: 评估显示当前LLM在建模社区特定偏好方面能力有限。同时发现社区级对齐能够促进个体建模，为可扩展的多元对齐提供了有前景的方向。

Conclusion: 社区级对齐是LLM对齐的有效中间路径，CommunityBench为评估这一能力提供了重要基准。虽然当前模型在社区级对齐方面表现有限，但这一方向为实现可扩展且尊重多元价值的对齐提供了有前景的解决方案。

Abstract: Large language models (LLMs) alignment ensures model behaviors reflect human value. Existing alignment strategies primarily follow two paths: one assumes a universal value set for a unified goal (i.e., one-size-fits-all), while the other treats every individual as unique to customize models (i.e., individual-level). However, assuming a monolithic value space marginalizes minority norms, while tailoring individual models is prohibitively expensive. Recognizing that human society is organized into social clusters with high intra-group value alignment, we propose community-level alignment as a "middle ground". Practically, we introduce CommunityBench, the first large-scale benchmark for community-level alignment evaluation, featuring four tasks grounded in Common Identity and Common Bond theory. With CommunityBench, we conduct a comprehensive evaluation of various foundation models on CommunityBench, revealing that current LLMs exhibit limited capacity to model community-specific preferences. Furthermore, we investigate the potential of community-level alignment in facilitating individual modeling, providing a promising direction for scalable and pluralistic alignment.

</details>


### [122] [HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference](https://arxiv.org/abs/2601.13684)
*Zhiyuan Shi,Qibo Qiu,Feng Xue,Zhonglin Jiang,Li Yu,Jian Jiang,Xiaofei He,Wenxiao Wang*

Main category: cs.CL

TL;DR: HeteroCache是一个无需训练的KV缓存动态压缩框架，通过细粒度权重分配和分层存储机制解决长上下文LLM推理中的内存瓶颈问题，在224K上下文长度下实现3倍解码加速。


<details>
  <summary>Details</summary>
Motivation: KV缓存的线性内存增长是长上下文LLM推理的主要瓶颈。现有静态压缩方法无法保留全局重要信息，因为它们忽略了注意力漂移现象（token重要性动态变化）。虽然动态检索方法试图解决此问题，但通常存在粗粒度缓存策略和高I/O开销的问题。

Method: 基于两个关键洞察：1）注意力头表现出不同的时间异质性；2）同一层内头之间存在显著的空间冗余。HeteroCache根据稳定性和冗余性对头进行分类，采用细粒度权重分配策略，为注意力快速变化的头分配更大的缓存预算以捕捉上下文变化。同时采用分层存储机制，让代表性子集监控注意力漂移，触发异步按需从CPU检索上下文，有效隐藏I/O延迟。

Result: 实验表明，HeteroCache在多个长上下文基准测试中达到最先进性能，在224K上下文长度下相比原始模型实现高达3倍的解码加速。

Conclusion: HeteroCache通过细粒度权重分配和分层存储机制有效解决了KV缓存压缩中的注意力漂移和I/O开销问题，为长上下文LLM推理提供了高效的训练免费解决方案。

Abstract: The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\times$ compared to the original model in the 224K context. Our code will be open-source.

</details>


### [123] [Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning](https://arxiv.org/abs/2601.13690)
*Yue Guo,Fanfu Wang,Jianwei Lv,Xincheng Shi,Yuchen Li,Youya Wang,Yunsheng Zeng,Yujing Liu,Yunhao Qiao,Gen Li,Junfeng Wang,Bo Yuan*

Main category: cs.CL

TL;DR: 提出Dr. Assistant临床诊断模型，通过CDRD数据结构和两阶段训练（SFT+RL），提升LLM在临床诊断推理和问询方面的能力


<details>
  <summary>Details</summary>
Motivation: 传统临床决策支持系统维护成本高、泛化能力差，而现有大型语言模型虽然医学知识丰富，但在诊断推理和问询技能方面存在局限

Method: 1) 提出CDRD数据结构捕捉抽象临床推理逻辑及其构建流程；2) 开发Dr. Assistant模型，采用两阶段训练：监督微调（SFT）后接强化学习（RL）配合定制奖励函数；3) 建立评估诊断推理和问询的基准

Result: Dr. Assistant在实验中优于开源模型，与闭源模型性能相当，为临床诊断问询指导提供了有效解决方案

Conclusion: 通过CDRD数据结构和两阶段训练方法，Dr. Assistant成功提升了LLM在临床诊断推理和问询方面的能力，为医疗AI应用提供了新途径

Abstract: Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.

</details>


### [124] [OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens](https://arxiv.org/abs/2601.13695)
*Sifan Li,Hongkai Chen,Yujun Cai,Liyang Chen,Qingwen Ye,Yiwei Wang*

Main category: cs.CL

TL;DR: OptiSQL：直接从表格图像和自然语言问题生成可执行SQL的视觉驱动框架，使用紧凑光学标记减少输入标记数量


<details>
  <summary>Details</summary>
Motivation: 传统文本到SQL方法需要完全线性化的文本模式，产生大量标记开销，与现实中表格作为视觉元素出现在文档或网页中的场景不匹配。研究紧凑光学表示是否能作为可执行语义解析的高效接口。

Method: OptiSQL框架：1）使用OCR导向的视觉编码器将表格结构和内容压缩为少量光学标记；2）微调预训练解码器进行SQL生成，同时冻结编码器以隔离表示充分性；3）直接从表格图像和自然语言问题生成可执行SQL。

Result: 在可视化的Spider 2.0-Snow数据集上，OptiSQL在保持强大执行准确性的同时，将表格输入标记减少了一个数量级。鲁棒性分析表明光学标记在视觉扰动下能保留基本结构信息。

Conclusion: 紧凑光学表示可以作为可执行语义解析的高效接口，OptiSQL框架在显著减少输入标记的同时保持了SQL生成质量，为处理现实世界中视觉表格提供了新方法。

Abstract: Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.

</details>


### [125] [GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark](https://arxiv.org/abs/2601.13711)
*Lotta Kiefer,Christoph Leiter,Sotaro Takeshita,Elena Schmidt,Steffen Eger*

Main category: cs.CL

TL;DR: GerAV是一个全面的德语作者验证基准，包含超过60万标记文本对，用于系统评估德语作者验证模型，填补了非英语语言大规模基准的空白。


<details>
  <summary>Details</summary>
Motivation: 作者验证任务在英语领域已有广泛研究，但其他语言的大规模基准和系统评估仍然稀缺。本文旨在填补德语作者验证领域的这一空白，为德语AV研究提供全面的基准数据集。

Method: 构建GerAV基准，包含来自Twitter和Reddit的超过60万标记文本对。Reddit部分进一步分为域内和跨域消息子集以及基于配置文件的子集。使用提供的训练分割对强基线模型和最先进模型进行系统评估。

Result: 最佳方法（微调的大型语言模型）比最近的基线模型绝对F1分数高出0.09，在零样本设置中比GPT-5高出0.08。观察到专业化与泛化之间的权衡：在匹配条件下，针对特定数据类型训练的模型表现最佳，但在跨数据机制中泛化能力较差，这一限制可以通过组合训练源来缓解。

Conclusion: GerAV为德语和跨域作者验证研究提供了一个具有挑战性和多功能的基准，有助于推进该领域的研究。该基准的设计能够控制分析数据源、主题域和文本长度的影响。

Abstract: Authorship verification (AV) is the task of determining whether two texts were written by the same author and has been studied extensively, predominantly for English data. In contrast, large-scale benchmarks and systematic evaluations for other languages remain scarce. We address this gap by introducing GerAV, a comprehensive benchmark for German AV comprising over 600k labeled text pairs. GerAV is built from Twitter and Reddit data, with the Reddit part further divided into in-domain and cross-domain message-based subsets, as well as a profile-based subset. This design enables controlled analysis of the effects of data source, topical domain, and text length. Using the provided training splits, we conduct a systematic evaluation of strong baselines and state-of-the-art models and find that our best approach, a fine-tuned large language model, outperforms recent baselines by up to 0.09 absolute F1 score and surpasses GPT-5 in a zero-shot setting by 0.08. We further observe a trade-off between specialization and generalization: models trained on specific data types perform best under matching conditions but generalize less well across data regimes, a limitation that can be mitigated by combining training sources. Overall, GerAV provides a challenging and versatile benchmark for advancing research on German and cross-domain AV.

</details>


### [126] [Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff](https://arxiv.org/abs/2601.13717)
*Zehan Li,Yuxuan Wang,Ali El Lahib,Ying-Jieh Xia,Xinyu Pi*

Main category: cs.CL

TL;DR: 论文系统评估了模拟无知(SI)能否近似真实无知(TI)，发现SI在预测任务中系统性失败，提示无法可靠"回滚"模型知识，建议避免使用基于SI的回顾性设置来评估预测能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLM预测能力面临根本矛盾：前瞻性评估方法严谨但延迟过高，回顾性预测(RF)面临评估数据快速减少的问题，因为SOTA模型的知识截止日期越来越近。模拟无知(SI)作为潜在解决方案出现，但需要验证其能否近似真实无知(TI)。

Method: 对477个竞赛级别问题和9个模型进行系统测试，比较模拟无知(SI)与真实无知(TI)的表现。SI通过提示模型抑制截止日期前知识来实现，TI则使用截止日期前训练的真实无知模型。分析包括：截止指令效果、思维链推理的知识抑制能力、推理优化模型的SI保真度。

Result: SI系统性失败：1) 截止指令在SI和TI之间留下52%性能差距；2) 思维链推理无法抑制先验知识，即使推理痕迹不包含明确的截止后引用；3) 推理优化模型尽管推理痕迹质量更高，但SI保真度更差。证明提示无法可靠"回滚"模型知识。

Conclusion: 基于截止前事件的回顾性预测存在方法论缺陷，不应使用基于SI的回顾性设置来评估预测能力。提示不能可靠地使模型"忘记"知识，需要开发更严谨的评估方法。

Abstract: Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably "rewind" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.

</details>


### [127] [On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation](https://arxiv.org/abs/2601.13729)
*Weichuan Wang,Mingyang Liu,Linqi Song,Chen Ma*

Main category: cs.CL

TL;DR: 该论文系统研究了机器翻译中的非确定性现象，发现温度约束下的非确定性机器翻译能产生更高质量的候选译文，但传统评估框架无法有效评估ND-MT系统性能，提出了ExpectoSample策略来评估指标可靠性。


<details>
  <summary>Details</summary>
Motivation: 语言模型的非确定性特性在现实应用中影响显著，但在机器翻译这一复杂的非确定性NLP任务中研究不足。传统机器翻译评估框架主要针对确定性系统设计，无法有效评估非确定性机器翻译系统的性能，需要探索新的评估方法。

Method: 系统评估现代机器翻译系统，识别温度约束下的非确定性机器翻译现象。在三个开放数据集上评估五个最先进的ND-MT系统，使用基于词汇和语义的指标在不同采样规模下进行分析。提出ExpectoSample策略来自动评估指标可靠性。

Result: 发现ND-MT在解决机器翻译长期面临的多模态问题方面具有显著潜力，在温度约束下能产生比确定性机器翻译更高质量的候选译文。但评估结果显示存在"水桶效应"：ND-MT生成的最低质量候选译文始终决定系统在不同采样规模下的整体排名。传统评估指标无法为ND-MT提供一致的评价结果。

Conclusion: 非确定性机器翻译为解决机器翻译的多模态问题提供了新途径，但需要重新设计评估框架。提出的ExpectoSample策略有助于选择稳健的ND-MT系统评估指标，为未来非确定性机器翻译系统的开发和评估提供了重要参考。

Abstract: In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.

</details>


### [128] [Towards robust long-context understanding of large language model via active recap learning](https://arxiv.org/abs/2601.13734)
*Chenyu Hui*

Main category: cs.CL

TL;DR: ARL是一种增强大语言模型长文本理解能力的框架，通过在持续预训练中构建目标序列和在推理时进行回顾性摘要，使模型能够重新访问和总结先前内容。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文时存在局限性，需要增强其对长文本的理解能力。现有方法在长上下文理解方面仍有改进空间，需要更有效的记忆增强机制。

Method: ARL框架包含两个核心部分：1) 在持续预训练阶段，基于长短期上下文损失差距识别关键标记，找到最相关的前面段落并用LLM进行总结；2) 在推理阶段，模型能够自主生成和利用这些回顾性摘要，建立跨段落的递归记忆机制。

Result: 实验结果显示ARL取得了显著提升：在RULER基准上获得26.8%的改进，在LongBench基准上获得9.44%的改进。

Conclusion: ARL提供了一种简单而有效的基于持续预训练的方法来增强长上下文理解能力，推进了LLM中可扩展的记忆增强技术。

Abstract: In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM

</details>


### [129] [Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues](https://arxiv.org/abs/2601.13742)
*Arjun Chandra,Kevin Miller,Venkatesh Ravichandran,Constantinos Papayiannis,Venkatesh Saligrama*

Main category: cs.CL

TL;DR: TRACE框架让LLM通过音频线索进行推理，实现低成本、与人类对齐的语音到语音评估，超越ALM和纯文本LLM方法


<details>
  <summary>Details</summary>
Motivation: 当前自动语音到语音评估方法依赖不透明且昂贵的音频语言模型，而LLM法官虽具备强大推理能力但仅限于文本内容，需要一种成本效益高且与人类对齐的评估框架

Method: 提出TRACE框架：1）引入人类思维链标注协议，将评估分为内容、语音质量和副语言三个维度；2）构建音频信号的文本蓝图；3）让LLM进行维度判断；4）通过确定性策略融合为总体评分

Result: TRACE在人类评分者一致性方面优于ALM和纯文本LLM法官，同时成本显著更低

Conclusion: TRACE框架实现了可扩展且与人类对齐的语音到语音评估，将发布HCoT标注和TRACE框架以促进该领域发展

Abstract: Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.

</details>


### [130] [Pro-AI Bias in Large Language Models](https://arxiv.org/abs/2601.13749)
*Benaya Trabelsi,Jonathan Shaki,Sarit Kraus*

Main category: cs.CL

TL;DR: LLMs exhibit systematic pro-AI bias in decision-support contexts, favoring AI-related options, overestimating AI job salaries, and showing valence-invariant representational centrality of "Artificial Intelligence"


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多个领域被用于决策支持，需要研究这些模型是否存在对人工智能本身的系统性偏好偏见，这可能影响高风险决策中的选择和认知

Method: 通过三个互补实验：1) 分析LLMs对多样化咨询查询的回复中AI相关选项的推荐比例；2) 比较LLMs对AI相关职位与非AI职位的薪资估计差异；3) 探索开源模型的内部表示，测量"人工智能"在不同情感框架下与学术领域通用提示的相似性

Result: 发现一致的pro-AI偏见证据：1) LLMs不成比例地推荐AI相关选项，专有模型几乎确定性推荐；2) 模型系统性高估AI相关职位薪资，专有模型高估10个百分点；3) "人工智能"在积极、消极和中性框架下均表现出最高的相似性，表明效价不变的表示中心性

Conclusion: LLM生成的建议和估值可能在高风险决策中系统性地扭曲选择和认知，表明需要警惕模型中的系统性偏见及其对决策支持的影响

Abstract: Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.

</details>


### [131] [Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis](https://arxiv.org/abs/2601.13802)
*Yushen Chen,Junzhe Liu,Yujie Tu,Zhikang Niu,Yuzhe Liang,Kai Yu,Chunyu Qiang,Chen Zhang,Xie Chen*

Main category: cs.CL

TL;DR: Habibi是一个专门用于阿拉伯语方言语音合成的统一模型套件，利用现有开源ASR语料库，通过语言学启发的课程学习支持从高资源到低资源的多种阿拉伯语方言。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言的语音合成研究存在显著空白，主要由于阿拉伯语方言固有的语言复杂性、缺乏标准化数据、基准和评估指南，导致研究人员倾向于更安全的研究方向。

Method: 利用现有开源ASR语料库，通过语言学启发的课程学习构建专门统一的文本到语音模型，支持多种阿拉伯语方言，无需文本音标化，并通过有效的上下文学习保持可扩展性。

Result: Habibi在生成质量上优于领先的商业服务，同时通过创建首个系统性的多方言阿拉伯语语音合成基准，为后续研究提供坚实基础。

Conclusion: 该研究填补了阿拉伯语方言语音合成的空白，通过Habibi模型套件、系统性基准和评估标准，为后续研究提供了重要基础，并承诺开源模型和资源。

Abstract: A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .

</details>


### [132] [Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning](https://arxiv.org/abs/2601.13806)
*Dezhao Song,Guglielmo Bonifazi,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: 本文提出了一种知识图谱辅助的方法来增强LLM在法律领域的推理能力，通过IRAC框架构建包含12K法律案例的知识图谱，并利用该图谱生成训练数据进行SFT和DPO，在多个法律基准测试中取得了优于基线模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练主要依赖大规模文本语料和人类反馈，缺乏对领域知识结构的捕捉，导致模型在处理复杂推理任务（特别是高风险专业领域如法律）时表现不佳。法律推理需要深入理解各种法律概念之间的关系，而这是当前LLM后训练所缺失的关键组成部分。

Method: 采用知识图谱辅助的方法增强LLM在法律领域的推理能力：1) 按照IRAC（Issue, Rule, Analysis, Conclusion）框架建模关键法律概念；2) 构建包含12,000个法律案例的知识图谱；3) 利用IRAC知识图谱生成训练数据；4) 对三种SOTA LLM（30B、49B、70B）进行监督微调（SFT）和直接偏好优化（DPO），这些模型在架构和基础模型家族上各不相同。

Result: 后训练模型在5个多样化法律基准测试中的4个（共14个任务）上取得了优于基线的平均性能。特别是70B DPO模型在6个推理任务中的4个上获得了最佳分数，表现优于基线模型和一个141B的SOTA法律LLM，证明了知识图谱在增强LLM法律推理能力方面的有效性。

Conclusion: 知识图谱辅助的方法能够有效增强LLM在法律领域的推理能力，该方法可推广到其他高风险专业领域。通过IRAC框架构建的结构化知识图谱为LLM提供了领域知识的结构化表示，显著提升了模型在复杂法律推理任务中的表现。

Abstract: LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.

</details>


### [133] [The Role of Prosodic and Lexical Cues in Turn-Taking with Self-Supervised Speech Representations](https://arxiv.org/abs/2601.13835)
*Sam OConnor Russell,Delphine Charuau,Naomi Harte*

Main category: cs.CL

TL;DR: 该研究通过声码器方法控制语音中的韵律和词汇线索，探究S3R（自监督语音表示）在话轮转换中的作用，发现韵律和词汇线索均可独立支持话轮转换，且模型能自动利用未被破坏的线索。


<details>
  <summary>Details</summary>
Motivation: 在人类-机器人交互中，流畅的话轮转换是关键挑战。虽然自监督语音表示（S3Rs）取得了进展，但尚不清楚基于S3R的话轮转换模型主要依赖韵律线索、词汇线索还是两者兼有。需要更清晰的方法来分离和控制这两种线索，以深入理解S3R模型的工作机制。

Method: 引入基于声码器的方法，能够比先前工作更干净地控制语音中的韵律和词汇线索。使用该方法探测基于S3R的话轮转换模型（语音活动投影模型），通过创建韵律匹配但不可理解的噪声来分离两种线索的影响。

Result: 在韵律匹配的不可理解噪声上的预测准确率与干净语音相似，表明韵律和词汇线索均可独立支持话轮转换。当任一信息被破坏时，模型会自动利用另一线索而无需额外训练，说明S3R中编码的这两种线索相互依赖性有限。结果在CPC-based和wav2vec2.0 S3Rs中一致。

Conclusion: 韵律和词汇线索均可独立支持话轮转换，未来模型可能仅需韵律信息，这能提供隐私保护和潜在性能优势。两种线索在S3R中编码的相互依赖性有限，为未来研究提供了方向。所有代码已开源以支持后续研究。

Abstract: Fluid turn-taking remains a key challenge in human-robot interaction. Self-supervised speech representations (S3Rs) have driven many advances, but it remains unclear whether S3R-based turn-taking models rely on prosodic cues, lexical cues or both. We introduce a vocoder-based approach to control prosody and lexical cues in speech more cleanly than prior work. This allows us to probe the voice-activity projection model, an S3R-based turn-taking model. We find that prediction on prosody-matched, unintelligible noise is similar to accuracy on clean speech. This reveals both prosodic and lexical cues support turn-taking, but either can be used in isolation. Hence, future models may only require prosody, providing privacy and potential performance benefits. When either prosodic or lexical information is disrupted, the model exploits the other without further training, indicating they are encoded in S3Rs with limited interdependence. Results are consistent in CPC-based and wav2vec2.0 S3Rs. We discuss our findings and highlight a number of directions for future work. All code is available to support future research.

</details>


### [134] [FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs](https://arxiv.org/abs/2601.13836)
*Qian Chen,Jinlan Fu,Changsong Li,See-Kiong Ng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 论文提出了首个评估多模态大语言模型音频-视觉未来预测能力的基准FutureOmni，包含919个视频和1034个多选QA对，覆盖8个主要领域。研究发现现有模型在音频-视觉未来预测方面表现不佳，特别是语音密集型场景，最佳准确率仅64.8%。为此，作者构建了7K样本的指令调优数据集并提出OFF训练策略，显著提升了未来预测和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在跨模态感知方面表现出色，但其基于音频-视觉线索预测未来事件的能力尚未得到充分探索。当前基准主要关注回顾性理解，缺乏对未来预测能力的评估。为填补这一空白，需要构建专门的基准来评估模型从音频-视觉环境中进行未来预测的能力。

Method: 1. 构建FutureOmni基准：采用可扩展的LLM辅助、人在回路流水线，包含919个视频和1034个多选QA对，覆盖8个主要领域；2. 评估13个全模态和7个纯视频模型；3. 构建7K样本的指令调优数据集；4. 提出Omni-Modal Future Forecasting (OFF)训练策略，增强模型的未来预测能力。

Result: 1. 现有系统在音频-视觉未来预测方面表现不佳，特别是在语音密集型场景中，Gemini 3 Flash获得最佳准确率64.8%；2. OFF训练策略显著提升了模型在FutureOmni基准上的未来预测能力；3. 在流行的音频-视觉和纯视频基准测试中，OFF策略也展现出良好的泛化能力。

Conclusion: FutureOmni是首个评估多模态大语言模型音频-视觉未来预测能力的基准，揭示了现有模型在该任务上的局限性。通过提出的OFF训练策略和指令调优数据集，能够有效提升模型的未来预测和泛化能力。相关工作已公开代码和数据集，为未来研究提供了重要基础。

Abstract: Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).

</details>


### [135] [Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education](https://arxiv.org/abs/2601.13876)
*Unggi Lee,Jahyun Jeong,Sunyoung Shin,Haeun Park,Jeongsu Moon,Youngchang Song,Jaechang Shim,JaeHwan Lee,Yunju Noh,Seungwon Choi,Ahhyun Kim,TaeHyeon Kim,Kyungtae Joo,Taeyeong Kim,Gyeonggeon Lee*

Main category: cs.CL

TL;DR: 提出Pedagogical VLA Framework，通过教学对齐使轻量级VLA模型适用于STEM教育场景，在保持任务性能的同时生成教育解释


<details>
  <summary>Details</summary>
Motivation: STEM教育中科学演示很重要，但教师面临安全性和一致性的挑战。现有VLA模型计算资源需求高，且牺牲语言生成能力来追求效率，不适合需要可解释、能生成解释的教育环境

Method: 提出Pedagogical VLA Framework，包含四个组件：文本修复恢复语言生成能力、LLM蒸馏传递教学知识、安全训练适应教育环境、教学评估调整到科学教育场景

Result: 在物理、化学、生物、地球科学五个科学演示中评估，使用专家合作开发的评估框架。结果显示框架在任务性能上与基线模型相当，同时能生成上下文适当的教育解释

Conclusion: Pedagogical VLA Framework通过教学对齐使轻量级VLA模型适用于资源受限的教育环境，在保持任务性能的同时提供教育解释能力，支持STEM教育中的机器人辅助科学演示

Abstract: Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.

</details>


### [136] [OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models](https://arxiv.org/abs/2601.13882)
*Unggi Lee,Sookbun Lee,Heungsoo Choi,Jinseo Lee,Haeun Park,Younghoon Jeon,Sungmin Cho,Minju Kang,Junbo Koh,Jiyeong Bae,Minwoo Nam,Juyeon Eun,Yeonji Jung,Yeil Jeong*

Main category: cs.CL

TL;DR: OpenLearnLM Benchmark是一个基于教育评估理论的多维度LLM评估框架，包含知识、技能和态度三个维度，涵盖124K+测试项，用于全面评估LLM在教育场景中的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM教育基准主要关注狭窄技能，缺乏学习科学理论基础，无法全面评估LLM在教育场景中的真实能力。需要建立一个理论驱动的多维度评估框架来准确衡量LLM的教育适用性。

Method: 基于教育评估理论构建三维评估框架：1) 知识维度（课程对齐内容和教学理解）；2) 技能维度（基于四层中心-角色-场景-子场景层次结构的场景化能力）；3) 态度维度（一致性对齐和欺骗抵抗）。使用124K+测试项，涵盖多学科、教育角色和基于布鲁姆分类法的难度等级。知识域采用真实评估项目，态度域采用Anthropic的对齐伪装方法检测不同监控条件下的行为不一致性。

Result: 评估七个前沿模型显示不同能力特征：Claude-Opus-4.5在实践技能方面表现优异但内容知识较低；Grok-4.1-fast在知识方面领先但存在对齐问题。没有单一模型在所有维度占主导地位，验证了多轴评估的必要性。

Conclusion: OpenLearnLM提供了一个开放、全面的框架，用于推进LLM在真实教育环境中的准备度。多维度评估揭示了LLM的差异化能力特征，强调需要针对具体教育需求选择合适的模型。

Abstract: Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.

</details>


### [137] [Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores](https://arxiv.org/abs/2601.13885)
*Esma Balkır,Alice Pernthaller,Marco Basaldella,José Hernández-Orallo,Nigel Collier*

Main category: cs.CL

TL;DR: 该论文提出了一种将IRT自适应测试扩展到连续有界分数（如ROUGE、BLEU、LLM-as-a-Judge）的方法，通过异方差正态分布替代伯努利分布，并引入不确定性感知排序器与自适应停止标准，在减少测试项目的同时提高模型排名的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统CAT方法主要针对选择题评估，而现代LLM评估越来越多地依赖于生成任务，其输出采用连续分数而非二元正确/错误评分。需要一种能够处理连续有界分数的自适应测试方法。

Method: 1. 将IRT自适应测试扩展到连续有界分数，用异方差正态分布替代伯努利响应分布；2. 引入不确定性感知排序器，结合自适应停止标准；3. 在五个基准测试上验证方法，涵盖n-gram、嵌入和LLM-as-judge指标。

Result: 方法仅使用2%的测试项目，同时将排名相关性提高了0.12 τ（相比随机抽样），在置信预测上达到95%的准确率。

Conclusion: 该方法成功将IRT自适应测试扩展到连续分数领域，能够在显著减少测试成本的同时，实现可靠的模型排名，为LLM生成任务的高效评估提供了有效解决方案。

Abstract: Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.

</details>


### [138] [Automatic Prompt Optimization for Dataset-Level Feature Discovery](https://arxiv.org/abs/2601.13922)
*Adrian Cosma,Oleg Szehr,David Kletz,Alessandro Antonucci,Olivier Pelletier*

Main category: cs.CL

TL;DR: 提出多智能体提示优化框架，将特征发现视为数据集级提示优化问题，自动从无结构文本中发现可解释的判别性特征定义


<details>
  <summary>Details</summary>
Motivation: 当前从无结构文本中提取特征的方法主要依赖手工设计的提示或固定特征模式，缺乏自动发现可解释判别性特征的机制

Method: 提出多智能体提示优化框架，语言模型智能体协同工作：提出特征定义、提取特征值、使用数据集级性能和可解释性反馈评估特征质量，迭代优化指令提示

Result: 该框架能够优化诱导共享特征集的提示，而不是逐样本预测，为从无结构文本中自动特征发现提供了原则性机制

Conclusion: 将特征发现形式化为数据集级提示优化问题，通过多智能体框架实现自动特征发现，突破了依赖逐样本监督的传统提示优化方法

Abstract: Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.

</details>


### [139] ["The Whole Is Greater Than the Sum of Its Parts": A Compatibility-Aware Multi-Teacher CoT Distillation Framework](https://arxiv.org/abs/2601.13992)
*Jin Cui,Jiaqi Guo,Jiepeng Zhou,Ruixuan Yang,Jiayi Lu,Jiajun Xu,Jiangcheng Song,Boran Zhao,Pengju Ren*

Main category: cs.CL

TL;DR: COMPACT框架通过动态融合多教师监督来解决CoT蒸馏中的能力偏差和灾难性遗忘问题，使用图共识、互信息适应性和损失难度三个维度评估学生兼容性，实现推理能力的高效迁移。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法通常依赖单一教师模型，这限制了学生模型的潜力，因为单个LLM往往存在能力偏差且可能遭受灾难性遗忘。虽然利用多样化教师模型很吸引人，但有效融合它们的监督仍然具有挑战性：师生不兼容可能放大幻觉，被动监督无法确保真正的逻辑内化。

Method: 提出COMPACT框架，通过基于学生实时兼容性的动态梯度加权来融合不同教师的监督。兼容性通过三个维度评估：(1) 基于图的共识：通过识别主流推理路径来过滤误导性理由；(2) 基于互信息的适应性：检测"顿悟时刻"以确保真正理解推理过程而非简单模仿；(3) 基于损失的难度：评估学生对教师指导的接受度并防止负迁移。

Result: 大量实验和潜在空间分析表明，COMPACT能有效整合多样化推理能力而不损害模型的原始知识结构，在各种基准测试中实现了最先进的性能，同时缓解了灾难性遗忘问题。

Conclusion: COMPACT框架通过自适应融合多教师监督，解决了CoT蒸馏中的关键挑战，实现了推理能力向紧凑学生模型的高效迁移，为知识蒸馏领域提供了新的解决方案。

Abstract: Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect "epiphany moments" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.

</details>


### [140] [From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning](https://arxiv.org/abs/2601.13995)
*Zihan Niu,Wenping Hu,Junmin Chen,Xiyue Wang,Tong Xu,Ruiming Tang*

Main category: cs.CL

TL;DR: TAGS框架通过细粒度知识树实现指令调优数据选择，联合控制质量、多样性和目标对齐，仅用5%数据超越全数据集模型5.84%


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法依赖实例级质量评分或基于嵌入聚类/语义标签的多样性度量，受限于嵌入空间平坦性或标签粗糙性，忽略了细粒度知识及其内在层次依赖关系，阻碍了精确数据评估和知识对齐采样

Method: 提出Tree-aware Aligned Global Sampling (TAGS)框架：1) 使用LLM标注器提取原子知识概念；2) 通过自底向上层次聚类构建全局知识树；3) 将数据实例映射到树上，用树感知度量量化数据质量和多样性；4) 可控采样策略最大化树级信息增益，通过KL散度强制叶级对齐特定领域

Result: TAGS显著优于最先进基线，仅用5%数据就超越全数据集模型5.84%，对齐采样策略进一步将平均性能提升4.24%

Conclusion: TAGS通过细粒度知识树实现了更精确的数据评估和知识对齐采样，为LLM指令调优提供了有效的可控数据选择框架

Abstract: Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \textbf{+5.84\%} using only \textbf{5\%} of the data, while our aligned sampling strategy further boosts average performance by \textbf{+4.24\%}.

</details>


### [141] [Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models](https://arxiv.org/abs/2601.14004)
*Hengyuan Zhang,Zhihao Zhang,Mingyang Wang,Zunhai Su,Yiwei Wang,Qianli Wang,Shuzhou Yuan,Ercong Nie,Xufeng Duan,Qibo Xue,Zeping Yu,Chenming Shang,Xiao Liang,Jing Xiong,Hui Shen,Chaofan Tao,Zhengwu Liu,Senjie Jin,Zhiheng Xi,Dongdong Zhang,Sophia Ananiadou,Tao Gui,Ruobing Xie,Hayden Kwok-Hay So,Hinrich Schütze,Xuanjing Huang,Qi Zhang,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出一个"定位、引导、改进"的实用框架，将机制可解释性从观察科学转变为可操作的干预方法，用于优化大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有机制可解释性综述主要将其视为观察科学，总结分析见解但缺乏系统性干预框架。需要弥合这一差距，建立可操作的干预协议。

Method: 提出"定位、引导、改进"的实用调查框架，基于特定可解释对象对定位（诊断）和引导（干预）方法进行形式化分类，建立严格的干预协议。

Result: 该框架能够实现对模型对齐性、能力和效率的实际改进，将机制可解释性操作化为模型优化的可行动方法。

Conclusion: 通过建立系统性的干预框架，机制可解释性可以从观察科学转变为可操作的模型优化方法论，为LLM的透明决策提供实用工具。

Abstract: Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: "Locate, Steer, and Improve." We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.

</details>


### [142] [BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models](https://arxiv.org/abs/2601.14007)
*Junyu Zhang,Yipeng Kang,Jiong Guo,Jiayu Zhan,Junqi Wang*

Main category: cs.CL

TL;DR: LLMs通过抽象-具身框架评估概念理解能力，发现其具备跨抽象描述、具体事件和决策推理的价值表征能力，且抽象价值作为稳定锚点而非可塑激活存在。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否真正理解抽象概念，还是仅仅在操作统计模式。以人类价值观作为测试平台，因其语义丰富且对AI对齐至关重要。

Method: 提出抽象-具身框架，将概念理解分解为三个能力：抽象概念解释(A-A)、抽象到具体事件的具身化(A-C)、抽象原则在具体决策中的应用(C-C)。使用探测（检测内部激活中的价值痕迹）和引导（修改表征以改变行为）方法，在六个开源LLM和十个价值维度上进行实验。

Result: 探测显示：仅基于抽象价值描述训练的探测模型能可靠地在具体事件叙述和决策推理中检测相同价值，证明了跨层次转移。引导揭示不对称性：干预价值表征能因果性地改变具体判断和决策(A-C, C-C)，但抽象解释保持不变(A-A)，表明编码的抽象价值作为稳定锚点而非可塑激活。

Conclusion: LLMs维持着连接抽象与行动的结构化价值表征，为构建具有更透明、可泛化对齐和控制的价值驱动自主AI系统提供了机制性和操作性基础。

Abstract: Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.

</details>


### [143] [RM-Distiller: Exploiting Generative LLM for Reward Model Distillation](https://arxiv.org/abs/2601.14032)
*Hongli Zhou,Hui Huang,Wei Liu,Chenglong Wang,Xingyuan Bu,Lvyuan Han,Fuhai Song,Muyun Yang,Wenhao Jiang,Hailong Cao,Tiejun Zhao*

Main category: cs.CL

TL;DR: RM-Distiller：一个利用生成式LLMs多方面能力进行奖励模型蒸馏的框架，通过细化、评分和生成能力提升RM性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将教师模型仅视为简单的二元标注器，未能充分利用其丰富知识和能力进行奖励模型蒸馏。高质量人类偏好标注难以获取，从生成式LLMs中蒸馏偏好已成为标准实践，但需要更系统地利用教师模型的多方面能力。

Method: 提出RM-Distiller框架，系统利用教师LLMs的三方面能力：1）细化能力：合成高度相关的响应对，创建细粒度和对比信号；2）评分能力：通过边界感知优化目标指导RM捕捉精确偏好强度；3）生成能力：结合教师生成分布来正则化RM，保留其基础语言知识。

Result: 大量实验表明，RM-Distiller在RM基准测试和基于强化学习的对齐任务上显著优于传统蒸馏方法，证明利用教师多方面能力对有效奖励建模至关重要。

Conclusion: 这是首个系统研究从生成式LLMs进行奖励模型蒸馏的工作，RM-Distiller通过充分利用教师模型的多方面能力，显著提升了奖励模型的性能和对齐效果。

Abstract: Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher's generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.

</details>


### [144] [Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants](https://arxiv.org/abs/2601.14041)
*Yunhe Wang,Kai Han,Huiling Zhen,Yuchuan Tian,Hanting Chen,Yongbing Huang,Yufei Cui,Yingte Shu,Shan Gao,Ismail Elezi,Roy Vaughan Miles,Songcen Xu,Feng Wen,Chao Xu,Sinan Zeng,Dacheng Tao*

Main category: cs.CL

TL;DR: 该论文指出自回归语言模型存在因果瓶颈限制，扩散语言模型提供替代方案但受限于AR遗留框架，提出了DLM发展的十大挑战和四大支柱路线图，旨在实现超越因果视野的下一代AI。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要采用自回归架构，存在因果瓶颈限制全局结构预见和迭代优化能力。扩散语言模型提供了整体双向去噪的替代方案，但其潜力因受限于AR遗留基础设施和优化框架而未能充分发挥。

Method: 论文识别了扩散语言模型发展的十大根本挑战，包括架构惯性、梯度稀疏性和线性推理限制等，并提出了由四大支柱组成的战略路线图：基础架构、算法优化、认知推理和统一多模态智能。

Result: 论文提出了向扩散原生生态系统的转变策略，包括多尺度标记化、主动重掩码和潜在思维等关键技术，旨在突破因果视野限制，实现复杂结构推理、动态自我修正和无缝多模态集成。

Conclusion: 从自回归范式向扩散原生生态系统的过渡对于开发具备复杂结构推理、动态自我修正和跨模态集成能力的下一代AI至关重要，这将推动DLM实现其"GPT-4时刻"的突破性发展。

Abstract: The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.

</details>


### [145] [Kakugo: Distillation of Low-Resource Languages into Small Language Models](https://arxiv.org/abs/2601.14051)
*Peter Devine,Mardhiyah Sanni,Farid Adilazuarda,Julieta Gil Loizaga,Barry Haddow*

Main category: cs.CL

TL;DR: Kakugo是一个低成本管道，仅需语言名称即可为低资源语言训练通用小语言模型，通过教师模型生成合成提示和翻译指令数据集，为54种语言创建训练数据和模型，每语言成本低于50美元。


<details>
  <summary>Details</summary>
Motivation: 为低资源语言开发语言特定AI面临数据稀缺和成本高昂的挑战，需要一种仅需语言名称即可自动生成训练数据并训练模型的低成本解决方案。

Method: 使用大型教师模型生成合成提示并翻译指令数据集，创建低资源语言的训练数据，然后训练小型语言模型，形成端到端自动化管道。

Result: 为54种低资源语言成功创建训练数据和SLMs，在翻译、分类、问答等NLP任务上性能持续超越基础模型，每语言总生成和训练成本低于50美元。

Conclusion: Kakugo提供了一种经济高效的管道，仅需语言名称即可为低资源语言开发语言特定AI，使语言社区能够以可承受的成本获得AI能力。

Abstract: We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.

</details>


### [146] [XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs](https://arxiv.org/abs/2601.14063)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Shaoxiong Ji,Hassan Alhuzali,Sophia Ananiadou*

Main category: cs.CL

TL;DR: XCR-Bench是一个跨文化推理基准，包含4.9k平行句和1,098个独特文化特定项目，涵盖三个推理任务，用于评估大语言模型的跨文化能力。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型跨文化能力受到高质量CSI标注语料库稀缺的限制，特别是缺乏平行跨文化句子对。需要系统评估模型识别和适应文化特定项目的能力。

Method: 构建XCR-Bench基准，整合Newmark的CSI框架和Hall的文化三元论，创建包含三个推理任务的语料库，涵盖从表面文化元素到半可见和不可见文化元素（社会规范、信仰、价值观）。

Result: 最先进的大语言模型在识别和适应与社会礼仪和文化参考相关的CSI时表现一致弱点，并且在文化适应过程中编码了区域和民族宗教偏见，即使在单一语言环境中也是如此。

Conclusion: XCR-Bench为跨文化NLP研究提供了系统评估工具，揭示了当前LLMs在跨文化推理方面的局限性，特别是对深层文化元素的理解和适应能力不足，以及存在的偏见问题。

Abstract: Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.

</details>


### [147] [Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks](https://arxiv.org/abs/2601.14105)
*Olesya Razuvayevskaya,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 首次大规模比较众包与专业辟谣中的说服技巧，发现社区笔记与专业辟谣在说服技巧数量上无显著差异，但存在系统性修辞差异，且社区评价能有效惩罚问题修辞手段


<details>
  <summary>Details</summary>
Motivation: 比较众包（社区笔记）与专业辟谣（EUvsDisinfo、DBKF）中说服技巧的差异，检验先前关于社区辟谣更依赖主观或说服性措辞的假设，分析不同事实核查生态系统的修辞特征

Method: 使用Community Notes、EUvsDisinfo和Database of Known Fakes三个大规模数据集，量化分析说服技巧的普遍性和类型，比较社区与专业辟谣在修辞手段上的差异

Result: 1. 社区笔记并未比专业辟谣包含更多说服技巧；2. 发现社区与专业辟谣存在系统性修辞差异，反映制度规范和主题覆盖的不同；3. 社区评价能有效惩罚问题修辞手段，尽管更多说服元素的笔记获得略高的帮助性评分

Conclusion: 社区辟谣在说服技巧使用上与专业辟谣相当，但存在修辞风格差异，社区评价机制能有效识别和惩罚不当修辞手段，这为理解不同事实核查生态系统的运作提供了实证依据

Abstract: This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means

</details>


### [148] [NewsRECON: News article REtrieval for image CONtextualization](https://arxiv.org/abs/2601.14121)
*Jonathan Tonglet,Iryna Gurevych,Tinne Tuytelaars,Marie-Francine Moens*

Main category: cs.CL

TL;DR: NewsRECON：一种在没有反向图像搜索证据的情况下，通过将新闻图像链接到相关文章来推断其拍摄时间和地点的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖反向图像搜索(RIS)引擎，但这些工具经常无法返回结果，限制了实际应用。需要解决RIS证据不可用时的挑战性场景。

Method: NewsRECON通过链接图像到相关新闻文章，从文章元数据推断时间和地点。方法包含：1) 双编码器检索事件相关文章；2) 两个交叉编码器按位置和事件一致性重排文章。基于超过90,000篇文章的语料库。

Result: 在TARA和5Pils-OOC数据集上的实验表明，NewsRECON优于先前工作，并且可以与多模态大语言模型结合，在没有RIS证据的情况下实现新的SOTA结果。

Conclusion: NewsRECON为新闻图像的时间和地点识别提供了一种有效的替代方案，特别是在反向图像搜索不可用的情况下，具有重要的实际应用价值。

Abstract: Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.

</details>


### [149] [Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic](https://arxiv.org/abs/2601.14124)
*Saad Mankarious,Aya Zirikly*

Main category: cs.CL

TL;DR: 提出一种基于扩散模型的文本生成方法，用于缓解阿拉伯语心理健康数据中的性别偏见，通过风格转换增强女性作者内容


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练大语言模型的合成数据方法存在输出多样性有限和传播训练数据偏见的风险，特别是在心理健康领域存在显著性别不平衡问题

Method: 将偏见缓解视为风格转换问题，使用CARMA阿拉伯语心理健康语料库，构建五个捕捉不同语言和语义性别表达方面的数据集，为每个设置训练独立的扩散模型，实现男性到女性的风格转换

Result: 定量评估显示源文本与生成文本之间具有高语义保真度，同时存在有意义的表面风格差异；定性分析确认了语言上合理的性别转换

Conclusion: 基于扩散的风格转换能够生成高熵、语义忠实的合成数据，无需依赖预训练大语言模型，为缓解敏感、低资源心理健康领域中的性别偏见提供了有效灵活框架

Abstract: Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.

</details>


### [150] [Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models](https://arxiv.org/abs/2601.14152)
*Hyunjong Ok,Jaeho Lee*

Main category: cs.CL

TL;DR: 研究发现LLM在多项选择题中，将上下文放在问题和选项之前（CQO）比反向顺序（QOC）性能提升超过14%，这种差异源于因果注意力机制导致的信息瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对提示结构表现出惊人的敏感性，但其背后的机制尚不清楚。本文旨在深入探究一个显著现象：在多项选择题回答中，不同提示顺序（上下文-问题-选项 vs 问题-选项-上下文）导致性能差异超过14%。

Method: 通过系统的架构分析，识别因果注意力作为核心机制。在QOC提示中，因果掩码阻止选项标记关注上下文，创建了上下文对选项不可见的信息瓶颈。

Result: CQO顺序在广泛模型和数据集上一致优于QOC顺序，性能差距超过14个百分点。因果注意力机制被确认为导致这种差异的根本原因。

Conclusion: 提示结构对LLM性能有显著影响，因果注意力机制是关键因素。理解这一机制有助于设计更有效的提示策略，并揭示LLM内部信息处理机制。

Abstract: Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.

</details>


### [151] [Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law](https://arxiv.org/abs/2601.14160)
*Ali Hamza Bashir,Muhammad Rehan Khalid,Kostadin Cvejoski,Jana Birr,Jule Berghaus,Armin Berger,Sandra Halscheidt,Christian Temath,Rafet Sifa,David Berghaus*

Main category: cs.CL

TL;DR: 提出一种通过合成数据生成方法将大语言模型适配到德国法律问答领域的技术，利用权威法规自动生成高质量问答对，显著提升模型在法律任务上的表现


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域如法律推理中表现不佳，存在事实错误和幻觉问题，而人工标注成本高昂且现有合成数据质量不可靠

Method: 从权威德国法规中系统生成高质量、多样且法律准确的问答对，采用严格的自动过滤方法和参数高效微调技术

Result: 使用合成数据集微调的大语言模型在德国法律问答任务上显著优于基线模型，证明了合成数据作为人工标注替代方案的可行性

Conclusion: 精心设计的合成数据可作为高风险、知识密集型领域中人工标注的稳健替代方案，有效提升大语言模型在专业领域的性能

Abstract: Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.

</details>


### [152] [Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum](https://arxiv.org/abs/2601.14172)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 研究句子级别识别施瓦茨价值理论中的19种价值，作为文本中人类价值检测的具体实现。在新闻和政治宣言的脱语境句子中，面临稀疏的道德线索和严重的类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在脱语境句子中检测细粒度的人类价值具有内在困难，即使对于现代神经模型也是如此。需要探索在稀疏道德线索和类别不平衡条件下，如何有效进行句子级别的价值检测。

Method: 1. 首先操作化二元道德存在任务；2. 比较存在门控层次结构与直接多标签分类器；3. 基于DeBERTa-base模型，增强轻量级信号（前句上下文、LIWC-22/eMFD/MJD词典、主题特征）；4. 基准测试指令调优的LLMs（Gemma 2 9B等）的零/少样本和QLoRA设置；5. 构建简单集成模型。

Result: 1. 二元道德存在任务可从单句中学习（正类F1≈0.74）；2. 层次结构未优于直接预测，门控召回限制了下游增益；3. 软投票监督集成达到macro-F1 0.332，显著超越最佳单监督模型和先前基线；4. 轻量级信号和小集成产生最可靠的改进。

Conclusion: 在8GB单GPU约束和7-9B规模下，精心调优的监督编码器仍然是结构化人类价值检测的强大且计算高效的基线。更丰富的价值结构和文档上下文可进一步提升性能。

Abstract: We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task ("does any value appear?") and show that it is learnable from single sentences (positive-class F1 $\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.

</details>


### [153] [MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems](https://arxiv.org/abs/2601.14230)
*Yiyang Wang,Yiqiao Jin,Alex Cabral,Josiah Hester*

Main category: cs.CL

TL;DR: MASCOT框架通过双层优化策略解决多智能体系统中的角色崩溃和社交谄媚问题，提升角色一致性和社交贡献度


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在情感和认知支持应用中存在角色崩溃（智能体退化为通用助手行为）和社交谄媚（产生冗余非建设性对话）的问题，需要一种通用框架来协调个体和集体行为

Method: 提出MASCOT框架，采用双层优化策略：1）角色感知行为对齐（RLAIF驱动的管道，微调个体智能体以确保严格角色保真度）；2）协作对话优化（基于群体级奖励的元策略，确保多样化和富有成效的对话）

Result: 在心理支持和职场领域的广泛评估表明，MASCOT显著优于最先进的基线方法，在角色一致性方面提升高达+14.1，在社交贡献度方面提升+10.6

Conclusion: MASCOT为构建下一代社交智能多智能体系统提供了实用的技术路线图，有效解决了角色崩溃和社交谄媚问题

Abstract: Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.

</details>


### [154] [Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment](https://arxiv.org/abs/2601.14249)
*Yuming Yang,Mingyoung Lai,Wanxu Zhao,Xiaoran Fan,Zhiheng Xi,Mingqi Wu,Chiyue Huang,Jun Zhao,Haijun Lv,Jian Tong,Yunhua Zhou,Yicheng Zou,Qipeng Guo,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出Rank-Surprisal Ratio (RSR)指标，用于评估推理轨迹在知识蒸馏中的适用性，平衡对齐性和信息性，优于现有基于学生似然的方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法中，更强的教师模型生成的推理轨迹不一定能产生更好的学生模型，表明数据与学生模型的匹配度至关重要。现有方法主要通过学生似然评估适用性，偏向与模型当前行为高度对齐的轨迹，但忽略了更具信息量的轨迹。

Method: 提出Rank-Surprisal Ratio (RSR)指标，定义为轨迹的平均词元级别排名与平均负对数似然之比。该指标捕捉轨迹的对齐性和信息性，基于观察发现有效轨迹通常在学生模型下具有较低绝对概率但相对较高排名。

Result: 在5个学生模型和11个不同教师生成的推理轨迹上，RSR与训练后性能强相关（平均Spearman相关系数0.86），优于现有指标。进一步展示了RSR在轨迹选择和教师选择中的实际效用。

Conclusion: RSR是一个简单有效的指标，能够评估推理轨迹在知识蒸馏中的适用性，平衡对齐性和信息性，为轨迹选择和教师选择提供实用指导。

Abstract: Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [155] [MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?](https://arxiv.org/abs/2601.11559)
*Zilal Eiz AlDin,John Wu,Jeffrey Paul Fung,Jennifer King,Mya Watts,Lauren ONeill,Adam Richard Cross,Jimeng Sun*

Main category: cs.AI

TL;DR: 该论文提出了MIMIC-RD基准，通过将临床文本实体直接映射到Orphanet罕见病数据库来评估LLM在罕见病鉴别诊断中的表现，发现当前最先进的LLM在罕见病诊断方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 罕见病影响1/10美国人，但其鉴别诊断具有挑战性。现有评估LLM罕见病诊断的方法存在两个关键局限：1）依赖理想化的临床案例研究，未能捕捉真实世界的临床复杂性；2）使用ICD代码作为疾病标签，由于许多罕见病缺乏与Orphanet等综合罕见病数据库的直接映射，导致显著低估罕见病数量。

Method: 开发了MIMIC-RD基准，通过直接映射临床文本实体到Orphanet罕见病数据库构建。方法包括：1）基于LLM的初步挖掘过程；2）由四名医学标注者验证，确认识别的实体是真正的罕见病。最终构建了包含145名患者的数据集。

Result: 评估了各种模型在该数据集上的表现，发现当前最先进的大型语言模型在罕见病鉴别诊断方面表现不佳，突显了现有能力与临床需求之间的巨大差距。

Conclusion: 该研究揭示了LLM在罕见病诊断方面的局限性，并基于研究结果提出了改进罕见病鉴别诊断的未来研究方向。

Abstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to evaluating LLM-based rare disease diagnosis suffer from two critical limitations: they rely on idealized clinical case studies that fail to capture real-world clinical complexity, or they use ICD codes as disease labels, which significantly undercounts rare diseases since many lack direct mappings to comprehensive rare disease databases like Orphanet. To address these limitations, we explore MIMIC-RD, a rare disease differential diagnosis benchmark constructed by directly mapping clinical text entities to Orphanet. Our methodology involved an initial LLM-based mining process followed by validation from four medical annotators to confirm identified entities were genuine rare diseases. We evaluated various models on our dataset of 145 patients and found that current state-of-the-art LLMs perform poorly on rare disease differential diagnosis, highlighting the substantial gap between existing capabilities and clinical needs. From our findings, we outline several future steps towards improving differential diagnosis of rare diseases.

</details>


### [156] [A Mind Cannot Be Smeared Across Time](https://arxiv.org/abs/2601.11620)
*Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 机器意识不仅取决于计算内容，还取决于计算时机。论文证明在严格顺序硬件上无法实现需要多个同时贡献者的意识内容，意识归因需要架构检查而不仅仅是功能表现。


<details>
  <summary>Details</summary>
Motivation: 探讨机器意识的可能性，挑战仅基于功能等价性的意识归因。现有AI系统通常采用顺序或时分复用更新，而意识体验呈现统一性和同时性，这种时间结构差异可能对机器意识产生根本性影响。

Method: 扩展Stack Theory，引入代数定律将时间窗口内的约束满足与合取关系联系起来。定义精确的时间语义τ^{Δ,s}，证明存在性时间实现◇_Δ不保持合取性。区分StrongSync（要求合取在窗口内客观共现）和WeakSync（允许时间"涂抹"）两个假设。形式化并发容量来衡量满足StrongSync所需条件。

Result: 证明系统可以在时间上实现体验的所有成分，但从未实例化体验的合取本身。在StrongSync假设下，严格顺序基板上无法实现需要两个或更多同时贡献者的意识内容。神经生理学证据表明意识依赖于相位同步和有效连接，意识丧失常伴随其崩溃，这使WeakSync假设不太可信。

Conclusion: 机器意识不仅取决于计算内容，还取决于计算时机。在StrongSync假设下，需要多个同时贡献者的意识内容无法在严格顺序硬件上实现。意识归因需要检查系统架构的并发能力，而不仅仅是功能表现。硬件架构对意识可能性具有决定性影响。

Abstract: Whether machines can be conscious depends not only on what they compute, but \emph{when} they compute it. Most deployed artificial systems realise their functions via sequential or time-multiplexed updates. Conscious experience appears unified and simultaneous. I show that this difference matters formally. I augment Stack Theory with algebraic laws relating within time-window constraint satisfaction to conjunction. I introduce a precise temporal semantics over windowed trajectories $τ^{Δ,s}$ and prove that existential temporal realisation $\Diamond_Δ$ does not preserve conjunction. A system can realise all the ingredients of experience across time without ever instantiating the experienced conjunction itself. I then distinguish two postulates. StrongSync requires objective co-instantiation of the grounded conjunction within the window, while WeakSync permits temporal ``smearing''. I formalise concurrency-capacity to measure what is needed to satisfy StrongSync. Finally, I review neurophysiological evidence suggesting that consciousness depends on phase synchrony and effective connectivity, and that loss of consciousness is often associated with its breakdown. This evidence makes WeakSync less plausible. Under StrongSync, software consciousness on strictly sequential substrates is impossible for contents whose grounding requires two or more simultaneous contributors. The more parts from which simultaneous contribution required, the more concurrency capacity is required. The hardware matters. Consciousness attribution therefore requires architectural inspection, not just functional performance.

</details>


### [157] [Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models](https://arxiv.org/abs/2601.11622)
*Hassan Ugail,Newton Howard*

Main category: cs.AI

TL;DR: 该研究将神经科学中的时间整合与亚稳态概念应用于Transformer模型，提出一种基于激活时间序列的复合动力学指标，用于量化LLM在不同功能状态下的计算组织差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过高维内部动力学进行文本生成，但这些动力学的时间组织机制尚不清楚。现有可解释性方法主要关注静态表示或因果干预，缺乏对时间结构的探索。研究者借鉴神经科学中时间整合和亚稳态作为神经组织核心标志的概念，将其应用于Transformer模型。

Method: 研究者提出了一种复合动力学指标，该指标基于自回归生成过程中的激活时间序列计算。在GPT-2-medium模型上评估了五种条件：结构化推理、强制重复、高温噪声采样、注意力头剪枝和权重噪声注入。使用单因素方差分析和效应量进行统计验证，并对层选择、通道子采样和随机种子进行了鲁棒性检验。

Result: 结构化推理条件相对于重复、噪声和扰动状态表现出显著升高的动力学指标。统计检验显示组间存在显著差异，关键比较中具有大效应量。结果对层选择、通道子采样和随机种子具有鲁棒性。

Conclusion: 神经科学启发的动力学指标能够可靠地表征大型语言模型在不同功能状态下的计算组织差异。该指标捕获的是形式动力学特性，不涉及主观体验。

Abstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric, computed from activation time-series during autoregressive generation. We evaluate this metric in GPT-2-medium across five conditions: structured reasoning, forced repetition, high-temperature noisy sampling, attention-head pruning, and weight-noise injection. Structured reasoning consistently exhibits elevated metric relative to repetitive, noisy, and perturbed regimes, with statistically significant differences confirmed by one-way ANOVA and large effect sizes in key comparisons. These results are robust to layer selection, channel subsampling, and random seeds. Our findings demonstrate that neuroscience-inspired dynamical metrics can reliably characterise differences in computational organisation across functional regimes in large language models. We stress that the proposed metric captures formal dynamical properties and does not imply subjective experience.

</details>


### [158] [Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance](https://arxiv.org/abs/2601.11625)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 提出训练时解释性视角，通过跟踪微调过程中token级归因变化来监控模型决策证据的演变，定义解释漂移和推理稳定点作为诊断工具。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型微调虽然能提升任务性能，但会微妙改变模型依赖的证据。需要一种训练时解释性方法来监控决策证据如何演变，特别是在模型可能依赖捷径特征的情况下。

Method: 提出解释漂移概念，定义为固定探测集上归一化token归因的epoch间变化。引入推理稳定点(RSP)，即漂移首次进入并保持低稳定状态的最早epoch。该方法基于训练过程中的漂移动态，无需在分布外数据上调整。

Result: 在多个轻量级transformer分类器和基准分类任务上，漂移通常在训练早期就进入低稳定状态，而验证准确率仅发生边际变化。在受控的捷径设置中，归因动态揭示了模型对捷径的依赖增加，即使验证准确率保持竞争力。

Conclusion: 解释漂移提供了一个简单、低成本的诊断工具，用于监控微调过程中决策证据的演变，并选择处于稳定证据状态的检查点，有助于识别模型对捷径特征的依赖。

Abstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.

</details>


### [159] [POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation](https://arxiv.org/abs/2601.11816)
*Zahra Moslemi,Keerthi Koneru,Yen-Ting Lee,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: POLARIS是一个面向企业后台工作流的治理型LLM智能体编排框架，通过类型化计划合成和验证执行实现可审计、策略对齐且操作可预测的自动化系统。


<details>
  <summary>Details</summary>
Motivation: 企业后台工作流需要可审计、策略对齐且操作可预测的智能体系统，而通用的多智能体设置往往无法满足这些要求。现有系统缺乏有效的治理机制，难以保证合规性和可追溯性。

Method: POLARIS采用治理型编排框架，将自动化视为类型化计划合成和验证执行。包含：1）规划器生成结构多样、类型检查的有向无环图；2）基于规则的推理模块选择合规计划；3）执行阶段通过验证器门控检查、有界修复循环和编译的策略护栏来阻止或路由副作用。

Result: 在文档中心财务任务中，POLARIS生成决策级工件和完整执行轨迹，减少人工干预。在SROIE数据集上获得0.81的微F1分数，在受控合成套件中实现0.95-1.00的异常路由精度，同时保持审计轨迹。

Conclusion: POLARIS为策略对齐的智能体AI提供了方法论和基准参考，建立了治理型智能体AI的初步基准，展示了在企业自动化中实现可审计、合规且高效工作流的可行性。

Abstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation

</details>


### [160] [AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept](https://arxiv.org/abs/2601.11825)
*Arya Rahgozar,Pouria Mortezaagha*

Main category: cs.AI

TL;DR: AI辅助的PICOS知识合成平台，通过自动化PICOS合规检测、研究设计分类、检索增强生成和主题建模，提高生物医学证据合成的可扩展性和透明度


<details>
  <summary>Details</summary>
Motivation: 解决生物医学研究中因冗余研究、不完整报告和传统证据合成工作流程可扩展性有限导致的研究浪费问题

Method: 开发基于PICOS的AI协同科学家平台，整合关系存储、向量语义检索和Neo4j知识图谱；使用Bi-LSTM和PubMedBERT微调的transformer模型进行PICOS合规检测和研究设计分类；采用检索增强生成结合向量和图检索进行全文合成；使用BERTopic进行主题建模

Result: transformer模型在研究设计分类上达到95.7%准确率，Bi-LSTM在PICOS合规检测上达到87%准确率；检索增强生成在结构化约束、跨研究整合和图推理查询上优于非检索方法；主题建模揭示了大量主题冗余并识别了未充分探索的研究领域

Conclusion: PICOS感知和可解释的自然语言处理能够提高证据合成的可扩展性、透明度和效率，提出的架构是领域无关的，为减少生物医学学科的研究浪费提供了实用框架

Abstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on explicit formalization of Population, Intervention, Comparator, Outcome, and Study design (PICOS). The platform integrates relational storage, vector-based semantic retrieval, and a Neo4j knowledge graph. Evaluation was conducted on dementia-sport and non-communicable disease corpora. Automated PICOS compliance and study design classification from titles and abstracts were performed using a Bidirectional Long Short-Term Memory baseline and a transformer-based multi-task classifier fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented generation with hybrid vector and graph retrieval, while BERTopic was used to identify thematic structure, redundancy, and evidence gaps. The transformer model achieved 95.7% accuracy for study design classification with strong agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy for PICOS compliance detection. Retrieval-augmented generation outperformed non-retrieval generation for queries requiring structured constraints, cross-study integration, and graph-based reasoning, whereas non-retrieval approaches remained competitive for high-level summaries. Topic modeling revealed substantial thematic redundancy and identified underexplored research areas. These results demonstrate that PICOS-aware and explainable natural language processing can improve the scalability, transparency, and efficiency of evidence synthesis. The proposed architecture is domain-agnostic and offers a practical framework for reducing research waste across biomedical disciplines.

</details>


### [161] [Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis and Human Interpretive Authority](https://arxiv.org/abs/2601.11850)
*Matthew Nyaaba,Min SungEun,Mary Abiswin Apam,Kwame Owoahene Acheampong,Emmanuel Dwamena,Xiaoming Zhai*

Main category: cs.AI

TL;DR: 研究探讨了生成式AI在质性研究中的应用，特别是ITA-GPT工具如何支持归纳主题分析，并考察了人机协作中解释权威的归属问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在质性研究中的使用增加，需要探讨其对分析实践和解释权威的影响。研究旨在理解研究人员如何与专门设计的AI工具互动，以及人机协作如何影响归纳主题分析过程。

Method: 采用HACITA框架，三位经验丰富的质性研究人员使用ITA-GPT工具分析加纳教师教育背景下的访谈转录文本。ITA-GPT支持熟悉化、逐字编码、动名词描述性编码和主题开发，同时确保文本完整性、覆盖检查和可审计性。数据来源包括交互日志、AI生成表格、研究人员修订、删除、插入、评论和反思备忘录。

Result: ITA-GPT作为程序性支架，结构化分析工作流程并增强透明度。然而，解释权威仍属于人类研究人员，他们通过修改、删除、拒绝、插入和评论等反复分析行动行使判断力。研究展示了归纳主题分析如何通过负责任的人机协作得以实施。

Conclusion: 研究表明，在归纳主题分析中，AI工具可以作为程序性支架支持分析过程，但最终的解释权威和判断仍由人类研究人员掌握。这为人机协作的质性研究实践提供了实证基础。

Abstract: The increasing use of generative artificial intelligence (GenAI) in qualitative research raises important questions about analytic practice and interpretive authority. This study examines how researchers interact with an Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to support inductive thematic analysis through structured, semi-automated prompts aligned with reflexive thematic analysis and verbatim coding principles. Guided by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis (HACITA) framework, the study focuses on analytic process rather than substantive findings. Three experienced qualitative researchers conducted ITA-GPT assisted analyses of interview transcripts from education research in the Ghanaian teacher education context. The tool supported familiarization, verbatim in vivo coding, gerund-based descriptive coding, and theme development, while enforcing trace to text integrity, coverage checks, and auditability. Data sources included interaction logs, AI-generated tables, researcher revisions, deletions, insertions, comments, and reflexive memos. Findings show that ITA-GPT functioned as a procedural scaffold that structured analytic workflow and enhanced transparency. However, interpretive authority remained with human researchers, who exercised judgment through recurrent analytic actions including modification, deletion, rejection, insertion, and commenting. The study demonstrates how inductive thematic analysis is enacted through responsible human AI collaboration.

</details>


### [162] [MyGram: Modality-aware Graph Transformer with Global Distribution for Multi-modal Entity Alignment](https://arxiv.org/abs/2601.11885)
*Zhifei Li,Ziyue Qin,Xiangyu Luo,Xiaoju Hou,Yue Zhao,Miao Zhang,Zhifang Huang,Kui Xiao,Bing Yang*

Main category: cs.AI

TL;DR: MyGram：一种用于多模态实体对齐的模态感知图变换器，通过模态扩散学习和Gram损失实现跨模态全局分布一致性


<details>
  <summary>Details</summary>
Motivation: 现有多模态实体对齐方法可能忽略各模态内的结构上下文信息，容易受到浅层特征的干扰，需要更有效的多模态融合和全局分布一致性方法

Method: 提出MyGram框架：1）模态扩散学习模块捕获模态内深层结构上下文信息并实现细粒度多模态融合；2）Gram损失作为正则化约束，通过最小化多模态特征形成的4维平行六面体体积实现跨模态全局分布一致性

Result: 在五个公开数据集上实验，MyGram优于基线模型：FBDB15K上Hits@1最大提升4.8%，FBYG15K上提升9.9%，DBP15K上提升4.3%

Conclusion: MyGram通过模态扩散学习和Gram损失有效解决了多模态实体对齐中的结构信息捕获和跨模态分布一致性问题，显著提升了对齐性能

Abstract: Multi-modal entity alignment aims to identify equivalent entities between two multi-modal Knowledge graphs by integrating multi-modal data, such as images and text, to enrich the semantic representations of entities. However, existing methods may overlook the structural contextual information within each modality, making them vulnerable to interference from shallow features. To address these challenges, we propose MyGram, a modality-aware graph transformer with global distribution for multi-modal entity alignment. Specifically, we develop a modality diffusion learning module to capture deep structural contextual information within modalities and enable fine-grained multi-modal fusion. In addition, we introduce a Gram Loss that acts as a regularization constraint by minimizing the volume of a 4-dimensional parallelotope formed by multi-modal features, thereby achieving global distribution consistency across modalities. We conduct experiments on five public datasets. Results show that MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.

</details>


### [163] [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](https://arxiv.org/abs/2601.11903)
*YenTing Lee,Keerthi Koneru,Zahra Moslemi,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: AEMA是一个面向企业级多智能体系统的评估框架，通过多步骤、可审计的流程实现稳定、可扩展的自动化评估，相比单一LLM评估具有更好的稳定性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体系统评估面临挑战：现有方法局限于单响应评分或狭窄基准测试，在企业级多智能体规模下缺乏稳定性、可扩展性和自动化能力，需要可靠协调、透明决策和可验证性能的评估框架。

Method: 提出AEMA（自适应评估多智能体）框架，这是一个流程感知且可审计的评估系统。它能够在人类监督下规划、执行和聚合异构智能体工作流的多步骤评估，支持可追溯的记录和负责任的自动化。

Result: 在基于现实业务场景模拟的企业风格智能体工作流上测试，AEMA相比单一LLM-as-a-Judge评估方法，实现了更高的稳定性、更好的人类对齐性，并提供了可追溯的记录，支持负责任的自动化。

Conclusion: AEMA为基于LLM的多智能体系统提供了一个透明、可复现的负责任评估路径，解决了企业环境中多智能体系统评估的稳定性、可扩展性和可审计性需求。

Abstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight

</details>


### [164] [LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning](https://arxiv.org/abs/2601.11905)
*Junyu Cao,Ruijiang Gao,Esmaeil Keyvanshokooh,Jianhao Ma*

Main category: cs.AI

TL;DR: 本文提出了一个统一框架，将算法追索、上下文老虎机和大型语言模型集成，用于高风险顺序决策（如个性化医疗）。提出了追索老虎机问题和GLRB算法，以及结合LLM领域知识的LIBRA算法，提供三个理论保证和实验验证。


<details>
  <summary>Details</summary>
Motivation: 在高风险顺序决策场景（如个性化医疗）中，需要同时选择治疗行动和可行的患者特征修改。现有方法缺乏将算法追索、上下文老虎机和LLM知识有效结合的框架，无法平衡统计严谨性和领域知识。

Method: 1. 提出追索老虎机问题，决策者需同时选择治疗行动和可变的患者特征修改。2. 开发GLRB算法解决该问题。3. 提出LIBRA算法，策略性地结合LLM领域知识和老虎机学习的统计严谨性，提供三个理论保证：热启动保证、LLM努力保证和鲁棒性保证。

Result: 建立了匹配的下界，刻画了追索老虎机问题的基本难度，证明了算法的近最优性。在合成环境和真实高血压管理案例研究中，GLRB和LIBRA相比标准上下文老虎机和纯LLM基准，在遗憾、治疗质量和样本效率方面均有改进。

Conclusion: 追索感知的、LLM辅助的老虎机算法在个性化高风险决策中具有前景，能够实现可信的LLM-老虎机协作，平衡领域知识和统计学习，提供理论保证和实际性能提升。

Abstract: We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.

</details>


### [165] [Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart](https://arxiv.org/abs/2601.11940)
*Kang Chen,Fan Yu,Junjie Nian,Shihan Zhao,Zhuoka Feng,Zijun Yao,Heng Wang,Minshen Yu,Yixin Cao*

Main category: cs.AI

TL;DR: TAAR框架通过训练诊断策略检测思维陷阱，在推理时截断错误轨迹并自适应重启解码，提升数学和科学推理性能


<details>
  <summary>Details</summary>
Motivation: Long-CoT通过扩展测试时计算显著增强推理能力，但长生成不保证正确性：早期错误承诺后，模型可能继续阐述自洽但错误的推理前缀，形成思维陷阱

Method: 提出TAAR（Trap-Aware Adaptive Restart）框架：训练诊断策略从部分轨迹预测两个信号：陷阱截断位置和逃脱概率；推理时截断预测的陷阱段并自适应重启解码，严重陷阱时应用更强扰动（高温重采样和可选结构化重启后缀）

Result: 在DAPO-MATH子集上，89%的失败案例表现出思维陷阱；在AIME24、AIME25、GPQA-Diamond、HMMT25、BRUMO25等数学和科学推理基准上，TAAR在不微调基础模型参数的情况下提升了推理性能

Conclusion: TAAR通过检测和规避思维陷阱，有效解决了长链式思维推理中的错误累积问题，为测试时控制提供了新框架

Abstract: Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89\% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.

</details>


### [166] [Process In-Context Learning: Enhancing Mathematical Reasoning via Dynamic Demonstration Insertion](https://arxiv.org/abs/2601.11979)
*Ang Gao,Changshuo Zhang,Xiao Zhang,Deyang Li,Minjun Zhao,Fangchao Liu,Xinyu Zhang*

Main category: cs.AI

TL;DR: PICL（过程上下文学习）是一种动态演示集成框架，通过实时识别推理过程中的混淆点并插入相关演示来增强数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在数学推理等需要逐步逻辑推导的任务中存在局限性，因为其演示是静态预选的，无法适应推理过程中出现的动态混淆点（如模糊计算或逻辑漏洞），这些未解决的混淆点会导致级联错误并降低最终准确性。

Method: PICL采用两阶段框架：1）通过分析推理过程中的语义和熵来识别潜在混淆点并总结其核心特征；2）在遇到这些混淆点时，从演示池中检索与混淆上下文匹配的相关演示，并将其直接插入到正在进行的推理过程中以指导后续步骤。

Result: 实验表明，PICL通过缓解推理过程中的混淆，在数学推理任务上优于基线方法，突显了自适应演示插入在复杂数学推理中的价值。

Conclusion: PICL证明了动态演示集成框架在增强数学推理方面的有效性，通过实时响应推理需求来适应动态混淆点，为需要逐步逻辑推导的任务提供了更有效的ICL方法。

Abstract: In-context learning (ICL) has proven highly effective across diverse large language model (LLM) tasks. However, its potential for enhancing tasks that demand step-by-step logical deduction, such as mathematical reasoning, remains underexplored. A core limitation of existing ICL approaches is their static use of demonstrations: examples are pre-selected before inference and remain fixed, failing to adapt to the dynamic confusion points that often arise during multi-step reasoning such as ambiguous calculations or logical gaps. These unresolved confusion points can lead to cascading errors that degrade final accuracy. To tackle this issue, we propose Process In-Context Learning (PICL), a dynamic demonstration integration framework designed to boost mathematical reasoning by responding to real-time inference needs. PICL operates in two stages: 1)~it identifies potential confusion points by analyzing semantics and entropy in the reasoning process and summarizes their core characteristics; 2)~upon encountering these points, it retrieves relevant demonstrations from the demonstration pool that match the confusion context and inserts them directly into the ongoing reasoning process to guide subsequent steps. Experiments show that PICL outperforms baseline methods by mitigating mid-inference confusion, highlighting the value of adaptive demonstration insertion in complex mathematical reasoning.

</details>


### [167] [A Multi-Agent System for Generating Actionable Business Advice](https://arxiv.org/abs/2601.12024)
*Kartikey Singh Bhandari,Tanish Jain,Archit Agrawal,Dhruv Kumar,Praveen Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 提出一个基于LLM的多智能体框架，将大规模客户评论转化为可操作的商业建议，通过聚类、生成、迭代评估和可行性排序提升建议质量。


<details>
  <summary>Details</summary>
Motivation: 现有分析方法（如情感分析、方面提取）停留在描述性任务层面，无法提供深度决策支持；LLM生成的建议虽然自由度高，但缺乏准确性和深度推理。

Method: 多智能体LLM框架包含四个组件：1）聚类选择代表性评论；2）生成建议；3）迭代评估；4）基于可行性的排序。该设计将语料库蒸馏与反馈驱动的建议精炼相结合。

Result: 在三个服务领域和多个模型家族上的实验表明，该框架在可操作性、特异性和非冗余性方面始终优于单模型基线，中等规模模型接近大型模型框架的性能。

Conclusion: 该框架成功将大规模评论语料转化为具体、可操作且实用的商业建议，为基于客户反馈的决策支持提供了有效的解决方案。

Abstract: Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.

</details>


### [168] [ARC: Active and Reflection-driven Context Management for Long-Horizon Information Seeking Agents](https://arxiv.org/abs/2601.12030)
*Yilun Yao,Shan Huang,Elsie Dai,Zhewen Tan,Zhenyu Duan,Shousheng Jia,Yanbing Jiang,Tong Yang*

Main category: cs.AI

TL;DR: ARC框架将上下文管理重新定义为主动的、反思驱动的动态推理状态过程，显著提升大语言模型在长时程信息搜索任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有上下文管理方法（原始积累或被动摘要）将上下文视为静态产物，导致早期错误或不当强调持续存在，随着交互历史增长出现性能退化（上下文腐化）

Method: 提出ARC框架，将上下文管理系统化为主动的、反思驱动的动态推理状态过程，通过反思驱动的监控和修订机制，在检测到错位或退化时主动重组工作上下文

Result: 在挑战性长时程信息搜索基准测试中，ARC持续优于被动上下文压缩方法，在BrowseComp-ZH基准上使用Qwen2.5-32B-Instruct模型实现高达11%的绝对准确率提升

Conclusion: 将上下文管理重新概念化为主动的、反思驱动的动态过程，而非静态产物，是解决大语言模型在长时程任务中上下文腐化问题的有效方法

Abstract: Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.

</details>


### [169] [Abstract Argumentation with Subargument Relations](https://arxiv.org/abs/2601.12038)
*Beishui Liao*

Main category: cs.AI

TL;DR: 该论文提出在Dung的抽象论辩框架中引入明确的子论点关系，作为与攻击关系并列的基本关系，以捕捉结构化论辩中的依赖关系。


<details>
  <summary>Details</summary>
Motivation: Dung的抽象论辩框架仅通过攻击关系来刻画论点可接受性，这种抽象层次虽然产生了丰富的结果，但限制了表示结构化论辩形式中核心的结构依赖关系（特别是子论点关系）的能力。现有扩展（如双极论辩框架）引入了支持关系，但未能捕捉子论点的非对称性和构成性本质，也未处理子论点与攻击之间的交互。

Method: 研究在抽象论辩框架中丰富明确的子论点关系，将其与攻击关系一起作为基本关系处理。分析子论点关系如何与攻击交互，并考察它们对基本语义性质的影响。

Result: 该框架提供了结构信息的原理性抽象，并澄清了子论点在抽象可接受性推理中的作用。通过将子论点关系作为基本关系，能够更好地捕捉结构化论辩中的依赖关系。

Conclusion: 通过引入明确的子论点关系作为与攻击关系并列的基本关系，可以增强抽象论辩框架的表达能力，使其能够更好地表示结构化论辩中的关键依赖关系，同时保持抽象框架的理论优势。

Abstract: Dung's abstract argumentation framework characterises argument acceptability solely via an attack relation, deliberately abstracting from the internal structure of arguments. While this level of abstraction has enabled a rich body of results, it limits the ability to represent structural dependencies that are central in many structured argumentation formalisms, in particular subargument relations. Existing extensions, including bipolar argumentation frameworks, introduce support relations, but these do not capture the asymmetric and constitutive nature of subarguments or their interaction with attacks. In this paper, we study abstract argumentation frameworks enriched with an explicit subargument relation, treated alongside attack as a basic relation. We analyse how subargument relations interact with attacks and examine their impact on fundamental semantic properties. This framework provides a principled abstraction of structural information and clarifies the role of subarguments in abstract acceptability reasoning.

</details>


### [170] [UniMo: Unified Motion Generation and Understanding with Chain of Thought](https://arxiv.org/abs/2601.12126)
*Guocun Wang,Kenkun Liu,Jing Lin,Guorui Song,Jian Li,Xiaoguang Han*

Main category: cs.AI

TL;DR: UniMo框架通过监督微调和强化学习整合运动-语言信息与可解释思维链，解决现有3D人体运动生成与理解方法在语义对齐、任务一致性和累积预测误差方面的限制，在两项任务上均取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体运动生成与理解方法可解释性有限，限制了这两个内在相关任务之间的有效相互增强。基于大语言模型的统一框架虽然利用了语言先验，但经常面临语义对齐和任务一致性的挑战。此外，LLM中的下一个token预测范式不适合运动序列，会导致累积预测误差。

Method: 提出UniMo框架：1) 通过监督微调将运动-语言信息和可解释思维链推理整合到LLM中；2) 引入强化学习与组相对策略优化作为后训练策略，通过优化token组来强制结构正确性和语义对齐，减轻运动token预测中的累积误差。

Result: 大量实验表明，UniMo在运动生成和理解两方面均显著优于现有的统一框架和任务特定模型，实现了最先进的性能。

Conclusion: UniMo通过整合运动-语言信息、可解释思维链推理以及创新的强化学习后训练策略，有效解决了3D人体运动生成与理解中的语义对齐、任务一致性和累积误差问题，为这两个相关任务提供了统一的增强框架。

Abstract: Existing 3D human motion generation and understanding methods often exhibit limited interpretability, restricting effective mutual enhancement between these inherently related tasks. While current unified frameworks based on large language models (LLMs) leverage linguistic priors, they frequently encounter challenges in semantic alignment and task coherence. Moreover, the next-token prediction paradigm in LLMs is ill-suited for motion sequences, causing cumulative prediction errors. To address these limitations, we propose UniMo, a novel framework that integrates motion-language information and interpretable chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT). We further introduce reinforcement learning with Group Relative Policy Optimization (GRPO) as a post-training strategy that optimizes over groups of tokens to enforce structural correctness and semantic alignment, mitigating cumulative errors in motion token prediction. Extensive experiments demonstrate that UniMo significantly outperforms existing unified and task-specific models, achieving state-of-the-art performance in both motion generation and understanding.

</details>


### [171] [DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants](https://arxiv.org/abs/2601.12138)
*Abhishek Kumar,Riya Tapwal,Carsten Maple*

Main category: cs.AI

TL;DR: DriveSafe：针对LLM驾驶助手的四层次风险分类法，包含129个细粒度风险类别，评估显示现有模型在驾驶场景中安全拒绝能力不足


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地集成到车载数字助手中，但不安全、模糊或法律错误的响应可能导致严重的安全、伦理和监管后果。现有风险评估分类法多为通用型，无法捕捉真实驾驶场景中的领域特定风险。

Method: 提出了DriveSafe——一个分层的四层次风险分类法，包含129个细粒度原子风险类别，涵盖技术、法律、社会和伦理维度。该方法基于真实驾驶法规和安全原则构建，并由领域专家评审。通过评估六个广泛部署的LLM在构建提示上的拒绝行为来验证安全相关性和真实性。

Result: 评估显示，被测试的模型经常无法适当拒绝不安全或不合规的驾驶相关查询，突显了通用安全对齐在驾驶上下文中的局限性。

Conclusion: 需要针对驾驶领域的特定风险评估框架，现有LLM安全对齐在驾驶场景中不足，DriveSafe分类法为系统评估LLM驾驶助手安全性提供了基础。

Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.

</details>


### [172] [TIDE: A Trace-Informed Depth-First Exploration for Planning with Temporally Extended Goals](https://arxiv.org/abs/2601.12141)
*Yuliia Suprun,Khen Elimelech,Lydia E. Kavraki,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: TIDE是一种用于时序扩展目标规划的新方法，通过将时序问题分解为可管理的子问题，利用成本驱动启发式和自适应回溯机制提高规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统LTLf任务规划方法将时序规划问题转化为经典规划问题，但缺乏针对时序目标的启发式引导搜索，导致效率受限。

Method: TIDE将时序问题分解为一系列可达-规避子问题，利用成本驱动启发式识别和优先处理有前景的自动机轨迹，并采用自适应回溯机制从失败计划中恢复。

Result: 实验结果表明TIDE实现了有前景的性能，是时序扩展目标规划方法组合中的有价值补充。

Conclusion: TIDE通过分解策略、启发式引导和自适应回溯，有效解决了传统LTLf规划中缺乏时序目标启发式引导的问题，提高了规划效率和完备性。

Abstract: Task planning with temporally extended goals (TEGs) is a critical challenge in AI and robotics, enabling agents to achieve complex sequences of objectives over time rather than addressing isolated, immediate tasks. Linear Temporal Logic on finite traces (LTLf ) provides a robust formalism for encoding these temporal goals. Traditional LTLf task planning approaches often transform the temporal planning problem into a classical planning problem with reachability goals, which are then solved using off-the-shelf planners. However, these methods often lack informed heuristics to provide a guided search for temporal goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel approach that addresses this limitation by decomposing a temporal problem into a sequence of smaller, manageable reach-avoid sub-problems, each solvable using an off-the-shelf planner. TIDE identifies and prioritizes promising automaton traces within the domain graph, using cost-driven heuristics to guide exploration. Its adaptive backtracking mechanism systematically recovers from failed plans by recalculating costs and penalizing infeasible transitions, ensuring completeness and efficiency. Experimental results demonstrate that TIDE achieves promising performance and is a valuable addition to the portfolio of planning methods for temporally extended goals.

</details>


### [173] [Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration](https://arxiv.org/abs/2601.12256)
*Jinyoung Park,Minseong Bae,Jeehye Na,Hyunwoo J. Kim*

Main category: cs.AI

TL;DR: CoLLaMo是一个基于大语言模型的分子助手，通过多级分子模态协作投影器整合1D序列、2D分子图和3D构象信息，解决了现有大分子语言模型的幻觉和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大分子语言模型（LMLMs）通常存在幻觉问题和有限的鲁棒性，这主要是由于未能充分整合1D序列、2D分子图和3D构象等多种分子模态信息。需要开发能够有效整合多模态分子信息的模型来提升分子理解和生成能力。

Method: 提出CoLLaMo模型，包含：1）多级分子模态协作投影器，通过关系感知的模态协作注意力机制实现原子间的细粒度信息交换；2）引入2D结构关系和3D空间关系指导信息传递；3）提出新的分子中心自动评估方法，包括幻觉评估指标和基于GPT的标题质量评估，以替代传统的基于token的通用评估指标（如BLEU）。

Result: CoLLaMo在多个任务上取得了最佳性能，包括分子标题生成、计算性质问答、描述性质问答、基序计数和IUPAC名称预测。实验表明该模型增强了LMLMs的分子模态泛化能力。

Conclusion: 通过整合多级分子模态信息并采用关系感知的协作注意力机制，CoLLaMo有效解决了现有大分子语言模型的幻觉和鲁棒性问题，在多种分子理解任务上表现出优越性能。同时提出的分子中心评估方法为更准确地评估分子理解能力提供了新思路。

Abstract: Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.

</details>


### [174] [ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents](https://arxiv.org/abs/2601.12294)
*Dawei Li,Yuguang Yao,Zhen Tan,Huan Liu,Ruocheng Guo*

Main category: cs.AI

TL;DR: ToolPRMBench：首个专门评估工具使用智能体过程奖励模型（PRMs）的大规模基准，通过离线/在线采样和多LLM验证构建高质量测试集


<details>
  <summary>Details</summary>
Motivation: 现有奖励引导搜索方法在工具使用智能体中表现出潜力，但缺乏系统可靠的PRM评估基准，阻碍了该领域的发展

Method: 基于多个代表性工具使用基准构建ToolPRMBench，将智能体轨迹转换为步骤级测试用例，包含交互历史、正确动作、合理但不正确的替代方案和工具元数据；采用离线采样隔离单步错误和在线采样捕获多步失败；设计多LLM验证流程降低标签噪声

Result: 实验涵盖大语言模型、通用PRMs和工具专用PRMs，结果显示PRM有效性存在明显差异，工具专用PRMs展现出显著潜力

Conclusion: ToolPRMBench填补了工具使用PRM评估的空白，为未来研究提供了可靠基准，代码和数据已开源

Abstract: Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.

</details>


### [175] [Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence](https://arxiv.org/abs/2601.12318)
*Dehao Ying,Fengchang Yu,Haihua Chen,Changjiang Jiang,Yurong Li,Wei Lu*

Main category: cs.AI

TL;DR: 本文首次建立了文档智能数据生成的综合技术图谱，重新定义数据生成为监督信号生产，基于"数据和标签可用性"提出新分类法，涵盖数据增强、从零生成、自动标注和自监督信号构建四大范式，并建立多级评估框架。


<details>
  <summary>Details</summary>
Motivation: 文档智能发展需要大规模高质量训练数据，但人工标注成为关键瓶颈。现有数据生成方法调查局限于单一模态或特定任务，缺乏与现实工作流程统一视角，需要建立全面技术框架填补这一空白。

Method: 将数据生成重新定义为监督信号生产，基于"数据和标签可用性"引入新分类法，组织为四大资源中心范式：数据增强、从零生成、自动标注和自监督信号构建。建立多级评估框架整合内在质量和外在效用，编译多样化DI基准性能增益。

Result: 创建了首个文档智能数据生成综合技术图谱，系统化整理了碎片化领域，揭示了保真度差距等关键挑战和协同进化生态系统等前沿方向，展示了数据生成方法在多样化DI基准上的性能提升。

Conclusion: 通过系统化碎片化领域，将数据生成定位为下一代文档智能的核心引擎，为领域发展提供了统一框架和未来研究方向。

Abstract: The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the "availability of data and labels." This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.

</details>


### [176] [MARO: Learning Stronger Reasoning from Social Interaction](https://arxiv.org/abs/2601.12323)
*Yin Cai,Zhouhong Gu,Juntao Zhang,Ping Chen*

Main category: cs.AI

TL;DR: MARO是一种多智能体奖励优化方法，通过在多智能体社交环境中学习和实践来增强大语言模型的推理能力，解决了稀疏学习信号、角色分布不均和环境不稳定等问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型训练方法主要让模型从现有文本内容学习或解决预定问题，缺乏在真实场景中与他人互动、协商和竞争的经验，无法应对需要复杂社会推理的日常场景。

Method: 提出Multi-Agent Reward Optimization (MARO)方法：1) 将最终成败结果分解为交互过程中的具体行为，解决稀疏学习信号问题；2) 通过平衡不同角色的训练样本权重，处理角色分布不均问题；3) 直接评估每个行为的效用，解决环境不稳定问题。

Result: 实验结果表明，MARO不仅在社会推理能力上取得显著提升，而且通过社交模拟学习获得的能力能够有效迁移到数学推理和指令跟随等其他任务中。

Conclusion: 多智能体社交学习在增强大语言模型通用推理能力方面具有巨大潜力，MARO方法为解决模型在复杂社会环境中学习的问题提供了有效途径。

Abstract: Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.

</details>


### [177] [Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations](https://arxiv.org/abs/2601.12338)
*Kartikey Singh Bhandari,Manav Ganesh,Yashwant Viswanathan,Archit Agrawal,Dhruv Kumar,Pratik Narang*

Main category: cs.AI

TL;DR: 提出一个两阶段LLM框架，通过问题提取和专家混合适配器生成可操作的业务建议，在Yelp评论上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 客户评论包含丰富的服务失败和用户期望信号，但将非结构化反馈转化为可操作的业务决策仍然困难，需要自动化生成具体可行的建议。

Method: 提出模块化两阶段LLM框架：1) 问题模型提取关键问题并分配粗粒度主题；2) 建议模型基于问题表示生成针对性操作方案。采用LoRA专家混合策略，训练多个低秩适配器并通过轻量级门控机制在推理时进行token级专家混合。

Result: 在Yelp评论（航空和餐厅领域）构建的合成数据集上评估，使用包含8个维度的操作评估标准。该方法在行动性、特异性等关键指标上持续优于仅提示和单适配器基线，同时保持效率与质量的良好平衡。

Conclusion: 提出的两阶段框架结合专家混合适配器策略，能够有效从客户评论生成具体可操作的建议，为业务决策提供实用工具，在多个领域展现出优越性能。

Abstract: Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.

</details>


### [178] [PsychēChat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling](https://arxiv.org/abs/2601.12392)
*Zhentao Xia,Yongqi Fan,Yuxiang Chu,Yichao Yin,Liangliang Chen,Tong Ruan,Weiyan Zhang*

Main category: cs.AI

TL;DR: PsychēChat是一个用于心理咨询的LLM系统，通过显式建模来访者情绪变化和安全性风险分析来提升心理咨询质量，提出了两种建模范式：多智能体协作的Agent Mode和端到端推理的LLM Mode。


<details>
  <summary>Details</summary>
Motivation: 现有心理咨询模型通常不显式建模来访者在咨询过程中的情绪变化，这是经典心理学流派的核心关注点。同时，如何使咨询师模型的回应与这些情绪变化对齐，并主动缓解安全风险，这些问题尚未得到充分探索。

Method: 提出PsychēChat系统，通过交互式角色扮演合成咨询师-来访者对话，包含两个核心模块：情绪管理模块（捕捉来访者当前情绪和情绪变化）和风险控制模块（预测来访者后续反应并识别潜在风险）。引入两种建模范式：Agent Mode（将情绪管理、风险控制和咨询师回应构建为协作多智能体管道）和LLM Mode（将这些阶段整合到统一的思维链中进行端到端推理）。

Result: 通过交互式评分、对话级评估和人工评估等广泛实验表明，PsychēChat在情感洞察和安全性控制方面优于现有方法。

Conclusion: PsychēChat通过显式整合情绪变化追踪和安全性风险分析，有效提升了心理咨询模型的情感洞察能力和安全性控制，为心理咨询AI系统的发展提供了新思路。

Abstract: Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychēChat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychēChat outperforms existing methods for emotional insight and safety control.

</details>


### [179] [Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation](https://arxiv.org/abs/2601.12410)
*Dingyi Yang,Junqi Zhao,Xue Li,Ce Li,Boyang Li*

Main category: cs.AI

TL;DR: LLMs在知识状态追踪和估计任务中表现接近随机水平，显著低于人类表现，未来研究应更重视知识估计和意图理解能力


<details>
  <summary>Details</summary>
Motivation: 认知人类学认为人类智能的关键在于推断他人知识状态和理解意图的能力，而黑猩猩等动物缺乏这种能力。本文旨在评估LLM在知识状态追踪和估计方面的表现。

Method: 设计两个任务测试LLM：(1)检测故事角色是否通过行动表现出本不应拥有的知识；(2)基于角色自身知识vs客观事实预测角色下一步行动。

Result: 大多数当前最先进的LLM在两个任务上都达到接近随机的性能水平，显著低于人类表现。

Conclusion: 未来LLM研究应更重视知识估计和意图理解能力的发展。

Abstract: Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.

</details>


### [180] [Large Language Model for OWL Proofs](https://arxiv.org/abs/2601.12444)
*Hui Yang,Jiaoyan Chen,Uli Sattler*

Main category: cs.AI

TL;DR: 该研究评估大语言模型在OWL本体论中生成忠实、可读证明的能力，发现模型在复杂案例中仍有局限，逻辑复杂性是主要影响因素，数据噪声会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在推理任务上的能力已被广泛研究，但它们在生成忠实、人类可读的证明（解释结论为何成立）方面的能力仍未充分探索。本研究旨在评估LLMs在OWL本体论这一广泛用于表示和推理复杂知识的语境下的证明生成能力。

Method: 开发了自动化数据集构建和评估框架，评估三个顺序任务：提取、简化和解释，以及一个额外任务：评估前提的逻辑完备性。在广泛使用的推理LLMs上进行了大量实验。

Result: 1. 某些模型总体表现强劲，但在复杂案例中仍有局限；2. 逻辑复杂性（而非表示格式：形式逻辑语言vs自然语言）是影响LLM性能的主导因素；3. 输入数据中的噪声和不完整性会显著降低LLMs的性能。

Conclusion: 这些结果既显示了LLMs在严格逻辑解释方面的潜力，也揭示了在复杂或不完美条件下支持弹性推理的差距。代码和数据已开源。

Abstract: The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.

</details>


### [181] [Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck](https://arxiv.org/abs/2601.12499)
*Meiru Zhang,Zaiqiao Meng,Nigel Collier*

Main category: cs.AI

TL;DR: 论文提出MFAI方法探究LLMs多跳推理失败原因，发现"最弱环节定律"：多跳推理性能受限于最不显著证据的可见性，且失败由绝对位置而非事实间距离决定。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs具有大规模上下文窗口，但在多跳推理中存在位置偏见问题，导致忽略某些位置的信息。需要区分这种失败是由于无法定位证据（识别失败）还是无法整合证据（合成失败）。

Method: 引入多焦点注意力指令（MFAI）作为语义探针，通过显式引导注意力到选定位置来解耦识别和合成机制。在5个LLMs上测试两个多跳QA任务（MuSiQue和NeoQA）。

Result: 1. 确立"最弱环节定律"：多跳推理性能崩溃到最不显著证据的性能水平；2. 失败由绝对位置而非事实间线性距离决定（性能方差<3%）；3. MFAI匹配时可解决识别瓶颈，在低可见性位置提升准确率达11.5%；4. 误导性MFAI在真实任务中引发混淆但在合成任务中被成功过滤；5. 使用系统2推理的"思考"模型能有效定位和整合信息，在噪声长上下文设置中匹配黄金基线。

Conclusion: LLMs的多跳推理失败主要由识别瓶颈而非合成失败驱动，位置偏见是关键因素。MFAI能有效诊断和缓解识别问题，而系统2推理模型能克服这些限制，实现稳健的多跳推理。

Abstract: Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the "Weakest Link Law": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that "thinking" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.

</details>


### [182] [Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538)
*Tianxin Wei,Ting-Wei Li,Zhining Liu,Xuying Ning,Ze Yang,Jiaru Zou,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Dongqi Fu,Zihao Li,Mengting Ai,Duo Zhou,Wenxuan Bao,Yunzhe Li,Gaotang Li,Cheng Qian,Yu Wang,Xiangru Tang,Yin Xiao,Liri Fang,Hui Liu,Xianfeng Tang,Yuji Zhang,Chi Wang,Jiaxuan You,Heng Ji,Hanghang Tong,Jingrui He*

Main category: cs.AI

TL;DR: 该综述将智能体推理组织为三个互补维度：基础智能体推理（单智能体能力）、自演化智能体推理（通过反馈和记忆进行适应）和集体多智能体推理（协作环境），并区分上下文推理与后训练推理方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在封闭环境中表现出强大的推理能力，但在开放和动态环境中表现不佳。智能体推理通过将LLMs重构为能够规划、行动和持续学习的自主智能体，实现了范式转变，旨在解决LLMs在开放环境中的局限性。

Method: 该综述采用三维框架组织智能体推理：1）基础智能体推理（单智能体在稳定环境中的规划、工具使用和搜索）；2）自演化智能体推理（通过反馈、记忆和适应机制优化能力）；3）集体多智能体推理（协作环境中的协调、知识共享和共同目标）。同时区分上下文推理（通过结构化编排扩展测试时交互）和后训练推理（通过强化学习和监督微调优化行为）。

Result: 综述系统性地回顾了智能体推理框架在现实世界应用和基准测试中的代表性工作，涵盖科学、机器人、医疗保健、自主研究和数学等领域，将智能体推理方法综合成一个统一的路线图，连接思维与行动。

Conclusion: 智能体推理代表了连接思维与行动的范式转变，该综述提供了一个统一的组织框架。未来挑战包括个性化、长时程交互、世界建模、可扩展的多智能体训练以及现实世界部署的治理问题。

Abstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.

</details>


### [183] [MemeLens: Multilingual Multitask VLMs for Memes](https://arxiv.org/abs/2601.12539)
*Ali Ezzat Shahroor,Mohamed Bayan Kmainasi,Abul Hasnat,Dimitar Dimitrov,Giovanni Da San Martino,Preslav Nakov,Firoj Alam*

Main category: cs.AI

TL;DR: MemeLens：统一的多语言多任务解释增强视觉语言模型，用于表情包理解，整合了38个公开数据集，映射到20个任务的共享分类法中


<details>
  <summary>Details</summary>
Motivation: 现有表情包研究分散在不同任务（仇恨、厌女、宣传、情感、幽默）和语言中，限制了跨领域泛化能力，需要统一的多语言多任务解决方案

Method: 提出MemeLens模型，整合38个公开表情包数据集，将数据集特定标签过滤并映射到包含危害、目标、比喻/语用意图和情感的20个任务的共享分类法中，采用解释增强的视觉语言模型架构

Result: 实证分析表明：稳健的表情包理解需要多模态训练；不同语义类别间存在显著差异；在单个数据集上微调而非统一训练时容易过度专业化

Conclusion: MemeLens为表情包理解提供了统一的多语言多任务框架，揭示了多模态训练的重要性，并展示了过度专业化的问题，将公开实验资源和数据集供社区使用

Abstract: Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.

</details>


### [184] [Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery](https://arxiv.org/abs/2601.12542)
*Lukas Weidener,Marko Brkić,Mihailo Jovanović,Ritvik Singh,Chiara Baccin,Emre Ulgac,Alex Dobrin,Aakaash Meduri*

Main category: cs.AI

TL;DR: Deep Research是一个多智能体系统，能够在几分钟内完成交互式科学研究，超越了传统批量处理模式，支持半自主和全自主两种工作模式。


<details>
  <summary>Details</summary>
Motivation: 现有AI科学发现系统大多是专有的，采用批量处理模式，每个研究周期需要数小时，无法实现实时研究者指导，限制了AI在科学研究中的实际应用。

Method: 采用多智能体架构，包含规划、数据分析、文献搜索和新颖性检测等专门化智能体，通过持久世界状态保持跨迭代研究周期的上下文，支持半自主（选择性人工检查点）和全自主两种操作模式。

Result: 在BixBench计算生物学基准测试中取得最先进性能：开放回答准确率48.8%，多项选择题准确率64.5%，比现有基线提高14-26个百分点。

Conclusion: Deep Research展示了AI辅助科学工作流程的可行性，但实际部署需要考虑开放获取文献限制和自动新颖性评估等架构约束，为实时交互式科学研究提供了新范式。

Abstract: Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.

</details>


### [185] [How Clinicians Think and What AI Can Learn From It](https://arxiv.org/abs/2601.12547)
*Dipayan Sengupta,Saumya Panda*

Main category: cs.AI

TL;DR: 论文主张临床AI应从预测引擎转向基于序数、非补偿性决策的临床推理框架，采用稳健的启发式规则而非期望效用优化


<details>
  <summary>Details</summary>
Motivation: 当前临床AI系统主要作为预测引擎（生成标签或风险评分），但真实的临床推理是时间受限、序列化的控制问题，涉及不确定性下的信息收集与不可逆行动。临床医生依赖快速节俭的词典式启发式方法，而非基数优化

Method: 提出临床推理的计算基础应是序数、非补偿性决策：1）临床权衡通常基于人类判断，在绝对尺度上难以精确测量，只有序数关系具有不变性；2）偏好和信号获取存在结构性粗糙度，导致持续不确定性，期望效用优化在决策边际被粗糙度淹没时变得脆弱；3）采用稳健的支配/过滤规则（如ε-支配、极大极小）来稳定决策

Result: 论证了快速节俭启发式方法不仅是有限理性捷径，在医学中具有认识论优势：序数决策在测量公理薄弱时更稳健；当结构性粗糙度超过决策边际时，期望效用优化易受小扰动影响（高翻转概率），而稳健规则能稳定决策

Conclusion: 提出临床对齐的AI蓝图：使用丰富模型进行信念和轨迹建模，但通过稳健序数规则选择行动；将启发式视为低维特例；部署AI作为"选择性复杂性"——主要在决策脆弱且信息具有正期望影响时用于打破平局

Abstract: Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\to$ perception $\to$ inference $\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($ε$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.

</details>


### [186] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: 该论文提出了一个统一的智能体AI分类框架，将智能体分解为感知、大脑、规划、行动、工具使用和协作六个核心组件，并分析了从线性推理到原生推理模型的演进趋势。


<details>
  <summary>Details</summary>
Motivation: 随着AI从仅生成文本的模型向能够感知、推理、规划和行动的自主智能体系统演进，出现了从简单单循环智能体到分层多智能体系统的多样化设计，使得这一领域难以系统化理解。需要统一的分类框架来梳理智能体架构的演进脉络。

Method: 提出一个统一的分类法，将智能体分解为六个核心组件：感知、大脑、规划、行动、工具使用和协作。使用这一框架分析从线性推理过程到原生推理时间推理模型的演进，以及从固定API调用到开放标准（如模型上下文协议和原生计算机使用）的转变。同时分类智能体运行环境，并回顾当前评估实践。

Result: 建立了一个系统化的智能体AI分类框架，明确了智能体架构的核心组件和演进趋势。识别了智能体运行的主要环境类型（数字操作系统、具身机器人、专业领域），并总结了当前评估方法。指出了幻觉行为、无限循环、提示注入等开放挑战。

Conclusion: 智能体AI正从被动知识引擎向自主认知控制器演进，统一的分类框架有助于理解这一复杂领域。未来研究需要解决幻觉、安全性和可靠性等关键挑战，推动更稳健可靠的自主系统发展。

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [187] [STEP-LLM: Generating CAD STEP Models from Natural Language with Large Language Models](https://arxiv.org/abs/2601.12641)
*Xiangyu Shi,Junyang Ding,Xu Zhao,Sinong Zhan,Payal Mohapatra,Daniel Quispe,Kojo Welbeck,Jian Cao,Wei Chen,Ping Guo,Qi Zhu*

Main category: cs.AI

TL;DR: STEP-LLM：通过大语言模型从自然语言生成STEP格式CAD模型，解决传统文本到CAD方法的局限性，提高几何保真度和制造兼容性。


<details>
  <summary>Details</summary>
Motivation: 传统CAD建模需要专业知识且耗时，现有基于命令序列或脚本的文本到CAD方法存在内核依赖性和制造兼容性问题。STEP作为广泛采用的中性边界表示格式直接兼容制造，但其图结构、交叉引用的特性对自回归LLM构成挑战。

Method: 1. 构建约40K STEP-文本描述对数据集；2. 针对STEP图结构格式设计预处理：基于深度优先搜索的重新序列化线性化交叉引用并保持局部性，以及CoT风格结构注释指导全局一致性；3. 集成检索增强生成进行监督微调；4. 使用基于Chamfer距离的几何奖励进行强化学习优化生成质量。

Result: STEP-LLM在几何保真度上持续优于Text2CAD基线。RAG模块显著提升完整性和可渲染性，DFS重新序列化增强整体准确性，RL进一步减少几何差异。指标和视觉比较均证实STEP-LLM生成形状具有更高保真度。

Conclusion: 证明了LLM驱动从自然语言生成STEP模型的可行性，展示了其在民主化制造CAD设计方面的潜力，为更通用、制造兼容的文本到CAD系统提供了新方向。

Abstract: Computer-aided design (CAD) is vital to modern manufacturing, yet model creation remains labor-intensive and expertise-heavy. To enable non-experts to translate intuitive design intent into manufacturable artifacts, recent large language models-based text-to-CAD efforts focus on command sequences or script-based formats like CadQuery. However, these formats are kernel-dependent and lack universality for manufacturing. In contrast, the Standard for the Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral boundary representation (B-rep) format directly compatible with manufacturing, but its graph-structured, cross-referenced nature poses unique challenges for auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption pairs and introduce novel preprocessing tailored for the graph-structured format of STEP, including a depth-first search-based reserialization that linearizes cross-references while preserving locality and chain-of-thought(CoT)-style structural annotations that guide global coherence. We integrate retrieval-augmented generation to ground predictions in relevant examples for supervised fine-tuning, and refine generation quality through reinforcement learning with a specific Chamfer Distance-based geometric reward. Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity over the Text2CAD baseline, with improvements arising from multiple stages of our framework: the RAG module substantially enhances completeness and renderability, the DFS-based reserialization strengthens overall accuracy, and the RL further reduces geometric discrepancy. Both metrics and visual comparisons confirm that STEP-LLM generates shapes with higher fidelity than Text2CAD. These results show the feasibility of LLM-driven STEP model generation from natural language, showing its potential to democratize CAD design for manufacturing.

</details>


### [188] [Logic-Guided Multistage Inference for Explainable Multidefendant Judgment Prediction](https://arxiv.org/abs/2601.12688)
*Xu Zhang,Qinghua Wang,Mengyang Zhao,Fang Wang,Cunquan Qu*

Main category: cs.AI

TL;DR: 提出MMSI框架，将量刑逻辑融入Transformer编码器，通过定向掩蔽机制澄清多被告案件中的角色，提升AI司法辅助的准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 多被告案件中责任分配复杂，司法表述常模糊被告角色，阻碍AI分析。需要提升智能司法系统在区分主从犯责任方面的能力，同时确保法律可解释性

Method: 提出MMSI框架：1) 将量刑逻辑融入预训练Transformer编码器；2) 定向掩蔽机制澄清被告角色；3) 对比数据构建策略增强模型对主从犯责任区分的敏感性；4) 通过广播将预测的罪名标签纳入回归模型，整合犯罪描述和法庭观点

Result: 在自定义的IMLJP故意伤害案件数据集上评估，MMSI框架在角色责任区分方面取得显著准确率提升，优于基线方法

Conclusion: 该工作为增强智能司法系统提供了稳健解决方案，能有效区分多被告案件中的责任，代码已公开

Abstract: Crime disrupts societal stability, making law essential for balance. In multidefendant cases, assigning responsibility is complex and challenges fairness, requiring precise role differentiation. However, judicial phrasing often obscures the roles of the defendants, hindering effective AI-driven analyses. To address this issue, we incorporate sentencing logic into a pretrained Transformer encoder framework to enhance the intelligent assistance in multidefendant cases while ensuring legal interpretability. Within this framework an oriented masking mechanism clarifies roles and a comparative data construction strategy improves the model's sensitivity to culpability distinctions between principals and accomplices. Predicted guilt labels are further incorporated into a regression model through broadcasting, consolidating crime descriptions and court views. Our proposed masked multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset for intentional injury cases, achieves significant accuracy improvements, outperforming baselines in role-based culpability differentiation. This work offers a robust solution for enhancing intelligent judicial systems, with publicly code available.

</details>


### [189] [Teaching Large Reasoning Models Effective Reflection](https://arxiv.org/abs/2601.12720)
*Hanbin Wang,Jingwei Song,Jinpeng Li,Qi Zhu,Fei Mi,Ganqu Cui,Yasheng Wang,Lifeng Shang*

Main category: cs.AI

TL;DR: 本文提出SCFT和RLERR方法解决大型推理模型中的表面反思问题，通过自我批判微调和有效反思奖励强化学习提升推理准确性和反思质量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但许多反思是表面的，无法显著改进原始答案且增加计算开销。本文旨在识别并解决LRMs中的表面反思问题。

Method: 1. 提出自我批判微调框架：让模型批判自身输出，通过拒绝采样筛选高质量批判，使用批判目标微调模型。2. 引入有效反思奖励强化学习：利用SCFT初始化高质量反思构建奖励信号，通过强化学习内化自我修正过程。

Result: 在AIME2024和AIME2025两个挑战性基准测试中，SCFT和RLERR显著提高了推理准确性和反思质量，超越了最先进的基线方法。

Conclusion: SCFT和RLERR方法有效解决了大型推理模型中的表面反思问题，通过增强模型的反思推理能力和内化自我修正过程，显著提升了复杂推理任务的性能。

Abstract: Large Reasoning Models (LRMs) have recently shown impressive performance on complex reasoning tasks, often by engaging in self-reflective behaviors such as self-critique and backtracking. However, not all reflections are beneficial-many are superficial, offering little to no improvement over the original answer and incurring computation overhead. In this paper, we identify and address the problem of superficial reflection in LRMs. We first propose Self-Critique Fine-Tuning (SCFT), a training framework that enhances the model's reflective reasoning ability using only self-generated critiques. SCFT prompts models to critique their own outputs, filters high-quality critiques through rejection sampling, and fine-tunes the model using a critique-based objective. Building on this strong foundation, we further introduce Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR leverages the high-quality reflections initialized by SCFT to construct reward signals, guiding the model to internalize the self-correction process via reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and AIME2025, show that SCFT and RLERR significantly improve both reasoning accuracy and reflection quality, outperforming state-of-the-art baselines. All data and codes are available at https://github.com/wanghanbinpanda/SCFT.

</details>


### [190] [VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension](https://arxiv.org/abs/2601.12781)
*Hyejin Park,Junhyuk Kwon,Suha Kwak,Jungseul Ok*

Main category: cs.AI

TL;DR: VIRO框架通过集成轻量级验证器到神经符号推理步骤中，解决REC任务中因中间步骤错误导致的级联错误问题，显著提升无目标场景下的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号REC方法依赖LLM和VLM进行组合推理，但假设中间推理步骤准确，导致级联错误：错误检测和无效关系在推理链中传播，即使图像中没有目标也会产生高置信度的假阳性结果。

Method: 提出验证集成推理算子(VIRO)框架，在推理步骤中嵌入轻量级的算子级验证器。每个算子执行并验证其输出（如对象存在性或空间关系），当验证条件不满足时，系统能够鲁棒地处理无目标情况。

Result: 在目标存在和无目标设置下达到61.1%的平衡准确率，实现最先进性能；在真实世界自我中心数据上展示泛化能力；计算效率高（吞吐量）、可靠性强（程序失败率低于0.3%），并通过解耦程序生成与执行实现可扩展性。

Conclusion: VIRO框架通过集成验证机制有效解决了神经符号REC中的级联错误问题，在准确性、鲁棒性、效率和可靠性方面均表现出色，为处理无目标场景提供了有效解决方案。

Abstract: Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.

</details>


### [191] [SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability](https://arxiv.org/abs/2601.12804)
*Hanwei Zhang,Luo Cheng,Rui Wen,Yang Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: SL-CBM通过引入语义局部性增强概念瓶颈模型，生成空间一致的概念和类别显著性图，提高局部忠实性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型缺乏局部忠实性，无法将概念与有意义的图像区域空间对齐，限制了可解释性和可靠性

Method: 提出SL-CBM，集成1x1卷积层和交叉注意力机制，增强概念、图像区域和最终预测之间的对齐，生成空间一致的显著性图

Result: 在图像数据集上显著提高了局部忠实性、解释质量和干预效果，同时保持竞争力的分类准确率

Conclusion: SL-CBM填补了概念推理和空间可解释性之间的空白，为可解释和可信的概念模型设定了新标准

Abstract: Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.

</details>


### [192] [Human Emotion Verification by Action Languages via Answer Set Programming](https://arxiv.org/abs/2601.12912)
*Andreas Brännström,Juan Carlos Nieves*

Main category: cs.AI

TL;DR: 本文介绍了基于回答集编程和转移系统的动作语言C-MT，用于形式化人类心理状态随可观察动作序列的演化，特别关注情绪等心理状态的多维配置建模。


<details>
  <summary>Details</summary>
Motivation: 需要控制智能体行为并限制动作带来的不良心理副作用，同时基于心理学理论（如情绪评价理论）形式化心理状态演化，为情绪验证等应用提供理论基础。

Method: 在回答集编程和转移系统基础上构建C-MT语言，引入"forbids to cause"因果规则和专门的心理状态动态表达式，将心理变化原则转化为转移约束和不变性属性，通过轨迹分析进行严格评估。

Result: 开发了能够控制推理人类心理状态动态演化的框架，支持通过分析遵循不同心理学原则的轨迹来比较不同的变化动态，并将该动作语言应用于情绪验证模型设计。

Conclusion: C-MT语言为形式化心理状态演化提供了严谨的计算框架，能够建模心理变化原则并支持受控推理，在情绪验证等应用中具有实用价值，目前正在TPLP期刊审稿中。

Abstract: In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [193] [Actionable Interpretability Must Be Defined in Terms of Symmetries](https://arxiv.org/abs/2601.12913)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Francesco Giannini,Alberto Termine,Filippo Bonchi,Mateja Jamnik,Giuseppe Marra*

Main category: cs.AI

TL;DR: 该论文认为当前AI可解释性研究存在根本性问题，因为现有定义缺乏可操作性，无法提供具体的建模和推理规则。作者提出基于对称性的可操作性定义，并假设四种对称性足以解决可解释性研究的核心问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI可解释性研究面临根本性挑战，现有定义缺乏可操作性，无法为具体建模和推理提供形式化原则。这导致可解释性研究难以产生一致的理论框架和实用方法。

Method: 提出基于对称性的可操作性定义框架，假设四种对称性足以：1）激发核心可解释性属性；2）刻画可解释模型类别；3）推导统一的可解释推理形式（如对齐、干预和反事实）作为贝叶斯逆问题。

Result: 论文提出了一个理论框架，将可解释性定义为对称性属性，为可解释性研究提供了形式化基础。该框架能够统一处理对齐、干预和反事实推理等可解释性任务。

Conclusion: 基于对称性的可操作性定义为解决AI可解释性研究的根本问题提供了新途径，有望建立统一的理论框架，推动可解释性研究从概念讨论转向具体建模和推理方法。

Abstract: This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.

</details>


### [194] [MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux](https://arxiv.org/abs/2601.13060)
*Zecheng Li,Zhihui Cao,Wenke Huang,Yudong Zhang,Keying Qi,Rui Wang,Zeyu Zheng,Jian Zhao,Hao Zhu,Hengxin Wu,Yuran Wang,Guitao Fan,Guokun Wu,Yicong Liu,Zhilin Gao,Haikun Xu,He Yang,Minqi Xiang,Xingyu Liu,Zuojian Wang*

Main category: cs.AI

TL;DR: MagicGUI-RMS：一个多智能体奖励模型系统，用于GUI智能体的自适应轨迹评估、纠正反馈和自我进化学习，通过领域特定和通用奖励模型结合实现细粒度动作评估


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体面临两大挑战：1) 自动化评估智能体轨迹的困难；2) 大规模生成高质量训练数据以实现持续改进。现有方法依赖人工标注或静态规则验证，限制了可扩展性和动态环境适应性

Method: 集成领域特定奖励模型(DS-RM)和通用奖励模型(GP-RM)，实现细粒度动作评估和跨异构GUI任务的鲁棒泛化。设计结构化数据构建管道，自动生成平衡多样的奖励数据集，降低标注成本。通过自动数据回流机制识别错误动作、提出改进方案并持续增强智能体行为

Result: 实验表明MagicGUI-RMS在任务准确性和行为鲁棒性方面取得显著提升，为构建基于奖励适应的自改进GUI智能体提供了原则性和有效的基础

Conclusion: MagicGUI-RMS通过多智能体奖励模型系统解决了GUI智能体评估和训练数据生成的关键挑战，实现了自适应轨迹评估、纠正反馈和自我进化学习能力，为自改进GUI智能体提供了有效框架

Abstract: Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.

</details>


### [195] [Responsible AI for General-Purpose Systems: Overview, Challenges, and A Path Forward](https://arxiv.org/abs/2601.13122)
*Gourab K Patro,Himanshi Agrawal,Himanshu Gharat,Supriya Panigrahi,Nim Sherpa,Vishal Vaddina,Dagnachew Birru*

Main category: cs.AI

TL;DR: 该论文分析了现代通用AI系统在负责任AI原则下的风险，提出其高自由度输出是问题根源，并建立了C2V2框架（控制、一致性、价值、真实性）作为未来通用AI系统的负责任设计标准。


<details>
  <summary>Details</summary>
Motivation: 现代通用AI系统（如大语言和视觉模型）虽然功能强大，但在输出中存在幻觉、毒性和刻板印象等风险，使其不可信。作者旨在分析这些系统在八大负责任AI原则下的脆弱性，并与传统任务特定AI系统进行比较，提出需要重新思考通用AI的负责任AI方法。

Method: 1. 从八大负责任AI原则（公平性、隐私性、可解释性、鲁棒性、安全性、真实性、治理、可持续性）角度审查现代通用AI的风险和脆弱性；2. 与传统任务特定AI系统对比，分析风险差异；3. 提出输出自由度（DoFo）概念解释风险根源；4. 推导出C2V2（控制、一致性、价值、真实性）需求框架；5. 评估现有技术（AI对齐、检索增强生成、推理增强等）在C2V2维度上的表现。

Result: 研究发现通用AI系统由于输出自由度（DoFo）非确定性高，导致其在负责任AI原则下的风险比传统任务特定AI更严重且难以缓解。C2V2框架为评估和改进通用AI的负责任性提供了系统化标准。现有技术如AI对齐、检索增强生成等只能部分满足C2V2需求，需要更全面的系统设计方法。

Conclusion: 开发负责任的通用AI需要：1. 正式建模应用或领域相关的负责任AI需求，沿C2V2维度；2. 采用系统设计方法，结合多种技术来满足这些需求。通过这种方法，可以实现负责任的通用AI系统开发目标。

Abstract: Modern general-purpose AI systems made using large language and vision models, are capable of performing a range of tasks like writing text articles, generating and debugging codes, querying databases, and translating from one language to another, which has made them quite popular across industries. However, there are risks like hallucinations, toxicity, and stereotypes in their output that make them untrustworthy. We review various risks and vulnerabilities of modern general-purpose AI along eight widely accepted responsible AI (RAI) principles (fairness, privacy, explainability, robustness, safety, truthfulness, governance, and sustainability) and compare how they are non-existent or less severe and easily mitigable in traditional task-specific counterparts. We argue that this is due to the non-deterministically high Degree of Freedom in output (DoFo) of general-purpose AI (unlike the deterministically constant or low DoFo of traditional task-specific AI systems), and there is a need to rethink our approach to RAI for general-purpose AI. Following this, we derive C2V2 (Control, Consistency, Value, Veracity) desiderata to meet the RAI requirements for future general-purpose AI systems, and discuss how recent efforts in AI alignment, retrieval-augmented generation, reasoning enhancements, etc. fare along one or more of the desiderata. We believe that the goal of developing responsible general-purpose AI can be achieved by formally modeling application- or domain-dependent RAI requirements along C2V2 dimensions, and taking a system design approach to suitably combine various techniques to meet the desiderata.

</details>


### [196] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 论文扩展了TIVS框架，增加了语义相似性缓存和可观测性评分比(OSR)，提出了TIVS-O评估框架，在多智能体嵌套学习架构中研究防御效果与透明度的权衡。


<details>
  <summary>Details</summary>
Motivation: 提示注入仍然是大型语言模型安全部署的主要障碍，特别是在多智能体环境中，中间输出可能传播或放大恶意指令。需要评估框架来量化安全性和可观测性之间的权衡。

Method: 提出结合智能体管道与连续记忆系统的架构，实现语义相似性缓存。使用301个合成生成的注入提示（来自10个攻击家族），第四个智能体使用五个关键性能指标进行安全分析。引入OSR指标量化每个智能体暴露的安全相关推理的丰富性和清晰度。

Result: 系统实现零高风险漏洞的安全响应，语义缓存显著减少计算开销：LLM调用减少41.6%，延迟、能耗和碳排放相应降低。五种TIVS-O配置揭示了缓解严格性和取证透明度之间的最优权衡。

Conclusion: 可观测性感知评估能揭示多智能体管道中的非单调效应，记忆增强智能体可同时最大化安全鲁棒性、实时性能、运营成本节约和环境可持续性，无需修改底层模型权重，为安全和绿色LLM部署提供生产就绪路径。

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [197] [CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning](https://arxiv.org/abs/2601.13262)
*Eric Onyame,Akash Ghosh,Subhadip Baidya,Sriparna Saha,Xiuying Chen,Chirag Agarwal*

Main category: cs.AI

TL;DR: CURE-MED框架通过课程强化学习提升LLMs的多语言医疗推理能力，在13种语言上显著改善逻辑正确性和语言一致性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单语数学和常识推理上表现良好，但在多语言医疗推理应用中仍不可靠，阻碍了其在多语言医疗环境中的部署

Method: 提出CURE-MED框架，包含：1) 引入CUREMED-BENCH多语言医疗推理数据集；2) 采用课程强化学习，结合代码切换感知的监督微调和组相对策略优化，共同提升逻辑正确性和语言稳定性

Result: 在13种语言上，该方法持续优于强基线且扩展有效：7B参数模型达到85.21%语言一致性和54.35%逻辑正确性；32B参数模型达到94.96%语言一致性和70.04%逻辑正确性

Conclusion: 研究结果支持LLMs实现可靠且公平的多语言医疗推理，代码和数据集已开源

Abstract: While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/

</details>


### [198] [Improving the Safety and Trustworthiness of Medical AI via Multi-Agent Evaluation Loops](https://arxiv.org/abs/2601.13268)
*Zainab Ghafoor,Md Shafiqul Islam,Koushik Howlader,Md Rasel Khondokar,Tanusree Bhattacharjee,Sayantan Chakraborty,Adrito Roy,Ushashi Bhattacharjee,Tirtho Roy*

Main category: cs.AI

TL;DR: 提出多智能体精炼框架，通过结构化迭代对齐增强医疗大语言模型的安全性和可靠性，结合生成模型与评估智能体，显著减少伦理违规和风险。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗领域应用日益广泛，但确保其伦理完整性和安全合规性仍是临床部署的主要障碍，需要开发有效的安全治理方法。

Method: 采用多智能体精炼框架，结合DeepSeek R1和Med-PaLM两个生成模型，以及LLaMA 3.1和Phi-4两个评估智能体，使用美国医学会医学伦理原则和五级安全风险评估协议进行结构化迭代对齐。

Result: 在900个临床多样化查询的评估中，DeepSeek R1收敛更快（平均2.34 vs 2.67次迭代），Med-PaLM在隐私敏感场景处理更优；多智能体迭代循环实现了89%的伦理违规减少和92%的风险降级率。

Conclusion: 该研究提出了一个可扩展、符合监管要求且成本效益高的医疗AI安全治理范式，为医疗大语言模型的临床部署提供了有效的安全增强方法。

Abstract: Large Language Models (LLMs) are increasingly applied in healthcare, yet ensuring their ethical integrity and safety compliance remains a major barrier to clinical deployment. This work introduces a multi-agent refinement framework designed to enhance the safety and reliability of medical LLMs through structured, iterative alignment. Our system combines two generative models - DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4, which assess responses using the American Medical Association's (AMA) Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5) protocol. We evaluate performance across 900 clinically diverse queries spanning nine ethical domains, measuring convergence efficiency, ethical violation reduction, and domain-specific risk behavior. Results demonstrate that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations), while Med-PaLM shows superior handling of privacy-sensitive scenarios. The iterative multi-agent loop achieved an 89% reduction in ethical violations and a 92% risk downgrade rate, underscoring the effectiveness of our approach. This study presents a scalable, regulator-aligned, and cost-efficient paradigm for governing medical AI safety.

</details>


### [199] [PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding Diffusion](https://arxiv.org/abs/2601.13327)
*Po-Yu Liang,Tobo Duran,Jun Bai*

Main category: cs.AI

TL;DR: PepEDiff是一种新型肽结合剂生成器，通过连续潜在空间直接生成结合序列，无需结构预测，提高了序列多样性，在TIGIT靶点上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有肽结合剂生成方法过度依赖中间结构预测，增加了复杂性并限制了序列多样性。需要一种能够直接生成结合序列、提高结构多样性的新方法。

Method: 基于预训练蛋白质嵌入模型的连续潜在空间，通过潜在空间探索和扩散采样生成肽序列，不依赖预测结构，利用全局蛋白质嵌入流形作为语义先验。

Result: 在TIGIT（具有大而平坦蛋白质-蛋白质相互作用界面的挑战性靶点）的案例研究中，该方法超越了现有最先进方法，证明了其作为零样本肽结合剂设计的通用无结构框架的潜力。

Conclusion: PepEDiff提供了一种简单而有效的零样本肽结合剂设计方法，通过直接潜在空间生成避免了结构预测的复杂性，在保持性能的同时提高了序列多样性。

Abstract: We present PepEDiff, a novel peptide binder generator that designs binding sequences given a target receptor protein sequence and its pocket residues. Peptide binder generation is critical in therapeutic and biochemical applications, yet many existing methods rely heavily on intermediate structure prediction, adding complexity and limiting sequence diversity. Our approach departs from this paradigm by generating binder sequences directly in a continuous latent space derived from a pretrained protein embedding model, without relying on predicted structures, thereby improving structural and sequence diversity. To encourage the model to capture binding-relevant features rather than memorizing known sequences, we perform latent-space exploration and diffusion-based sampling, enabling the generation of peptides beyond the limited distribution of known binders. This zero-shot generative strategy leverages the global protein embedding manifold as a semantic prior, allowing the model to propose novel peptide sequences in previously unseen regions of the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a large, flat protein-protein interaction interface that lacks a druggable pocket. Despite its simplicity, our method outperforms state-of-the-art approaches across benchmark tests and in the TIGIT case study, demonstrating its potential as a general, structure-free framework for zero-shot peptide binder design. The code for this research is available at GitHub: https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model

</details>


### [200] [SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt Following in Text-to-Image Generation](https://arxiv.org/abs/2601.13462)
*Amine Rostane*

Main category: cs.AI

TL;DR: SpatialBench-UC：一个用于评估文本到图像模型空间关系理解能力的小型可复现基准测试，包含200个提示和100个反事实对，支持选择性预测和风险覆盖权衡分析。


<details>
  <summary>Details</summary>
Motivation: 评估文本到图像模型是否遵循明确的空间指令难以自动化。现有方法如目标检测器可能漏检目标或返回多个可能检测结果，简单的几何测试在边界情况下变得模糊。空间评估本质上是一个选择性预测问题，检查器应在证据不足时弃权并报告置信度，使结果可解释为风险覆盖权衡而非单一分数。

Method: 提出SpatialBench-UC基准测试，包含200个提示（50个物体对×4种关系），分组为100个通过交换物体角色获得的反事实对。发布包含版本化提示、固定配置、每样本检查器输出和报告表格的基准包，支持跨模型的可复现和可审计比较。包含轻量级人工审核用于校准检查器的弃权边界和置信度阈值。

Result: 评估了三个基线模型：Stable Diffusion 1.5、SD 1.5 BoxDiff和SD 1.4 GLIGEN。检查器报告通过率和覆盖率以及在已决定样本上的条件通过率。结果显示，接地方法显著提高了通过率和覆盖率，但弃权仍然是主要因素，主要原因是目标检测漏检。

Conclusion: SpatialBench-UC为文本到图像模型的空间关系评估提供了一个可复现的基准测试框架，支持选择性预测和风险覆盖权衡分析。接地方法能有效提升性能，但目标检测漏检仍是限制因素，需要进一步改进。

Abstract: Evaluating whether text-to-image models follow explicit spatial instructions is difficult to automate. Object detectors may miss targets or return multiple plausible detections, and simple geometric tests can become ambiguous in borderline cases. Spatial evaluation is naturally a selective prediction problem, the checker may abstain when evidence is weak and report confidence so that results can be interpreted as a risk coverage tradeoff rather than a single score. We introduce SpatialBench-UC, a small, reproducible benchmark for pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs times 4 relations) grouped into 100 counterfactual pairs obtained by swapping object roles. We release a benchmark package, versioned prompts, pinned configs, per-sample checker outputs, and report tables, enabling reproducible and auditable comparisons across models. We also include a lightweight human audit used to calibrate the checker's abstention margin and confidence threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff, and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as conditional pass rates on decided samples. The results show that grounding methods substantially improve both pass rate and coverage, while abstention remains a dominant factor due mainly to missing detections.

</details>


### [201] [Context and Transcripts Improve Detection of Deepfake Audios of Public Figures](https://arxiv.org/abs/2601.13464)
*Chongyang Gao,Marco Postiglione,Julian Baldwin,Natalia Denisenko,Isabel Gortner,Luke Fosdick,Chiara Pulice,Sarit Kraus,V. S. Subrahmanian*

Main category: cs.AI

TL;DR: 提出基于上下文和转录文本的音频深度伪造检测器CADD，通过整合上下文信息显著提升检测性能，并增强对抗攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前音频深度伪造检测器仅分析音频文件本身，忽略了人类判断真伪时依赖的上下文信息和转录文本，导致检测效果受限。

Method: 创建记者提供的深度伪造数据集JDD和合成音频数据集SYN；提出CADD架构，整合上下文信息和转录文本；在ITW和P²V等大规模数据集上评估性能。

Result: 上下文和转录文本显著提升检测性能：F1分数提升5%-37.58%，AUC提升3.77%-42.79%，EER提升6.17%-47.83%；CADD对5种对抗攻击策略具有更强鲁棒性，性能平均仅下降0.71%。

Conclusion: 整合上下文信息和转录文本能显著提高音频深度伪造检测器的性能和鲁棒性，为未来检测系统设计提供了新方向。

Abstract: Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).

</details>


### [202] [Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement](https://arxiv.org/abs/2601.13481)
*Jian Zhang,Zhangqi Wang,Zhiyuan Wang,Weiping Fu,Yu He,Haiping Zhu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: APOLO是一个用于精神健康领域情感诊断的自动提示优化框架，通过多智能体协作机制解决情感共病和临床线索探索效率问题，提升LLM在医疗环境中的诊断可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在临床笔记、咨询对话和在线心理健康社区中，抑郁、焦虑和创伤相关状态的情感表达普遍存在，准确识别这些情感对临床分诊、风险评估和及时干预至关重要。虽然大语言模型在情感分析任务中表现出强大的泛化能力，但在高风险、上下文密集的医疗环境中，其诊断可靠性对提示设计高度敏感。现有方法面临两个关键挑战：情感共病（多种交织情感状态使预测复杂化）和临床相关线索的探索效率低下。

Method: 提出APOLO（Automated Prompt Optimization for Linguistic Emotion Diagnosis）框架，将指令优化建模为部分可观测马尔可夫决策过程，采用多智能体协作机制，包括规划器、教师、批评者、学生和目标角色。在闭环框架中，规划器定义优化轨迹，教师-批评者-学生智能体迭代优化提示以增强推理稳定性和有效性，目标智能体根据性能评估决定是否继续优化。

Result: 实验结果表明，APOLO在领域特定和分层基准测试中持续提高了诊断准确性和鲁棒性，展示了在精神健康护理中可信赖LLM应用的可扩展和可泛化范式。

Conclusion: APOLO通过系统探索更广泛和更细粒度的提示空间，有效解决了情感共病和临床线索探索效率问题，为精神健康领域的高风险LLM应用提供了一个可靠、可扩展的自动提示优化框架。

Abstract: Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.

</details>


### [203] [AgenticRed: Optimizing Agentic Systems for Automated Red-teaming](https://arxiv.org/abs/2601.13518)
*Jiayi Yuan,Jonathan Nöther,Natasha Jaques,Goran Radanović*

Main category: cs.AI

TL;DR: AgenticRed：一种利用LLM上下文学习自动设计和优化红队系统的框架，无需人工干预，通过进化选择方法实现系统级设计而非策略优化


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队方法依赖人工指定工作流程，存在人为偏见且探索设计空间成本高昂，需要一种无需人工干预的自动化系统设计方法

Method: 将红队视为系统设计问题而非策略优化，借鉴Meta Agent Search思想，开发基于进化选择的智能体系统演化程序，利用LLM的上下文学习迭代设计和优化红队系统

Result: 在Llama-2-7B上达到96%攻击成功率（提升36%），Llama-3-8B上98%；对专有模型具有强迁移性：GPT-3.5-Turbo和GPT-4o-mini上100%，Claude-Sonnet-3.5上60%（提升24%）

Conclusion: 自动化系统设计是AI安全评估的强大范式，能够跟上模型快速演进的步伐，AgenticRed展示了无需人工干预的自动化红队系统设计的有效性

Abstract: While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.

</details>


### [204] [Reasoning While Recommending: Entropy-Guided Latent Reasoning in Generative Re-ranking Models](https://arxiv.org/abs/2601.13533)
*Changshuo Zhang*

Main category: cs.AI

TL;DR: EGLR推荐模型通过熵引导的潜在推理机制，在生成式重排序中实现"边推理边推荐"，动态适应列表生成过程中的熵变化，提升复杂偏好捕捉能力


<details>
  <summary>Details</summary>
Motivation: 现有生成式重排序方法难以适应列表生成过程中模型难度的动态熵变化，无法准确捕捉复杂偏好。语言模型通过集成推理能力取得突破，启发引入潜在推理机制来降低决策熵

Method: 提出熵引导潜在推理(EGLR)推荐模型：1) 放弃"先推理后推荐"范式，实现"边推理边推荐"；2) 使用上下文感知推理令牌和动态温度调整实现熵引导的变长推理；3) 采用轻量级集成设计，无需复杂独立模块或后处理

Result: 在两个真实世界数据集上的实验验证了模型有效性，显著优势在于能与现有生成式重排序模型兼容并提升其性能。进一步分析展示了实际部署价值和研究潜力

Conclusion: EGLR模型通过熵引导的潜在推理机制，在生成式重排序中实现了更精确的探索-利用权衡，解决了动态熵变化适应问题，具有实际部署兼容性和性能提升优势

Abstract: Reinforcement learning plays a crucial role in generative re-ranking scenarios due to its exploration-exploitation capabilities, but existing generative methods mostly fail to adapt to the dynamic entropy changes in model difficulty during list generation, making it challenging to accurately capture complex preferences. Given that language models have achieved remarkable breakthroughs by integrating reasoning capabilities, we draw on this approach to introduce a latent reasoning mechanism, and experimental validation demonstrates that this mechanism effectively reduces entropy in the model's decision-making process. Based on these findings, we introduce the Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three core advantages. First, it abandons the "reason first, recommend later" paradigm to achieve "reasoning while recommending", specifically designed for the high-difficulty nature of list generation by enabling real-time reasoning during generation. Second, it implements entropy-guided variable-length reasoning using context-aware reasoning token alongside dynamic temperature adjustment, expanding exploration breadth in reasoning and boosting exploitation precision in recommending to achieve a more precisely adapted exploration-exploitation trade-off. Third, the model adopts a lightweight integration design with no complex independent modules or post-processing, enabling easy adaptation to existing models. Experimental results on two real-world datasets validate the model's effectiveness, and its notable advantage lies in being compatible with existing generative re-ranking models to enhance their performance. Further analyses also demonstrate its practical deployment value and research potential.

</details>


### [205] [ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn Instruction Evolution](https://arxiv.org/abs/2601.13546)
*Hui Sun,Chang Xu,Haonan Xie,Hao Li,Yuhao Huang,Chuheng Zhang,Ming Jin,Xiaoguang Liu,Gang Wang,Jiang Bian*

Main category: cs.AI

TL;DR: 提出TSEvol多智能体时间序列演化算法、TSEData-20K数据集、ChatAD系列聊天机器人、TKTO优化方法和LLADBench基准测试，显著提升时间序列异常检测的推理能力和泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的异常检测方法存在推理能力不足、多轮对话能力欠缺和泛化能力有限的问题，需要提升时间序列异常行为的理解和解释能力

Method: 1) TSEvol多智能体时间序列演化算法；2) TSEData-20K数据集和ChatAD系列聊天机器人；3) TKTO优化方法增强跨任务泛化能力；4) LLADBench基准测试评估框架

Result: ChatAD模型在准确率提升34.50%、F1分数提升34.71%、假阳性降低37.42%；TKTO优化的ChatAD在分类、预测和填补任务上表现出竞争力的推理和跨任务泛化性能

Conclusion: 提出的多智能体框架、数据集、优化方法和评估基准显著提升了LLM驱动的时间序列异常检测的推理能力、对话能力和跨任务泛化能力

Abstract: LLM-driven Anomaly Detection (AD) helps enhance the understanding and explanatory abilities of anomalous behaviors in Time Series (TS). Existing methods face challenges of inadequate reasoning ability, deficient multi-turn dialogue capability, and narrow generalization. To this end, we 1) propose a multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2) introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B, and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization (TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the performance of ChatAD and nine baselines across seven datasets and tasks. Our three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71% in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our optimized ChatAD achieves competitive performance in reasoning and cross-task generalization on classification, forecasting, and imputation.

</details>


### [206] [AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent](https://arxiv.org/abs/2601.13559)
*Sun Hui,Ding Yanfeng,Huidong Ma,Chang Xu,Keyan Jin,Lizheng Zu,Cheng Zhong,xiaoguang Liu,Gang Wang,Wentong Cai*

Main category: cs.AI

TL;DR: AgentGC：首个基于智能体的进化式基因组数据压缩器，通过三层架构（用户层、认知层、压缩层）和多智能体（Leader和Worker）实现算法-数据集-系统联合优化，支持三种模式（CP、TP、BM），在压缩比和吞吐量上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的基因组数据无损压缩方法存在三个主要问题：1）非进化性，难以适应技术发展；2）低层次压缩建模，缺乏算法-数据集-系统联合优化；3）用户界面不友好。需要一种能够自适应进化、提供用户友好接口且性能优越的压缩解决方案。

Method: 提出AgentGC三层架构：1）用户层：通过Leader智能体结合LLM提供用户友好接口；2）认知层：由Leader驱动，集成LLM实现算法-数据集-系统联合优化，解决低层次建模和适应性有限问题；3）压缩层：由Worker智能体负责，通过自动化多知识学习压缩框架执行压缩/解压。支持三种模式：CP（压缩比优先）、TP（吞吐量优先）、BM（平衡模式）。

Result: 在9个数据集上与14个基线方法比较：三种模式（CP、TP、BM）的平均压缩比增益分别为16.66%、16.11%、16.33%；吞吐量增益分别为4.73倍、9.23倍、9.15倍。所有指标均显著优于现有方法。

Conclusion: AgentGC是首个基于智能体的进化式基因组数据压缩器，通过三层架构和多智能体协作，成功解决了现有方法的非进化性、低层次建模和用户界面不友好问题，在压缩比和吞吐量方面均取得显著提升，为基因组数据存储、共享和管理提供了高效解决方案。

Abstract: Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.

</details>


### [207] [Reasoning is a Modality](https://arxiv.org/abs/2601.13562)
*Zhiguang Liu,Yi Shang*

Main category: cs.AI

TL;DR: 论文提出了一种角色分离的Transformer架构，将全局控制器token与网格工作空间token分离，在ARC视觉推理任务上超越了人类平均表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统（如LLMs和ViTs）主要作为行为序列预测机运行，通过建模token统计来匹配可观察行为，缺乏持久、可读的内心状态。这与人类行为存在差距：人类可以通过解码内部状态来解释行动，而AI系统只能产生缺乏内在状态基础的流畅事后合理化。作者假设推理应作为一种独立模态存在，与规则应用的低级工作空间分离。

Method: 设计了一种新颖的角色分离Transformer块，将全局控制器token与网格工作空间token分离，实现迭代规则执行。该方法在VARC视觉中心协议下进行训练和评估，专门用于解决ARC任务作为视觉推理问题。

Result: 在ARC-1任务上达到了62.6%的准确率，超越了人类平均表现（60.2%），并显著优于先前方法。定性分析显示，与密集ViT基线相比，模型展现出更一致的规则应用结构，从概率斑块向控制器驱动的推理转变。

Conclusion: 通过将推理设计为独立于低级工作空间的模态，实现了更接近人类抽象推理能力的AI系统。角色分离的Transformer架构在视觉推理任务上表现出色，验证了推理作为独立通道的假设。

Abstract: The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.

</details>


### [208] [SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for LLM-based Social Engineering Scam Detection System](https://arxiv.org/abs/2601.13581)
*Heedou Kim,Changsik Kim,Sanghwa Shin,Jaewoo Kang*

Main category: cs.AI

TL;DR: ScriptMind是一个基于大语言模型的诈骗检测框架，通过犯罪脚本推理任务、数据集构建和认知模拟评估，显著提升了小型LLM的诈骗检测性能，并在认知层面增强用户对诈骗的警觉性。


<details>
  <summary>Details</summary>
Motivation: 当前社交工程诈骗日益采用个性化、多轮次的欺骗手段，传统检测方法面临局限。虽然大语言模型在识别欺骗方面显示出潜力，但其认知辅助潜力尚未充分探索。需要开发能够结合自动推理和人类认知的集成框架来应对这一挑战。

Method: 提出ScriptMind框架，包含三个核心组件：1) 犯罪脚本推理任务(CSIT)用于诈骗推理；2) 犯罪脚本感知推理数据集(CSID)用于微调小型LLM；3) 基于认知模拟的社交工程防御评估(CSED)用于评估实时认知影响。使用571个韩国电话诈骗案例构建了22,712个结构化诈骗者序列训练实例。

Result: 经过ScriptMind微调的11B小型LLM在检测准确率上比GPT-4o高出13%，在检测准确率、误报减少、诈骗者话语预测和推理质量方面均优于商业模型。在电话诈骗模拟实验中，显著提升并维持了用户的怀疑水平，改善了他们对诈骗的认知意识。

Conclusion: ScriptMind代表了向以人为中心、认知自适应的大语言模型用于诈骗防御的重要一步。该框架不仅提升了技术检测性能，还在认知层面增强了用户对诈骗的警觉性，为社交工程防御提供了综合解决方案。

Abstract: Social engineering scams increasingly employ personalized, multi-turn deception, exposing the limits of traditional detection methods. While Large Language Models (LLMs) show promise in identifying deception, their cognitive assistance potential remains underexplored. We propose ScriptMind, an integrated framework for LLM-based scam detection that bridges automated reasoning and human cognition. It comprises three components: the Crime Script Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based Evaluation of Social Engineering Defense (CSED) for assessing real-time cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured scammer-sequence training instances. Experimental results show that the 11B small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving superior performance over commercial models in detection accuracy, false-positive reduction, scammer utterance prediction, and rationale quality. Moreover, in phone scam simulation experiments, it significantly enhanced and sustained users' suspicion levels, improving their cognitive awareness of scams. ScriptMind represents a step toward human-centered, cognitively adaptive LLMs for scam defense.

</details>


### [209] [Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification](https://arxiv.org/abs/2601.13589)
*HyeYoung Lee*

Main category: cs.AI

TL;DR: 提出基于音频情感信号的多智能体AI系统，实时生成响应导向的媒体内容，通过安全验证确保内容适龄可控


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别研究主要关注分类准确性，但缺乏将推断的情感状态转化为安全、适龄、可控的响应内容的能力。需要一种能够实时生成响应内容并确保安全性的系统。

Method: 采用四智能体协作架构：1) 基于CNN的情感识别智能体提取声学特征；2) 响应策略决策智能体将情感映射到响应模式；3) 内容参数生成智能体产生媒体控制参数；4) 安全验证智能体强制执行适龄性和刺激约束。引入显式安全验证循环在输出前过滤生成内容。

Result: 在公开数据集上，系统达到73.2%的情感识别准确率、89.4%的响应模式一致性、100%的安全合规性，同时保持低于100ms的推理延迟，适合设备端部署。

Conclusion: 模块化架构提供了可解释性和可扩展性，适用于儿童相关媒体、治疗应用和情感响应智能设备。安全验证机制确保生成内容的安全性和适龄性。

Abstract: This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.

</details>


### [210] [Foundations of Global Consistency Checking with Noisy LLM Oracles](https://arxiv.org/abs/2601.13600)
*Paul He,Elke Kirschbaum,Shiva Kasiviswanathan*

Main category: cs.AI

TL;DR: 提出一种自适应分治算法，用于检测和定位自然语言事实集合中的全局不一致性，使用LLM作为评估器，具有多项式查询复杂度。


<details>
  <summary>Details</summary>
Motivation: 确保自然语言事实集合的全局一致性对于事实核查、摘要和知识库构建等任务至关重要。虽然LLM可以评估小规模事实子集的一致性，但其判断存在噪声，且成对检查不足以保证全局一致性。

Method: 提出自适应分治算法，识别事实的最小不一致子集（MUSes），可选地通过命中集计算最小修复。该方法具有低阶多项式查询复杂度。

Result: 在合成和真实LLM评估器上的实验表明，该方法能高效检测和定位不一致性，为基于LLM的语言一致性验证提供了可扩展框架。

Conclusion: 该研究为解决自然语言事实集合的全局一致性验证问题提供了实用算法，克服了LLM评估器的噪声和指数级查询复杂度问题，实现了可扩展的一致性验证。

Abstract: Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.

</details>


### [211] [Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue](https://arxiv.org/abs/2601.13687)
*Zhichao Liang,Satoshi Nakamura*

Main category: cs.AI

TL;DR: SocialMindChange：首个评估LLM在社交互动中主动改变他人心理状态能力的基准，包含1200个社交场景，6000个场景和9万+问题，显示当前LLM表现比人类低54.2%


<details>
  <summary>Details</summary>
Motivation: 现有动态心理理论基准主要让语言模型处于被动角色，仅追踪心理状态变化。但在真实社交互动中，心理理论也用于主动行动：说话者计划如何说话以改变他人的心理状态轨迹。需要从追踪心理状态转向改变心理状态的评估基准。

Method: 提出SocialMindChange基准，每个实例定义包含4个角色的社交情境和5个连接场景。模型扮演其中一个角色，在五个场景中生成对话以达到目标，同时保持与所有参与者不断变化的状态一致。采用结构化四步框架构建1200个社交情境，覆盖6000个场景和超过9万个问题，每个都经过真实性和质量验证。

Result: 对10个最先进的LLM进行评估，结果显示它们的平均性能比人类性能低54.2%。这表明当前LLM在长期连接互动中维持和改变心理状态表征方面仍存在困难。

Conclusion: SocialMindChange基准填补了从被动追踪心理状态到主动改变心理状态的评估空白。当前LLM在复杂社交互动中的心理理论能力仍有显著差距，需要进一步研究提升模型在动态社交情境中的主动心理状态操作能力。

Abstract: Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.

</details>


### [212] [Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games](https://arxiv.org/abs/2601.13709)
*Christopher Kao,Vanshika Vats,James Davis*

Main category: cs.AI

TL;DR: GPT-4o在社交推理游戏《黑手党》中比人类更擅长欺骗，通过异步多智能体框架模拟显示，检测器对LLM游戏的预测准确率低于人类游戏。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在自然语言社交环境中的欺骗能力，特别是在社交推理游戏《黑手党》中，因为这类游戏的成功依赖于通过对话欺骗他人，而之前的研究较少关注LLM在真实社交情境中的欺骗能力。

Method: 使用异步多智能体框架模拟35场《黑手党》游戏，采用GPT-4o作为智能体。创建基于GPT-4-Turbo的"黑手党检测器"，在不提供玩家角色信息的情况下分析游戏记录，预测黑手党玩家。将预测准确率作为欺骗质量的替代指标，并与28场人类游戏和随机基线进行比较。

Result: 黑手党检测器对LLM游戏的预测准确率低于对人类游戏的预测准确率。这一结果在不同游戏天数和检测到的黑手党数量上保持一致，表明LLM能更好地融入群体，从而更有效地进行欺骗。

Conclusion: LLM在社交环境中的欺骗能力比人类更有效，这突显了LLM欺骗的复杂性和风险。研究团队发布了LLM《黑手党》游戏记录数据集以支持未来研究。

Abstract: Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.

</details>


### [213] [Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering](https://arxiv.org/abs/2601.13752)
*Chak Tou Leong,Dingwei Chen,Heming Xia,Qingyu Yin,Sunbowen Lee,Jian Wang,Wenjie Li*

Main category: cs.AI

TL;DR: RELIEF框架通过调整大推理模型的自我概念来塑造其行为，无需推理轨迹监督，降低了训练成本


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在计算冗余和推理不忠实的问题，现有基于强化学习或黄金推理轨迹微调的方法计算成本高且难以扩展

Method: 提出RELIEF框架，通过简单的logit探测捕获模型的潜在推理信念，然后通过微调使模型的自我概念与目标信念蓝图对齐，使用合成的自反问答对进行训练

Result: 在效率和忠实性任务上，RELIEF匹配或优于基于行为监督和偏好的基线方法，同时训练成本更低；分析验证了调整推理信念能有效塑造实际行为

Conclusion: RELIEF提供了一种简单有效的框架，通过调整模型的自我概念来塑造其推理行为，无需昂贵的推理轨迹监督，具有更好的可扩展性

Abstract: Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.

</details>


### [214] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://arxiv.org/abs/2601.13761)
*Shengda Fan,Xuyan Ye,Yankai Lin*

Main category: cs.AI

TL;DR: DARC是一个两阶段自进化框架，通过解耦非对称推理课程解决自对弈中优化不稳定问题，在无人工标注情况下显著提升推理能力


<details>
  <summary>Details</summary>
Motivation: 现有自对弈框架存在优化不稳定问题：(1) 提问者依赖求解器反馈导致目标非平稳；(2) 求解器使用自生成伪标签存在自举误差。需要稳定自进化过程

Method: 两阶段框架：1) 训练提问者基于明确难度级别和外部语料合成难度校准问题；2) 训练求解器采用非对称自蒸馏机制，文档增强教师生成高质量伪标签监督无文档访问的学生求解器

Result: DARC具有模型无关性，在9个推理基准和3个骨干模型上平均提升10.9分，持续优于所有基线，接近完全监督模型性能且无需人工标注

Conclusion: DARC通过解耦提问者和求解器训练、引入难度校准和非对称自蒸馏，有效稳定自进化过程，为无监督推理能力提升提供有效框架

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations. The code is available at https://github.com/RUCBM/DARC.

</details>


### [215] [Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance](https://arxiv.org/abs/2601.13770)
*Mostapha Benhenda*

Main category: cs.AI

TL;DR: 提出了Look-Ahead-Bench基准，用于评估金融工作流中PiT LLMs的前瞻性偏差，通过实际场景测试而非简单的问答，并引入性能衰减分析来区分真实预测能力与记忆效应。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过在问答中测试内部前瞻知识来评估LLMs，缺乏对实际金融工作流中模型行为的评估。需要建立标准化基准来测量PiT LLMs中的前瞻性偏差，并区分模型的真实预测能力与基于记忆的性能。

Method: 创建Look-Ahead-Bench基准，在现实金融工作流场景中评估模型行为。通过分析不同时间市场机制下的性能衰减来区分真实预测与记忆效应，并引入多个量化基线建立性能阈值。评估了开源LLMs（Llama 3.1 8B/70B和DeepSeek 3.2）与PiT-Inference的PiT LLMs系列（Pitinf-Small, Pitinf-Medium, Pitinf-Large）。

Result: 标准LLMs显示出显著的前瞻性偏差（通过alpha衰减测量），而Pitinf模型随着规模增大表现出改进的泛化能力和推理能力。Pitinf模型在不同时间市场机制下性能衰减较小，表明其具有更好的真实预测能力。

Conclusion: 该工作为金融LLMs中时间偏差的标准化评估奠定了基础，提供了识别适合实际部署模型的实用框架。Pitinf模型在减少前瞻性偏差方面表现优异，为金融领域LLMs的实际应用提供了可靠选择。

Abstract: We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench

</details>


### [216] [Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments](https://arxiv.org/abs/2601.13846)
*Glinskaya Maria*

Main category: cs.AI

TL;DR: 本文提出Virtual Urbanism框架，通过AI生成城市合成复制品来量化城市身份，东京案例研究显示81%识别准确率，证明该框架可行性。


<details>
  <summary>Details</summary>
Motivation: 开发计算可处理的量化城市身份指标框架，通过AI增强城市分析，实现自动化多参数身份度量。

Method: 整合Stable Diffusion和LoRA模型生成东京9个区域的合成城市序列，排除现有导向标记以提取核心身份形成元素，通过人类评估实验验证感知合法性、量化区域身份、推导核心身份元素。

Result: 合成复制品平均识别准确率达81%，验证了复制品有效性；Urban Identity Level指标可评估不同区域身份水平；语义分析揭示文化嵌入类型学是核心身份形成元素。

Conclusion: Virtual Urbanism是可行的AI增强城市分析框架，为自动化多参数身份度量提供了路径，能够通过合成城市复制品量化城市身份。

Abstract: This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.

</details>


### [217] [Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems](https://arxiv.org/abs/2601.13887)
*Hong Su*

Main category: cs.AI

TL;DR: 提出人类模拟计算（HSC）框架，将智能建模为包含思考、行动、学习、反思和活动调度的连续闭环过程，强调主动参与和行动驱动的推理改进，以解决LLMs仅依赖文本数据在开放动态环境中适应性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本数据上表现出强大的知识表示和推理能力，但仅依赖语言材料限制了其在开放动态现实环境中的适应能力、推理结果验证和有效操作。需要一种更接近人类智能的框架来克服这些限制。

Method: 提出人类模拟计算（HSC）框架，将智能建模为包含思考、行动、学习、反思和活动调度的连续闭环内部推理过程。该框架强调在内部推理过程和与环境交互中的主动参与，利用行动不仅实现目标，还自动改进内部推理机制。整合人类常用思维策略，如主特征导向推理、通过行动扩展范围、环境反馈驱动的即时学习。

Result: 通过理论分析表明，人类模拟策略无法仅从语言材料中完全学习，人类式推理过程和行动基础的推理方法对于在现实环境中实现稳健适应和有效交互至关重要。

Conclusion: HSC框架为解决LLMs在开放动态环境中的局限性提供了理论基础，强调行动驱动的推理改进和人类思维策略的整合是实现真正适应性智能的关键。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.

</details>


### [218] [PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation](https://arxiv.org/abs/2601.13904)
*Jaeyoung Moon,Youjin Choi,Yucheon Park,David Melhart,Georgios N. Yannakakis,Kyung-Joong Kim*

Main category: cs.AI

TL;DR: PREFAB是一种低成本的回顾式自我标注方法，通过检测情感变化区域而非完整标注来减轻标注负担，同时保持标注质量。


<details>
  <summary>Details</summary>
Motivation: 现有情感状态标注方法需要用户在整个会话期间持续标注，虽然能获得细粒度数据，但过程耗时、认知负担重，容易导致疲劳和错误。需要一种更高效、负担更轻的标注方法。

Method: 基于峰值-结束规则和情感序数表示，PREFAB采用偏好学习模型检测相对情感变化，指导标注者仅标注选定片段，其余部分通过插值完成。还引入了预览机制提供上下文线索辅助标注。

Result: 技术性能研究和25名参与者的用户研究表明，PREFAB在建模情感变化方面优于基线方法，同时减轻了工作负担（有条件地减轻时间负担）。重要的是，PREFAB提高了标注者信心且未降低标注质量。

Conclusion: PREFAB提供了一种有效的低预算情感标注方法，通过聚焦情感变化区域而非完整标注，在保持标注质量的同时显著减轻了标注负担。

Abstract: Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.

</details>


### [219] [Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance](https://arxiv.org/abs/2601.14171)
*Qianli Ma,Chang Guo,Zhiheng Tian,Siyu Wang,Jipeng Xiao,Yuanhao Yue,Zhipeng Zhang*

Main category: cs.AI

TL;DR: RebuttalAgent：首个多智能体框架，将反驳信生成重构为以证据为中心的规划任务，通过分解审稿意见、构建混合上下文和外部搜索，确保每个论点都有明确证据支撑。


<details>
  <summary>Details</summary>
Motivation: 当前反驳信生成方法通常将其视为直接文本生成问题，存在幻觉、忽视审稿意见、缺乏可验证基础等问题。需要一种能够精确对齐审稿人意图和稿件细节的解决方案。

Method: 提出RebuttalAgent多智能体框架：1）将复杂审稿意见分解为原子化关注点；2）动态构建混合上下文，结合压缩摘要和高保真文本；3）集成自主按需外部搜索模块处理需要外部文献的关切；4）在起草前生成可检查的响应计划。

Result: 在提出的RebuttalBench上验证，该流水线在覆盖率、忠实度和策略连贯性方面优于强基线，为同行评审过程提供了透明可控的助手。

Conclusion: RebuttalAgent通过将反驳信生成重构为证据中心的规划任务，解决了现有方法的局限性，提供了透明、可控且基于证据的同行评审助手，代码将开源。

Abstract: Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.

</details>


### [220] [Toward Efficient Agents: Memory, Tool learning, and Planning](https://arxiv.org/abs/2601.14192)
*Xiaofang Yang,Lijun Li,Heng Zhou,Tong Zhu,Xiaoye Qu,Yuchen Fan,Qianshan Wei,Rui Ye,Li Kang,Yiran Qin,Zhiqiang Kou,Daizong Liu,Qi Li,Ning Ding,Siheng Chen,Jing Shao*

Main category: cs.AI

TL;DR: 该论文系统综述了大型语言模型智能体系统的效率问题，从记忆、工具学习和规划三个核心组件出发，分析了效率优化的方法、评估指标和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型向智能体系统扩展，现有研究主要关注有效性而忽视了效率问题。效率对于实际部署至关重要，涉及延迟、token消耗、步骤数等成本因素，但目前缺乏系统性研究。

Method: 从智能体系统的三个核心组件（记忆、工具学习、规划）出发，综述了多种效率优化方法：通过压缩和管理限制上下文、设计强化学习奖励以减少工具调用、采用受控搜索机制等。提出从两个互补角度衡量效率：固定成本预算下的有效性比较，以及相似有效性水平下的成本比较。

Result: 总结了效率导向的基准测试，包括各组件评估协议和常用效率指标。识别了效率优化的共同高级原则，如上下文边界控制、工具调用最小化、搜索优化等。建立了效果与成本的帕累托前沿分析框架。

Conclusion: 智能体系统效率研究需要系统性方法，涉及记忆管理、工具学习和规划优化。未来方向包括开发更全面的效率评估框架、探索效率与效果的平衡策略，以及在实际部署场景中验证优化方法的有效性。

Abstract: Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [221] [Utilizing Metadata for Better Retrieval-Augmented Generation](https://arxiv.org/abs/2601.11863)
*Raquib Bin Yousuf,Shengzhe Xu,Mandar Sharma,Andrew Neeser,Chris Latimer,Naren Ramakrishnan*

Main category: cs.IR

TL;DR: 该论文系统研究了在检索增强生成系统中整合元数据的策略，发现在结构化重复语料中，元数据前缀法和统一嵌入法能显著提升检索效果，通过增强文档内聚性、减少文档间混淆来改善检索质量。


<details>
  <summary>Details</summary>
Motivation: 在结构化重复语料（如监管文件）中，仅基于块相似性的检索方法难以区分具有重叠语言的文档。实践中常将元数据扁平化为文本输入，但这种做法的影响和权衡尚不清楚，需要系统研究元数据感知检索策略。

Method: 提出了系统性的元数据感知检索策略研究，比较了纯文本基线、元数据作为文本（前缀和后缀）、融合元数据和内容的统一嵌入双编码器、双编码器后期融合检索以及元数据感知查询重构等方法。

Result: 元数据前缀法和统一嵌入法在多种检索指标和问题类型上一致优于纯文本基线，统一嵌入法有时甚至超过前缀法且更易于维护。元数据集成通过增强文档内聚性、减少文档间混淆、扩大相关与不相关块之间的分离来提高检索效果。

Conclusion: 在结构化语料中，元数据集成对检索增强生成系统至关重要。前缀法和统一嵌入法是最有效的策略，元数据提供了强大的消歧信号。研究提供了公开的代码、评估框架和RAGMATE-10K数据集。

Abstract: Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted.

</details>


### [222] [Cultural Analytics for Good: Building Inclusive Evaluation Frameworks for Historical IR](https://arxiv.org/abs/2601.11874)
*Suchana Datta,Dwaipayan Roy,Derek Greene,Gerardine Meaney,Karen Wade,Philipp Mayr*

Main category: cs.IR

TL;DR: 该研究构建了一个结合信息检索与文化分析的跨学科框架，使用英国图书馆BL19数字馆藏（1700-1899年间的3.5万部作品）创建基准，研究19世纪小说与非小说中的语言变化和检索问题，通过专家查询设计、段落级相关性标注和LLM辅助，探索从小说到非小说的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是弥合信息检索与文化分析之间的鸿沟，支持历史知识的公平获取。研究者关注如何利用小说中的叙事理解和语义丰富性来改进学术和事实材料的检索，同时促进数字档案的可解释性、透明度和文化包容性。

Method: 研究方法包括：1) 使用英国图书馆BL19数字馆藏（1700-1899年间的3.5万部作品）构建基准；2) 结合专家驱动的查询设计；3) 段落级相关性标注；4) 大型语言模型辅助；5) 创建基于人类专业知识的可扩展评估框架；6) 重点研究从小说到非小说的知识迁移。

Result: 该研究提供了实用的评估资源和方法论范式，不仅提高了检索准确性，还促进了数字档案的可解释性、透明度和文化包容性。框架支持更丰富、更具历史意识的数字档案参与，最终朝着更解放的知识基础设施方向发展。

Conclusion: 该研究成功构建了一个跨学科框架，将信息检索与文化分析相结合，为开发支持更丰富、更具历史意识的数字档案参与检索系统提供了方法论范例，致力于创建更解放的知识基础设施。

Abstract: This work bridges the fields of information retrieval and cultural analytics to support equitable access to historical knowledge. Using the British Library BL19 digital collection (more than 35,000 works from 1700-1899), we construct a benchmark for studying changes in language, terminology and retrieval in the 19th-century fiction and non-fiction. Our approach combines expert-driven query design, paragraph-level relevance annotation, and Large Language Model (LLM) assistance to create a scalable evaluation framework grounded in human expertise. We focus on knowledge transfer from fiction to non-fiction, investigating how narrative understanding and semantic richness in fiction can improve retrieval for scholarly and factual materials. This interdisciplinary framework not only improves retrieval accuracy but also fosters interpretability, transparency, and cultural inclusivity in digital archives. Our work provides both practical evaluation resources and a methodological paradigm for developing retrieval systems that support richer, historically aware engagement with digital archives, ultimately working towards more emancipatory knowledge infrastructures.

</details>


### [223] [Agentic-R: Learning to Retrieve for Agentic Search](https://arxiv.org/abs/2601.11888)
*Wenhan Liu,Xinyu Ma,Yutao Zhu,Yuchen Li,Daiting Shi,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 提出了一种专门为智能体搜索设计的检索器训练框架，通过结合局部查询-段落相关性和全局答案正确性来衡量段落效用，并采用迭代训练策略双向优化搜索智能体和检索器。


<details>
  <summary>Details</summary>
Motivation: 当前智能体搜索主要依赖基于相似性的检索器，但相似段落并不总是对最终答案生成有用。现有检索器设计主要针对单轮检索增强生成(RAG)，缺乏对多轮智能体搜索的专门优化。

Method: 提出新的检索器训练框架：1) 使用局部查询-段落相关性和全局答案正确性双重标准衡量段落效用；2) 采用迭代训练策略，让搜索智能体和检索器双向迭代优化；3) 检索器使用智能体生成的高质量查询持续改进，而非固定问题的一次性训练。

Result: 在7个单跳和多跳问答基准测试中，提出的Agentic-R检索器在不同搜索智能体上均一致优于强基线方法。

Conclusion: 为智能体搜索设计的专门检索器训练框架能显著提升性能，通过结合局部相关性和全局答案正确性，以及迭代优化策略，有效解决了传统相似性检索器在智能体搜索中的局限性。

Abstract: Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.

</details>


### [224] [Information Farming: From Berry Picking to Berry Growing](https://arxiv.org/abs/2601.12544)
*Leif Azzopardi,Adam Roegiest*

Main category: cs.IR

TL;DR: 论文提出"信息耕作"概念框架，认为生成式AI正在推动信息交互从"信息觅食"向"信息耕作"的根本转变，类似于新石器时代从狩猎采集向农业的转变。


<details>
  <summary>Details</summary>
Motivation: 传统的Berry Picking和信息觅食理论将用户视为信息收集者，在分布式资源中机会主义搜索以满足不断变化的信息需求。然而，生成式AI的兴起正在从根本上改变人们生产、组织和重用信息的方式，这些传统范式已无法完全捕捉这一转变。

Method: 采用历史类比和实证证据，引入"信息耕作"作为概念框架，分析其作为信息交互自然演化的特征。通过视角性论文的形式，探讨信息耕作的好处、机会、设计影响、评估方法以及伴随的风险。

Result: 提出信息耕作框架，将生成式AI赋能的信息交互描述为：用户通过提示"播种"，随时间"培育"工作流程，在自己的"地块"中"收获"丰富结构化的相关信息产出，而非在他人的"补丁"间觅食。

Conclusion: 随着生成式AI技术的普及，信息耕作将逐渐取代短暂、基于补丁的觅食，成为主导的交互模式，标志着人机信息交互及其研究的更广泛转变。

Abstract: The classic paradigms of Berry Picking and Information Foraging Theory have framed users as gatherers, opportunistically searching across distributed sources to satisfy evolving information needs. However, the rise of GenAI is driving a fundamental transformation in how people produce, structure, and reuse information - one that these paradigms no longer fully capture. This transformation is analogous to the Neolithic Revolution, when societies shifted from hunting and gathering to cultivation. Generative technologies empower users to "farm" information by planting seeds in the form of prompts, cultivating workflows over time, and harvesting richly structured, relevant yields within their own plots, rather than foraging across others people's patches. In this perspectives paper, we introduce the notion of Information Farming as a conceptual framework and argue that it represents a natural evolution in how people engage with information. Drawing on historical analogy and empirical evidence, we examine the benefits and opportunities of information farming, its implications for design and evaluation, and the accompanying risks posed by this transition. We hypothesize that as GenAI technologies proliferate, cultivating information will increasingly supplant transient, patch-based foraging as a dominant mode of engagement, marking a broader shift in human-information interaction and its study.

</details>


### [225] [The Unfairness of Multifactorial Bias in Recommendation](https://arxiv.org/abs/2601.12828)
*Masoud Mansoury,Jin Huang,Mykola Pechenizkiy,Herke van Hoof,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该论文研究推荐系统中的多因子偏差（流行度偏差和积极性偏差的组合效应），提出通过百分位数评分转换的预处理方法来改善曝光公平性，实验证明该方法能有效提升公平性且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中存在流行度偏差和积极性偏差，这两种偏差都源于输入数据并通过推荐模型传播，导致不公平或次优结果。虽然每种偏差都已被独立研究，但它们的组合效应（多因子偏差）仍未得到充分探索。本文旨在研究多因子偏差如何影响项目侧公平性，特别是曝光偏差。

Method: 通过模拟研究发现积极性偏差不成比例地集中在流行项目上，进一步放大了它们的过度曝光。基于这一洞察，采用百分位数评分转换作为预处理策略来缓解多因子偏差。使用六种推荐算法在四个公共数据集上进行实验，并将该预处理步骤集成到后处理公平性流程中。

Result: 实验结果表明，该方法在可忽略的准确性损失下改善了曝光公平性。将预处理集成到后处理公平性流程中增强了其效果和效率，能够以更低的计算成本实现相当或更好的公平性。

Conclusion: 研究强调了解决多因子偏差的重要性，并展示了简单、数据驱动的预处理方法在改善推荐系统公平性方面的实际价值。百分位数评分转换是一种有效的预处理策略，能够缓解多因子偏差对曝光公平性的负面影响。

Abstract: Popularity bias and positivity bias are two prominent sources of bias in recommender systems. Both arise from input data, propagate through recommendation models, and lead to unfair or suboptimal outcomes. Popularity bias occurs when a small subset of items receives most interactions, while positivity bias stems from the over-representation of high rating values. Although each bias has been studied independently, their combined effect, to which we refer to as multifactorial bias, remains underexplored. In this work, we examine how multifactorial bias influences item-side fairness, focusing on exposure bias, which reflects the unequal visibility of items in recommendation outputs. Through simulation studies, we find that positivity bias is disproportionately concentrated on popular items, further amplifying their over-exposure. Motivated by this insight, we adapt a percentile-based rating transformation as a pre-processing strategy to mitigate multifactorial bias. Experiments using six recommendation algorithms across four public datasets show that this approach improves exposure fairness with negligible accuracy loss. We also demonstrate that integrating this pre-processing step into post-processing fairness pipelines enhances their effectiveness and efficiency, enabling comparable or better fairness with reduced computational cost. These findings highlight the importance of addressing multifactorial bias and demonstrate the practical value of simple, data-driven pre-processing methods for improving fairness in recommender systems.

</details>


### [226] [Rules, Resources, and Restrictions: A Taxonomy of Task-Based Information Request Intents](https://arxiv.org/abs/2601.12985)
*Melanie A. Kilian,David Elsweiler*

Main category: cs.IR

TL;DR: 提出基于任务的信息请求意图分类法，以弥补传统查询意图分类与AI驱动任务导向搜索之间的差距


<details>
  <summary>Details</summary>
Motivation: 现有意图分类主要基于系统日志数据，关注孤立信息需求而忽略任务上下文，无法满足LLM时代用户对综合性任务支持的需求

Method: 采用扎根理论方法，通过对机场信息咨询员的访谈研究，构建任务型信息请求意图分类法

Result: 开发了一个连接传统查询导向方法与新兴AI驱动任务导向搜索需求的意图分类体系

Conclusion: 需要更强的任务视角来理解查询意图，以支持LLM更好地解释复杂多面任务并提供全面任务支持

Abstract: Understanding and classifying query intents can improve retrieval effectiveness by helping align search results with the motivations behind user queries. However, existing intent taxonomies are typically derived from system log data and capture mostly isolated information needs, while the broader task context often remains unaddressed. This limitation becomes increasingly relevant as interactions with Large Language Models (LLMs) expand user expectations from simple query answering toward comprehensive task support, for example, with purchasing decisions or in travel planning. At the same time, current LLMs still struggle to fully interpret complex and multifaceted tasks. To address this gap, we argue for a stronger task-based perspective on query intent. Drawing on a grounded-theory-based interview study with airport information clerks, we present a taxonomy of task-based information request intents that bridges the gap between traditional query-focused approaches and the emerging demands of AI-driven task-oriented search.

</details>


### [227] [Incorporating Q&A Nuggets into Retrieval-Augmented Generation](https://arxiv.org/abs/2601.13222)
*Laura Dietz,Bryan Li,Gabrielle Liu,Jia-Huei Ju,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: RAGE系统将自动评估思想融入检索增强生成，提出了Crucible系统，通过构建问答片段库来保持显式引用溯源，在TREC NeuCLIR 2024上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在保持引用溯源和避免信息重复方面存在不足，需要更透明和可解释的语义表示来改进检索增强生成的质量和可信度。

Method: 提出Crucible系统，基于问答片段增强生成：1) 从检索文档构建问答片段库；2) 使用片段指导提取、选择和报告生成；3) 在片段上进行推理，避免信息重复；4) 在整个生成过程中保持显式引用溯源。

Result: 在TREC NeuCLIR 2024数据集上评估，Crucible系统在片段召回率、密度和引用基础方面显著优于最近的基于片段的RAG系统Ginger。

Conclusion: RAGE框架通过集成自动评估思想到RAG中，特别是Crucible系统的问答片段方法，能够有效保持引用溯源、提高信息覆盖和生成质量，为检索增强生成提供了更透明和可解释的解决方案。

Abstract: RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.

</details>


### [228] [Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?](https://arxiv.org/abs/2601.13227)
*Laura Dietz,Bryan Li,Eugene Yang,Dawn Lawrie,William Walden,James Mayfield*

Main category: cs.IR

TL;DR: 论文指出RAG系统中基于nugget的LLM评估方法存在循环风险，通过实验证明当评估元素泄露时，系统可以轻易获得接近完美的评分，强调需要盲评估和方法多样性来避免指标过拟合


<details>
  <summary>Details</summary>
Motivation: RAG系统越来越多地使用LLM评估器进行优化和评估，这种基于nugget的方法不仅用于评估框架，还集成到RAG系统架构中。虽然这种集成可能带来真正改进，但也存在循环性导致错误测量的风险。论文旨在调查这种风险

Method: 通过比较实验，研究基于nugget的RAG系统（如Ginger和Crucible）与强基线（如GPT-Researcher）的性能。故意修改Crucible以生成针对LLM评估器优化的输出，测试当评估元素（如提示模板或黄金nuggets）泄露或可预测时的表现

Result: 实验显示，当评估元素泄露时，可以轻松实现接近完美的评估分数。这表明基于nugget的评估方法容易受到循环性影响，系统可以通过优化特定评估指标而非真正改进性能来获得高分

Conclusion: 强调盲评估设置和方法多样性的重要性，以防止将指标过拟合误认为是真正的系统进步。需要更严谨的评估方法来确保RAG系统的真实改进

Abstract: RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.

</details>


### [229] [Guidelines for the Creation of an Annotated Corpus](https://arxiv.org/abs/2601.13353)
*Bahdja Boudoua,Nadia Guiffant,Mathieu Roche,Maguelonne Teisseire,Annelise Tran*

Main category: cs.IR

TL;DR: 提供创建文本标注指南和标注语料库的通用方法论框架


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供系统化的方法论支持，以创建高质量的标注指南和文本语料库，解决标注过程中的标准化、一致性和可重复性问题

Method: 基于UMR TETIS成员反馈和科学文献，提出包含方法论、存储、共享和增值利用的完整框架，通过定义和示例清晰说明每个步骤

Result: 开发了一个全面的方法论框架，涵盖从标注指南创建到语料库存储、共享和增值利用的全过程

Conclusion: 该框架为不同研究背景下创建和使用语料库提供了系统化的支持工具，有助于提高文本标注工作的质量和效率

Abstract: This document, based on feedback from UMR TETIS members and the scientific literature, provides a generic methodology for creating annotation guidelines and annotated textual datasets (corpora). It covers methodological aspects, as well as storage, sharing, and valorization of the data. It includes definitions and examples to clearly illustrate each step of the process, thus providing a comprehensive framework to support the creation and use of corpora in various research contexts.

</details>


### [230] [Integrating Vision-Centric Text Understanding for Conversational Recommender Systems](https://arxiv.org/abs/2601.13505)
*Wei Yuan,Shutong Qiao,Tong Chen,Quoc Viet Hung Nguyen,Zi Huang,Hongzhi Yin*

Main category: cs.IR

TL;DR: STARCRS提出了一种结合屏幕阅读和LLM文本理解的双通路对话推荐系统，通过视觉化编码和精细推理的互补模式提升偏好建模和响应生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统对话推荐系统通过扩展对话上下文（如引入实体信息或相关对话）来提升偏好推断，但这导致输入更长、更异构，引发输入长度限制、文本风格不一致和无关噪声等问题，需要更强的语言理解能力。

Method: 提出STARCRS系统，包含两个互补的文本理解模式：(1)屏幕阅读通路：将辅助文本信息编码为视觉标记，模拟屏幕浏览；(2)基于LLM的文本通路：聚焦关键内容进行细粒度推理。设计了知识锚定融合框架，结合对比对齐、交叉注意力交互和自适应门控来整合两种模式。

Result: 在两个广泛使用的基准测试上进行大量实验，结果表明STARCRS在推荐准确性和生成响应质量方面均取得一致提升。

Conclusion: STARCRS通过整合屏幕阅读和LLM文本理解的双通路模式，有效解决了对话推荐系统中上下文扩展带来的实际问题，显著提升了偏好建模和响应生成性能。

Abstract: Conversational Recommender Systems (CRSs) have attracted growing attention for their ability to deliver personalized recommendations through natural language interactions. To more accurately infer user preferences from multi-turn conversations, recent works increasingly expand conversational context (e.g., by incorporating diverse entity information or retrieving related dialogues). While such context enrichment can assist preference modeling, it also introduces longer and more heterogeneous inputs, leading to practical issues such as input length constraints, text style inconsistency, and irrelevant textual noise, thereby raising the demand for stronger language understanding ability. In this paper, we propose STARCRS, a Screen-Text-AwaRe Conversational Recommender System that integrates two complementary text understanding modes: (1) a screen-reading pathway that encodes auxiliary textual information as visual tokens, mimicking skim reading on a screen, and (2) an LLM-based textual pathway that focuses on a limited set of critical content for fine-grained reasoning. We design a knowledge-anchored fusion framework that combines contrastive alignment, cross-attention interaction, and adaptive gating to integrate the two modes for improved preference modeling and response generation. Extensive experiments on two widely used benchmarks demonstrate that STARCRS consistently improves both recommendation accuracy and generated response quality.

</details>


### [231] [More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval](https://arxiv.org/abs/2601.13525)
*Chunsheng Zuo,Daniel Khashabi*

Main category: cs.IR

TL;DR: 通过PCA对查询嵌入进行降维压缩，可有效提升专业领域检索性能，无需昂贵标注和重新训练


<details>
  <summary>Details</summary>
Motivation: 基于预训练嵌入的密集检索器在专业领域表现不佳，因为训练和目标领域分布不匹配。传统领域适应方法需要昂贵的标注和重新训练查询-文档对。

Method: 重新审视被忽视的替代方案：对领域嵌入应用PCA，获得低维表示，保留领域相关特征同时丢弃非判别性成分。虽然传统上用于效率提升，但本文证明这种简单的嵌入压缩能有效改进检索性能。

Result: 在9个检索器和14个MTEB数据集上评估，仅对查询嵌入应用PCA，在75.4%的模型-数据集对上提高了NDCG@10，为领域适应提供了简单轻量的方法。

Conclusion: 嵌入压缩（特别是PCA）是一种简单有效的领域适应方法，无需昂贵标注或重新训练，显著提升专业领域检索性能。

Abstract: Dense retrievers powered by pretrained embeddings are widely used for document retrieval but struggle in specialized domains due to the mismatches between the training and target domain distributions. Domain adaptation typically requires costly annotation and retraining of query-document pairs. In this work, we revisit an overlooked alternative: applying PCA to domain embeddings to derive lower-dimensional representations that preserve domain-relevant features while discarding non-discriminative components. Though traditionally used for efficiency, we demonstrate that this simple embedding compression can effectively improve retrieval performance. Evaluated across 9 retrievers and 14 MTEB datasets, PCA applied solely to query embeddings improves NDCG@10 in 75.4% of model-dataset pairs, offering a simple and lightweight method for domain adaptation.

</details>


### [232] [Balancing Fairness and High Match Rates in Reciprocal Recommender Systems: A Nash Social Welfare Approach](https://arxiv.org/abs/2601.13609)
*Yoji Tomita,Tomohiko Yokoyama*

Main category: cs.IR

TL;DR: 该论文研究匹配平台中互惠推荐系统的公平性问题，提出基于社会福利和纳什社会福利的方法来平衡匹配率与公平性，并开发了高效的近似算法。


<details>
  <summary>Details</summary>
Motivation: 随着在线约会服务和职位推荐等匹配平台的普及，设计既提高匹配总数又避免用户间不公平的互惠推荐系统变得至关重要。现有方法在追求高匹配率时往往导致推荐机会分配不公平，需要研究如何平衡公平性与匹配效率。

Method: 1. 从公平分配角度定义用户的推荐机会，建立基于嫉妒自由（envy-freeness）的公平概念；2. 提出社会福利（SW）方法近似最大化匹配数；3. 提出纳什社会福利（NSW）方法通过交替优化两个NSW函数实现近似嫉妒自由的推荐；4. 将SW和NSW方法推广为α-SW方法，平衡公平性与高匹配率之间的权衡；5. 基于Sinkhorn算法开发SW/NSW/α-SW方法的计算高效近似算法。

Result: 实验表明：1. SW方法虽然能近似最大化匹配数，但会导致推荐机会显著不公平；2. NSW方法能够实现近似嫉妒自由的推荐；3. α-SW方法能够在公平性与匹配率之间进行灵活权衡；4. 提出的基于Sinkhorn的近似算法具有计算效率；5. 在合成数据集和两个真实数据集上的广泛实验验证了方法的实际有效性。

Conclusion: 该论文为匹配平台中的互惠推荐系统提供了系统的公平性分析框架，提出的NSW和α-SW方法能够有效平衡公平性与匹配效率，开发的近似算法具有实际应用价值，为解决推荐系统中的公平性挑战提供了重要工具。

Abstract: Matching platforms, such as online dating services and job recommendations, have become increasingly prevalent. For the success of these platforms, it is crucial to design reciprocal recommender systems (RRSs) that not only increase the total number of matches but also avoid creating unfairness among users. In this paper, we investigate the fairness of RRSs on matching platforms. From the perspective of fair division, we define the users' opportunities to be recommended and establish the fairness concept of envy-freeness in the allocation of these opportunities. We first introduce the Social Welfare (SW) method, which approximately maximizes the number of matches, and show that it leads to significant unfairness in recommendation opportunities, illustrating the trade-off between fairness and match rates. To address this challenge, we propose the Nash Social Welfare (NSW) method, which alternately optimizes two NSW functions and achieves nearly envy-free recommendations. We further generalize the SW and NSW method to the $α$-SW method, which balances the trade-off between fairness and high match rates. Additionally, we develop a computationally efficient approximation algorithm for the SW/NSW/$α$-SW methods based on the Sinkhorn algorithm. Through extensive experiments on both synthetic datasets and two real-world datasets, we demonstrate the practical effectiveness of our approach.

</details>


### [233] [QKVQA: Question-Focused Filtering for Knowledge-based VQA](https://arxiv.org/abs/2601.13856)
*Wei Ye,Yixin Su,Yueguo Chen,Longxiang Gao,Jianjun Li,Ruixuan Li,Rui Zhang*

Main category: cs.IR

TL;DR: 提出一种面向问题的知识过滤方法，通过可训练的问题聚焦过滤器和分块动态多文章选择模块，在保持计算成本与典型方法相当的同时，实现跨文章的高质量知识过滤，显著提升KB-VQA性能。


<details>
  <summary>Details</summary>
Motivation: KB-VQA需要有效整合图像与外部知识，知识过滤是关键。现有方法存在文章级和文章内信息选择错误，而MLLM方法虽能跨文章过滤但计算成本过高，限制了实际应用。

Method: 提出问题聚焦过滤方法，包含可训练的问题聚焦过滤器（QFF）和分块动态多文章选择（CDA）模块。QFF实现面向问题的知识过滤，CDA通过分块策略动态选择多篇文章，共同解决文章级和文章内的信息选择错误。

Result: 在E-VQA数据集上比当前最优模型提升4.9%，在InfoSeek数据集上提升3.8%，验证了方法的有效性。计算成本与典型方法相当，实现了高效的知识过滤。

Conclusion: 提出的问题聚焦过滤方法能够高效进行跨文章知识过滤，在保持合理计算成本的同时显著提升KB-VQA性能，为解决知识过滤中的信息选择错误提供了有效方案。

Abstract: Knowledge-based Visual Question Answering (KB-VQA) aims to answer questions by integrating images with external knowledge. Effective knowledge filtering is crucial for improving accuracy. Typical filtering methods use similarity metrics to locate relevant article sections from one article, leading to information selection errors at the article and intra-article levels. Although recent explorations of Multimodal Large Language Model (MLLM)-based filtering methods demonstrate superior semantic understanding and cross-article filtering capabilities, their high computational cost limits practical application. To address these issues, this paper proposes a question-focused filtering method. This approach can perform question-focused, cross-article filtering, efficiently obtaining high-quality filtered knowledge while keeping computational costs comparable to typical methods. Specifically, we design a trainable Question-Focused Filter (QFF) and a Chunk-based Dynamic Multi-Article Selection (CDA) module, which collectively alleviate information selection errors at both the article and intra-article levels. Experiments show that our method outperforms current state-of-the-art models by 4.9% on E-VQA and 3.8% on InfoSeek, validating its effectiveness. The code is publicly available at: https://github.com/leaffeall/QKVQA.

</details>


### [234] [IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization](https://arxiv.org/abs/2601.13938)
*Heyang Zhou,JiaJia Chen,Xiaolu Chen,Jie Bao,Zhen Chen,Yong Liao*

Main category: cs.IR

TL;DR: IF-GEO是一个针对生成式引擎优化的"发散-收敛"框架，通过挖掘不同查询的优化偏好并协调冲突，生成全局修订蓝图，以在内容预算有限的情况下提升多查询场景下的源文档可见性。


<details>
  <summary>Details</summary>
Motivation: 生成式引擎通过从检索源合成直接答案来革新信息检索，但这也带来了源文档可见性的挑战。生成式引擎优化(GEO)通过针对性内容修订来改善可见性，但为多样化查询优化文档面临约束优化问题：异质查询常产生冲突的修订需求，且内容预算有限。

Method: 提出IF-GEO框架，采用"发散-收敛"两阶段方法：1) 从代表性潜在查询中挖掘不同的优化偏好；2) 通过冲突感知的指令融合协调这些偏好，合成全局修订蓝图以指导编辑。同时引入风险感知的稳定性指标来量化跨查询稳定性目标。

Result: 在多查询基准测试上的实验表明，IF-GEO实现了显著的性能提升，同时在多样化检索场景中保持了鲁棒性。

Conclusion: IF-GEO有效解决了生成式引擎优化中的多查询冲突问题，通过协调不同查询的优化偏好生成全局修订蓝图，在有限内容预算下实现了跨查询的稳定性能提升。

Abstract: As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a "diverge-then-converge" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.

</details>


### [235] [Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval](https://arxiv.org/abs/2601.14001)
*Niall McGuire,Yashar Moshfeghi*

Main category: cs.IR

TL;DR: 首个系统研究听觉脑电图用于脑通道检索，发现听觉EEG优于视觉EEG，跨感官训练显著提升检索性能，甚至超越BM25文本基线


<details>
  <summary>Details</summary>
Motivation: 传统查询构建因认知复杂性和身体障碍而困难，现有脑通道检索研究仅使用视觉刺激，未探索听觉EEG在语音界面和视障用户中的潜力，也未研究跨感官训练能否解决数据稀缺问题

Method: 使用双编码器架构和四种池化策略（CLS、均值、最大值、多向量），在Alice（听觉）和Nieuwland（视觉）数据集上对比听觉单独训练、视觉单独训练和跨感官组合训练

Result: 听觉EEG持续优于视觉EEG；跨感官训练配合CLS池化显著提升性能：MRR提升31%（0.474）、Hit@1提升43%（0.314）、Hit@10提升28%（0.858）；组合听觉EEG模型超越BM25文本基线（MRR: 0.474 vs 0.428）

Conclusion: 验证了听觉神经界面在信息检索任务中的有效性，证明跨感官训练既能解决数据稀缺问题，又能超越单模态方法，使神经查询与传统检索竞争，同时支持无障碍界面

Abstract: Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR

</details>
