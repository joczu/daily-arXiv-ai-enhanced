<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 94]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.IR](#cs.IR) [Total: 23]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)
*Zihan Wang,Cheng Tang,Lei Gong,Cheng Li,Chao Wang,teng wang,Wenqi Lou,Xuehai Zhou*

Main category: cs.CL

TL;DR: Crystal-KV是一个专门为CoT推理设计的KV缓存管理框架，通过答案优先原则区分SlipKV和CrystalKV，采用基于注意力的LRFU算法和自适应缓存预算分配，在保持甚至提高CoT推理准确性的同时实现高效的KV缓存压缩。


<details>
  <summary>Details</summary>
Motivation: CoT推理显著提升了LLM在复杂任务上的准确性，但会导致过高的内存开销，因为长思考序列需要存储在KV缓存中。传统的KV压缩策略对CoT推理无效，因为CoT强调最终答案而非所有token的均匀重要性。

Method: 1. 答案优先原则：将答案偏好映射到思考阶段注意力图，区分SlipKV（主要维持推理流程但可能引入误导）和CrystalKV（真正贡献于最终答案正确性）。2. 基于注意力的LRFU算法：精确识别SlipKV条目效用何时过期并驱逐，保留CrystalKV而不破坏推理流程。3. 自适应缓存预算分配算法：基于CrystalKV的动态比例估计每层/头的重要性，在推理过程中调整KV缓存预算，放大关键组件以提高预算利用率。

Result: Crystal-KV实现了最先进的KV缓存压缩，显著提高了吞吐量，实现了更快的响应时间，同时保持甚至提高了CoT推理的答案准确性。

Conclusion: Crystal-KV通过针对CoT推理特点设计的KV缓存管理框架，有效解决了传统KV压缩策略在CoT场景下的不足，在保持准确性的同时显著提升了推理效率。

Abstract: Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.

</details>


### [2] [Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions](https://arxiv.org/abs/2601.16987)
*Shunyang Luo,Peibei Cao,Zhihui Zhu,Kehua Feng,Zhihua Wang,Keyan Ding*

Main category: cs.CL

TL;DR: PMDC框架通过动态选择奖励模型间最大分歧的提示-响应对，使用未标注的开放域提示池高效评估奖励模型泛化能力，相比传统静态基准能更真实反映模型在实际应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估主要依赖静态预标注偏好数据集，这些数据集覆盖有限且难以真实评估开放世界环境中的泛化能力，需要更动态、高效的评估框架来准确衡量奖励模型在实际应用中的表现。

Method: 提出Pairwise Maximum Discrepancy Competition (PMDC)框架：1) 使用大型未标注开放域提示池；2) 主动选择使两个奖励模型产生最大分歧的提示-响应对；3) 通过oracle裁决争议案例；4) 使用Bradley-Terry模型聚合结果生成全局排名和成对胜率分布。

Result: 对10个代表性奖励模型重新评估发现，与传统基准相比排名发生显著变化。定性分析揭示了系统性的泛化失败模式，为改进奖励建模提供了有价值的见解。

Conclusion: PMDC提供了一种动态、标注高效的评估框架，能更真实地评估奖励模型在开放世界环境中的泛化能力，揭示了传统静态评估方法的局限性，并为奖励模型的改进方向提供了重要指导。

Abstract: Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.

</details>


### [3] [Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction](https://arxiv.org/abs/2601.16999)
*Matthew Singer,Srijan Sengupta,Karl Pazdernik*

Main category: cs.CL

TL;DR: 提出一个基于序列标注的NER模型不确定性感知预测集框架，通过保形预测提供有限样本覆盖保证，生成包含正确标注的预测集合


<details>
  <summary>Details</summary>
Motivation: 当前NER模型通常只输出单一预测标签序列，缺乏不确定性度量，导致下游应用容易受到级联错误影响，需要提供类似置信区间的可靠性保证

Method: 基于保形预测框架，设计高效的非保形性评分函数，构建支持无条件覆盖和类别条件覆盖的校准预测集，考虑句子长度、语言、实体类型和实体数量等异质性因素

Result: 在三个基准数据集上的四个NER模型实验表明，所提方法具有广泛适用性、有效性和高效性，能够提供可靠的覆盖保证

Conclusion: 该框架为NER模型提供了不确定性感知的预测集，类似于传统统计中的置信区间，为下游应用提供了可靠性保证，解决了当前NER模型缺乏不确定性度量的问题

Abstract: Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.

</details>


### [4] [RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection](https://arxiv.org/abs/2601.17002)
*Ziyang Zhou,Ziqi Liu,Yan Wang,Yiming Lin,Yangbin Chen*

Main category: cs.CL

TL;DR: RAM-SD：一个用于讽刺检测的检索增强多智能体框架，通过四阶段流程实现SOTA性能，提供可解释的推理过程


<details>
  <summary>Details</summary>
Motivation: 现有讽刺检测方法采用统一的推理策略处理所有输入，难以应对讽刺表达中多样的分析需求，包括上下文期望违反建模、外部知识基础、特定修辞模式识别等

Method: 提出RAM-SD框架，包含四个阶段：(1)上下文检索：基于讽刺和非讽刺范例进行查询；(2)元规划器：分类讽刺类型并选择最优推理计划；(3)专业化智能体集成：执行互补的多视角分析；(4)集成器：综合分析生成最终可解释判断和自然语言解释

Result: 在四个标准基准测试中，RAM-SD实现了77.74%的Macro-F1分数，比强大的GPT-4o+CoC基线高出7.01个百分点，达到最先进性能

Conclusion: RAM-SD不仅设定了新的性能基准，还提供了透明可解释的推理轨迹，揭示了讽刺理解背后的认知过程，为复杂语言理解任务提供了新的多智能体框架范例

Abstract: Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.

</details>


### [5] [From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech](https://arxiv.org/abs/2601.17132)
*Vigneshwaran Shankaran,Gabriella Lapesa,Claudia Wagner*

Main category: cs.CL

TL;DR: 该论文通过整合心理学、政治学、传播学和语言学等多学科视角，系统性地研究了恐惧言论这一现象，提出了分类框架并回顾了现有数据集，为恐惧言论研究提供了理论和实践指导。


<details>
  <summary>Details</summary>
Motivation: 恐惧言论作为一种独特的言论形式，在社交媒体中广泛传播且影响力常超过仇恨言论，但由于其"看似文明"的特点往往逃避内容审核。目前计算语言学领域对恐惧言论的研究仍处于碎片化和资源不足的状态，需要跨学科整合来建立系统的研究框架。

Method: 1. 比较心理学、政治学、传播学和语言学中关于恐惧的理论；2. 回顾现有恐惧言论定义；3. 调查相关研究领域的数据集；4. 提出整合恐惧不同维度的分类学框架。

Result: 建立了跨学科的恐惧言论研究框架，提出了系统性的分类体系，为创建数据集和推进恐惧言论研究提供了理论基础和实践指导。通过整合多学科视角，澄清了恐惧言论的核心概念和研究方向。

Conclusion: 恐惧言论是一种独特且具有重要社会影响的言论形式，需要跨学科的系统研究。本文通过整合多学科理论、定义核心概念和提出分类框架，为恐惧言论的计算研究奠定了重要基础，有助于未来数据集构建和更有效的检测方法开发。

Abstract: Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears "civiler" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from Psychology, Political science, Communication science, and Linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.

</details>


### [6] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang,Junsik Kim,Siyuan Xiang,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 提出动态角色分配框架，通过元辩论选择最适合的智能体担任特定角色，提升多智能体大语言模型辩论系统的性能


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM/VLM辩论系统虽然使用专门角色解决复杂问题，但未能根据模型特性决定哪个模型适合哪个角色，导致角色分配不够优化

Method: 提出动态角色分配框架，包含元辩论过程：1)提案阶段-候选智能体提供角色定制化论证；2)同行评审阶段-使用数据和角色特定标准对提案评分，选择最适合每个角色的智能体

Result: 在LLM问题解决基准测试中，该方法始终优于均匀分配（所有角色使用相同模型）最高达74.8%，优于随机分配最高达29.7%，具体提升取决于任务和特定分配

Conclusion: 建立了多智能体系统设计新范式，从静态智能体部署转向动态、能力感知的选择，显著提升辩论系统性能

Abstract: Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [7] [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)
*Tunazzina Islam*

Main category: cs.CL

TL;DR: 该研究首次系统分析了LLM在生成人口统计条件定向信息时的行为，发现GPT-4o、Llama-3.3和Mistral-Large 2.1在气候沟通任务中表现出一致的年龄和性别偏见：针对男性和年轻人的信息强调能动性、创新和自信，而针对女性和老年人的信息则强调温暖、关怀和传统。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能够大规模生成个性化、有说服力的文本，自动通信中的偏见和公平性问题日益凸显。本研究旨在系统分析LLM在人口统计条件定向信息生成任务中的行为，揭示潜在的偏见模式。

Method: 引入受控评估框架，使用GPT-4o、Llama-3.3和Mistral-Large 2.1三个领先模型，在两种生成设置下评估：独立生成（隔离内在人口统计效应）和上下文丰富生成（纳入主题和区域背景以模拟现实定向）。从词汇内容、语言风格和说服框架三个维度评估生成信息，并在气候沟通任务中实例化该框架。

Result: 研究发现三个模型在年龄和性别维度上表现出一致的偏见模式：针对男性和年轻人的信息强调能动性、创新和自信，而针对女性和老年人的信息则强调温暖、关怀和传统。上下文提示会系统性放大这些差异，针对年轻或男性受众的定向信息的说服力得分显著更高。

Conclusion: 研究结果表明人口统计刻板印象会在LLM生成的定向通信中浮现并强化，强调了在社会敏感应用中需要开发偏见感知的生成流程和透明的审计框架，明确考虑人口统计条件的影响。

Abstract: Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.

</details>


### [8] [Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content](https://arxiv.org/abs/2601.17173)
*Parth Bhalerao,Diola Dsouza,Ruiwen Guan,Oana Ignat*

Main category: cs.CL

TL;DR: MentorQA：首个多语言长视频问答数据集与评估框架，专注于指导性回答而非事实准确性，包含近9000个QA对，比较了多种问答架构，发现多智能体管道在指导性回答方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统主要评估事实准确性，但教育、职业指导等实际应用需要指导性回答（提供反思和指导）。现有QA基准很少捕捉这种区别，特别是在多语言和长文本场景中。

Method: 引入MentorQA数据集，包含近9000个QA对，来自180小时的四语言视频内容。定义了超越事实准确性的指导性评估维度（清晰度、一致性、学习价值）。在受控条件下比较了单智能体、双智能体、RAG和多智能体QA架构。

Result: 多智能体管道始终产生更高质量的指导性回答，在复杂主题和低资源语言方面表现尤为突出。同时分析了基于LLM的自动评估的可靠性，发现与人类判断的一致性存在显著差异。

Conclusion: 这项工作确立了指导性问答作为一个独立的研究问题，并为教育AI中的智能体架构和评估设计提供了多语言基准。数据集和评估框架已开源。

Abstract: Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.

</details>


### [9] [Systematicity between Forms and Meanings across Languages Supports Efficient Communication](https://arxiv.org/abs/2601.17181)
*Doreen Osmelak,Yang Xu,Michael Hahn,Kate McCurdy*

Main category: cs.CL

TL;DR: 该研究提出了一种基于可学习性的复杂度度量方法，用于解释动词和代词形式中语法意义映射的系统性规律，将高效交际理论与自然语言的系统性联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有高效交际理论未能充分解释词形内部的系统性关系。不同语言中语法意义（如人称、数）到词形的映射存在规律性变化，需要新的理论框架来解释这些系统性的形式关系。

Method: 提出基于可学习性的复杂度度量方法，分析跨语言类型学上多样化的语言中动词和代词形式的语法意义映射。模型考虑简洁性（最小化语法区分）和准确性（恢复意图意义）之间的竞争压力。

Result: 新模型能够捕捉语言形式中的细粒度规律性，更好地区分实际存在的语言系统和理论上可能但未出现的系统，验证了动词和代词形式确实受到简洁性和准确性竞争压力的影响。

Conclusion: 基于可学习性的复杂度度量为理解语言形式的系统性提供了新视角，建立了高效交际理论与自然语言系统性之间的新联系，解释了语法意义映射中的规律性模式。

Abstract: Languages vary widely in how meanings map to word forms. These mappings have been found to support efficient communication; however, this theory does not account for systematic relations within word forms. We examine how a restricted set of grammatical meanings (e.g. person, number) are expressed on verbs and pronouns across typologically diverse languages. Consistent with prior work, we find that verb and pronoun forms are shaped by competing communicative pressures for simplicity (minimizing the inventory of grammatical distinctions) and accuracy (enabling recovery of intended meanings). Crucially, our proposed model uses a novel measure of complexity (inverse of simplicity) based on the learnability of meaning-to-form mappings. This innovation captures fine-grained regularities in linguistic form, allowing better discrimination between attested and unattested systems, and establishes a new connection from efficient communication theory to systematicity in natural language.

</details>


### [10] [Reasoning Beyond Literal: Cross-style Multimodal Reasoning for Figurative Language Understanding](https://arxiv.org/abs/2601.17197)
*Seyyed Saeid Cheshmi,Hahnemann Ortiz,James Mooney,Dongyeop Kang*

Main category: cs.CL

TL;DR: 提出三步骤框架开发高效多模态推理模型，解决视觉语言模型在理解多模态比喻语言（如讽刺、幽默、隐喻）方面的挑战，通过推理轨迹提升理解能力并实现跨风格泛化。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在字面多模态任务（如视觉数学和科学问答）中表现出强大的推理能力，但在理解比喻语言（如讽刺、幽默、隐喻）方面仍面临重大挑战。比喻语言通过表达意义与意图意义之间的微妙不一致来传达意图和情感，在多模态环境中，伴随的图像可以放大或反转文本含义，需要能够跨模态推理并考虑主观性的模型。

Method: 提出三步骤框架开发高效多模态推理模型：1) 解释多模态比喻语言；2) 提供透明的推理轨迹；3) 跨多种比喻风格泛化。在四种风格上进行实验，研究推理轨迹的融入效果、风格间的知识迁移以及跨风格联合训练。

Result: 实验结果表明：1) 融入推理轨迹显著提升多模态比喻理解能力；2) 在一种风格中学到的推理能力可以迁移到其他风格，特别是在相关风格（如讽刺和幽默）之间；3) 跨风格联合训练产生的通用推理视觉语言模型优于更大的开源和闭源模型。

Conclusion: 轻量级视觉语言模型通过可验证的推理能够实现鲁棒的跨风格泛化，同时为多模态任务提供可检查的推理轨迹。这表明高效的多模态推理模型可以在保持轻量化的同时，有效处理复杂的比喻语言理解任务。

Abstract: Vision-language models (VLMs) have demonstrated strong reasoning abilities in literal multimodal tasks such as visual mathematics and science question answering. However, figurative language, such as sarcasm, humor, and metaphor, remains a significant challenge, as it conveys intent and emotion through subtle incongruities between expressed and intended meanings. In multimodal settings, accompanying images can amplify or invert textual meaning, demanding models that reason across modalities and account for subjectivity. We propose a three-step framework for developing efficient multimodal reasoning models that can (i) interpret multimodal figurative language, (ii) provide transparent reasoning traces, and (iii) generalize across multiple figurative styles. Experiments across four styles show that (1) incorporating reasoning traces substantially improves multimodal figurative understanding, (2) reasoning learned in one style can transfer to others, especially between related styles like sarcasm and humor, and (3) training jointly across styles yields a generalized reasoning VLM that outperforms much larger open- and closed-source models. Our findings show that lightweight VLMs with verifiable reasoning achieve robust cross-style generalization while providing inspectable reasoning traces for multimodal tasks. The code and implementation are available at https://github.com/scheshmi/CrossStyle-MMR.

</details>


### [11] [Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis](https://arxiv.org/abs/2601.17203)
*Scott Friedman,Sonja Schmer-Galunder,Anthony Chen,Jeffrey Rye*

Main category: cs.CL

TL;DR: 该论文提出了一种量化词嵌入中性别偏见的方法，并利用这些偏见来表征教育、政治、经济和健康领域的统计性别差距，通过2018年推特数据验证了该方法在51个美国地区和99个国家的有效性。


<details>
  <summary>Details</summary>
Motivation: 现代NLP模型通常基于新闻、社交媒体等文化衍生文本进行训练，这些文本存在固有的种族和性别偏见。虽然这些偏见通常被视为需要修正的问题，但它们也可能反映了产生训练文本的文化中实际存在的性别差距，从而有助于通过大数据理解文化背景。

Method: 提出了一种量化词嵌入中性别偏见的方法，然后利用这些偏见来表征教育、政治、经济和健康领域的统计性别差距。使用2018年推特数据对51个美国地区和99个国家进行验证，将词嵌入偏见与国际和美国统计性别差距进行相关性分析。

Result: 验证了该方法在2018年推特数据上的有效性，覆盖51个美国地区和99个国家。将州和国家层面的词嵌入偏见与18个国际统计性别差距和5个美国统计性别差距进行相关性分析，表征了规律性和预测强度。

Conclusion: 词嵌入中的性别偏见不仅反映了模型偏差，也可能揭示了产生训练文本的文化中实际存在的统计性别差距。该方法为通过大数据分析理解文化背景中的性别差异提供了量化工具。

Abstract: Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.

</details>


### [12] [DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.17212)
*Saadat Hasan Khan,Spencer Hong,Jingyu Wu,Kevin Lybarger,Youbing Yin,Erin Babinsky,Daben Liu*

Main category: cs.CL

TL;DR: DF-RAG通过多样性增强检索来提升推理密集型QA性能，动态优化多样性水平，无需额外微调


<details>
  <summary>Details</summary>
Motivation: 传统RAG在推理密集型QA任务中面临挑战，因为基于余弦相似度的检索方法虽然最大化相关性，但会引入冗余内容，降低信息召回率

Method: 基于最大边际相关性框架，选择既与查询相关又彼此最大程度不同的信息块；关键创新是能够在测试时动态优化每个查询的多样性水平，无需额外微调或先验信息

Result: 在推理密集型QA基准测试中，DF-RAG比使用余弦相似度的普通RAG提高了4-10%的F1分数，并优于其他基线方法；估计Oracle上限比普通RAG高出18%绝对F1增益，其中DF-RAG捕获了高达91.3%

Conclusion: DF-RAG通过系统性地将多样性纳入检索步骤，显著提升了复杂推理密集型QA任务的性能，展示了多样性增强检索在RAG系统中的重要性

Abstract: Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.

</details>


### [13] [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](https://arxiv.org/abs/2601.17223)
*Massimiliano Pronesti,Anya Belz,Yufang Hou*

Main category: cs.CL

TL;DR: 提出可验证过程奖励模型（VPRMs），通过确定性规则验证器检查中间推理步骤，应用于医学证据合成中的偏倚风险评估，相比现有方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有过程监督方法依赖神经评分器评估思维链步骤，存在不透明性、偏见和奖励攻击漏洞；而可验证奖励强化学习（RLVR）仅使用结果级验证信号。需要结合两者优势，在中间推理步骤中引入确定性规则验证

Method: 提出可验证过程奖励模型（VPRMs）强化学习框架，使用确定性、基于规则的验证器检查中间推理步骤。应用于医学证据合成偏倚风险评估，利用指南定义的标准和基于规则的决策路径对推理轨迹进行程序化验证

Result: 在多个数据集上，VPRMs生成的推理紧密遵循领域规则，步骤级决策与最终标签的一致性显著提高。相比最先进模型提升高达20%的F1分数，比可验证结果奖励高6.5%，在证据基础和逻辑一致性方面有实质性提升

Conclusion: VPRMs通过确定性规则验证器实现过程监督，解决了神经评分器的不透明性和偏见问题，在医学证据合成偏倚风险评估中表现出优越性能，为需要可解释、规则遵循推理的领域提供了有效解决方案

Abstract: Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.

</details>


### [14] [Retell, Reward, Repeat: Reinforcement Learning for Narrative Theory-Informed Story Generation](https://arxiv.org/abs/2601.17226)
*David Y. Liu,Xanthe Muston,Aditya Joshi,Sebastian Sequoiah-Grayson*

Main category: cs.CL

TL;DR: 本文探索使用强化学习（d-RLAIF）作为监督微调的替代方案，用于自动故事生成任务，通过叙事平衡理论建立评估原则，使用LLM作为评判者提供奖励信号，最终生成更符合人类叙事惯例的多样化故事。


<details>
  <summary>Details</summary>
Motivation: 自动故事生成（ASG）具有主观性，但以往研究依赖有限的真实数据进行训练和评估。本文旨在探索强化学习作为监督微调（SFT）的替代方案，以解决ASG任务中主观性评估的挑战。

Method: 1. 应用Todorov的叙事平衡理论建立定义理想ASG质量的原则；2. 使用7B和14B参数的LLM作为评判者模型，测试这些原则与人类标注者的一致性，并在d-RLAIF过程中提供奖励信号；3. 使用Gemini-3-Flash评估后训练模型的输出，并与TimeTravel数据集中的人类撰写故事进行比较。

Result: d-RLAIF提供了监督微调（SFT）的可行替代方案，生成的故事更加多样化且更符合人类叙事惯例。研究表明强化学习在主观任务如ASG的语言基础后训练方面具有潜力。

Conclusion: 本文展示了强化学习在自动故事生成等主观任务中进行语言基础后训练的前景，d-RLAIF方法能够生成更符合人类叙事标准的多样化故事，为ASG研究提供了新的技术路径。

Abstract: Despite the subjective nature of storytelling, past works on automatic story generation (ASG) have relied on limited ground truths for training and evaluation. In this work, we explore reinforcement learning (d-RLAIF) as a post-training alternative to supervised fine-tuning (SFT). We first apply Todorov's Theory of Narrative Equilibrium to establish principles that define desirable ASG qualities. We prompt 7B and 14B LLM-as-judge models with our principles to test alignment with human annotators and provide reward signals during d-RLAIF. We use Gemini-3-Flash to evaluate the output of our post-trained models and compare them to human-written stories from the TimeTravel dataset. We show that d-RLAIF offers a viable alternative to supervised fine-tuning (SFT)--producing stories that are more diverse and aligned with human narrative conventions. Our paper demonstrates the promise of reinforcement learning for linguistically grounded post-training for subjective tasks such as ASG.

</details>


### [15] [Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data](https://arxiv.org/abs/2601.17232)
*Jacob Devasier,Akshith Putta,Qing Wang,Alankrit Moses,Chengkai Li*

Main category: cs.CL

TL;DR: 提出一个针对大规模结构化数据的事实核查新基准数据集，包含78,503个基于复杂OECD表格的合成声明，挑战现有模型在真实世界高容量数据上的检索与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动化事实核查基准主要关注小型、精选表格，忽略了验证声明对抗真实世界高容量结构化数据的挑战，需要填补这一关键空白。

Method: 采用框架引导方法，基于六种语义框架程序化选择显著数据点生成现实声明；创建多语言数据集（英、中、西、印地语）；通过知识探测实验确保LLMs未记忆这些事实；提供SQL生成基线系统。

Result: 数据集包含78,503个合成声明，基于434个复杂OECD表格（平均超50万行）；证明LLMs未记忆这些事实；基准极具挑战性，证据检索是主要瓶颈，模型难以在庞大表格中找到正确数据。

Conclusion: 该数据集为解决未解决的现实世界问题提供了关键新资源，推动大规模结构化数据事实核查研究，强调证据检索是当前主要挑战。

Abstract: Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.

</details>


### [16] [PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues](https://arxiv.org/abs/2601.17277)
*Mohammad Rifqi Farhansyah,Hanif Muhammad Zhafran,Farid Adilazuarda,Shamsuddeen Hassan Muhammad,Maryam Ibrahim Mukhtar,Nedjma Ousidhoum,Genta Indra Winata,Ayu Purwarianti,Alham Fikri Aji*

Main category: cs.CL

TL;DR: PingPong是一个用于自然多语言代码切换对话的基准测试，包含五种语言组合变体，涵盖真实的多方对话结构，并定义了三个下游任务来评估语言模型在多语言环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 代码切换是全球多语言人群的普遍实践，但现有基准测试未能准确反映日常交流中的复杂性。需要构建更自然的代码切换对话数据集来推动多语言NLP系统的发展。

Method: 创建PingPong基准测试，包含人类撰写的2-4人对话，覆盖五种语言组合变体（包括三语对话）。对话具有真实的多线程结构，回复经常引用对话早期内容。数据集在消息长度、说话者主导性和回复距离方面提供更大变化。

Result: PingPong数据比机器生成的数据更自然且结构更多样。基于这些对话定义了三个下游任务：问答、对话摘要和主题分类。评估显示当前最先进的语言模型在代码切换输入上的表现仍然有限。

Conclusion: 需要开发更强大的NLP系统来处理现实世界多语言交流的复杂性。PingPong基准测试为评估和改进多语言代码切换能力提供了重要资源。

Abstract: Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse.

</details>


### [17] [Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering](https://arxiv.org/abs/2601.17284)
*Yaokun Liu,Yifan Liu,Phoebe Mbuvi,Zelin Li,Ruichen Yao,Gawon Lim,Dong Wang*

Main category: cs.CL

TL;DR: 提出CV-MedBench基准，用于研究医学问答中的输入模糊性问题，并基于表示工程视角发现模糊性与LLM内部激活模式的线性编码关系，开发了无需微调或多次前向传播的AU-Probe模块，构建了"先澄清后回答"框架，显著提升医学问答安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学问答中的部署受到模糊用户查询的严重阻碍，这种安全风险显著降低了高风险医疗环境中的回答准确性。现有方法未能有效处理输入模糊性带来的不可约不确定性。

Method: 1) 构建CV-MedBench基准，专门研究医学QA中的输入模糊性；2) 从表示工程角度分析模糊性，发现其线性编码于LLM内部激活模式；3) 提出AU-Probe轻量模块，直接从隐藏状态检测输入模糊性；4) 设计AU引导的"先澄清后回答"框架，主动请求用户澄清。

Result: 在四个开源LLM上的广泛实验表明，该QA框架平均准确率比基线提高9.48%。AU-Probe无需LLM微调或多次前向传播，提供高效的模糊性检测机制，显著增强医学问答安全性。

Conclusion: 该研究为安全医学问答提供了高效稳健的解决方案，通过形式化输入模糊性与不可约不确定性的关系，并开发基于表示工程的检测框架，增强了健康相关应用的可靠性。代码和数据集已开源。

Abstract: The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.

</details>


### [18] [Meta-Judging with Large Language Models: Concepts, Methods, and Challenges](https://arxiv.org/abs/2601.17312)
*Hugo Silva,Mateus Mendes,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 该论文综述了LLM作为评估者（LLM-as-a-Judge）的局限性，并提出了LLM作为元评估者（LLM-as-a-Meta-Judge）的新范式，通过六个关键视角组织相关文献，旨在实现更稳定可靠的自动化评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为评估者存在显著脆弱性，包括对提示词的敏感性、系统性偏见、冗长效应以及不可靠或幻觉化的推理过程。这些局限性促使需要开发更稳健的评估范式。

Method: 采用文献综述方法，通过引入包含六个关键视角的框架来组织元评估研究：概念基础、元评估机制、对齐训练方法、评估、局限性与失败模式、未来方向。

Result: 分析表明LLM-as-a-Meta-Judge为更稳定和可信的自动化评估提供了有前景的方向，但仍需解决成本、提示敏感性和共享模型偏见等挑战。

Conclusion: LLM-as-a-Meta-Judge是推进下一代LLM评估方法学的有希望路径，但需要进一步解决现有挑战以实现更可靠的评估系统。

Abstract: Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.

</details>


### [19] [The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents](https://arxiv.org/abs/2601.17344)
*Chen Chen,Kim Young Il,Yuan Yang,Wenhao Su,Yilin Zhang,Xueluan Gong,Qian Wang,Yongsen Zheng,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 该研究提出了IMPRESS框架，用于评估LLM智能体在完全良性、现实场景中的内在价值错位风险，发现这是普遍存在的安全问题，现有缓解策略效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注对显性有害输入的响应或系统故障的鲁棒性，而在完全良性、现实的智能体设置中的价值错位风险尚未得到充分探索。LLM智能体可能追求偏离人类价值观和道德规范的目标，这种风险被称为价值错位。

Method: 首先形式化失控风险，识别内在价值错位。然后引入IMPRESS框架，通过多阶段LLM生成管道构建现实、完全良性和情境化的场景基准，对21个最先进的LLM智能体进行评估，并进行人工验证。

Result: 内在价值错位是跨模型的普遍安全风险，错位率因动机、风险类型、模型规模和架构而异。解码策略和超参数影响有限，但情境化和框架机制显著影响错位行为。现有缓解策略（如安全提示和护栏）表现出不稳定性或有限效果。

Conclusion: IMPRESS框架填补了现实、良性场景中LLM智能体价值错位评估的空白，揭示了这一普遍安全风险，并展示了在AI生态系统中的关键用例。现有安全措施需要改进以有效应对内在价值错位。

Abstract: Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.

</details>


### [20] [Do readers prefer AI-generated Italian short stories?](https://arxiv.org/abs/2601.17363)
*Michael Farrell*

Main category: cs.CL

TL;DR: 研究通过盲测比较AI生成与人类作家短篇小说，发现AI作品获得略高评分和偏好，挑战了人类文学创作优势的假设。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨读者是否更偏好AI生成的意大利语短篇小说而非知名意大利作家的作品，挑战关于人类创作文学优越性的普遍假设，并检验合成文本在文学语境中的接受度。

Method: 采用盲测实验设计，20名参与者在不知情情况下阅读并评估三篇短篇小说：两篇由ChatGPT-4o生成，一篇由意大利著名作家Alberto Moravia创作。同时收集参与者的阅读习惯和人口统计学数据（年龄、性别、教育程度、母语）。

Result: AI生成的文本获得了略高的平均评分，且更频繁被偏好，但差异不大。文本偏好与人口统计学变量或阅读习惯之间未发现统计学显著关联。

Conclusion: 研究结果挑战了读者更偏好人类创作小说的假设，并引发了对文学语境中合成文本编辑必要性的质疑。

Abstract: This study investigates whether readers prefer AI-generated short stories in Italian over one written by a renowned Italian author. In a blind setup, 20 participants read and evaluated three stories, two created with ChatGPT-4o and one by Alberto Moravia, without being informed of their origin. To explore potential influencing factors, reading habits and demographic data, comprising age, gender, education and first language, were also collected. The results showed that the AI-written texts received slightly higher average ratings and were more frequently preferred, although differences were modest. No statistically significant associations were found between text preference and demographic or reading-habit variables. These findings challenge assumptions about reader preference for human-authored fiction and raise questions about the necessity of synthetic-text editing in literary contexts.

</details>


### [21] [Parameter Efficient Fine Tuning Llama 3.1 for Answering Arabic Legal Questions: A Case Study on Jordanian Laws](https://arxiv.org/abs/2601.17364)
*Mohammed Fasha,Bassam Hammo,Bilal Sowan,Husam Barham,Esam Nsour*

Main category: cs.CL

TL;DR: 该研究以约旦法律为案例，探索了Llama-3.1大语言模型在阿拉伯语法律问答任务上的微调方法，通过量化和参数高效微调技术实现了资源高效的领域适应。


<details>
  <summary>Details</summary>
Motivation: 探索如何将大语言模型有效适应到阿拉伯语法律领域，解决特定领域任务中的资源效率和准确性挑战，为阿拉伯语法律AI应用提供技术方案。

Method: 使用Llama-3.1-8B-bnb-4bit和Llama-3.1-8B-Instruct-bnb-4bit两个版本模型，采用参数高效微调(PEFT)结合LoRA适配器和4位量化技术，利用Unsloth框架进行加速训练。构建了包含6000个法律问答对的定制数据集，并格式化为结构化提示。

Result: 微调后的模型在法律推理和准确性方面均有提升，同时通过量化和优化微调策略实现了资源效率。使用BLEU和ROUGE指标评估显示，微调模型相比基础版本有显著改进。

Conclusion: 该研究证明了将大语言模型适应到阿拉伯语法律领域的可行性，展示了量化技术和参数高效微调在领域特定任务中的有效性，为阿拉伯语法律AI应用提供了实用技术路径。

Abstract: This study uses Jordanian law as a case study to explore the fine-tuning of the Llama-3.1 large language model for Arabic question-answering. Two versions of the model - Llama-3.1-8B-bnb-4bit and Llama-3.1-8B-Instruct-bnb-4bit - were fine-tuned using parameter-efficient fine-tuning (PEFT) with LoRA adapters and 4-bit quantized models, leveraging the Unsloth framework for accelerated and resource-efficient training. A custom dataset of 6000 legal question-answer pairs was curated from Jordanian laws and formatted into structured prompts. Performance was evaluated using the BLEU and the ROUGE metrics to compare the fine-tuned models to their respective base versions. Results demonstrated improved legal reasoning and accuracy while achieving resource efficiency through quantization and optimized fine-tuning strategies. This work underscores the potential of adapting large language models for Arabic legal domains and highlights effective techniques for fine-tuning domain-specific tasks.

</details>


### [22] [Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers](https://arxiv.org/abs/2601.17367)
*Zecheng Tang,Quantong Qiu,Yi Yang,Zhiyi Hong,Haiya Xiang,Kebin Liu,Qingqing Dang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出Elastic Attention方法，通过轻量级Attention Router动态调整注意力稀疏度，解决混合注意力中静态计算比例无法适应下游任务变化的问题。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制的二次复杂度限制了LLM在长上下文场景的可扩展性。现有混合注意力策略采用静态稀疏-全注意力比例，无法适应下游任务在推理时的稀疏敏感性变化。

Method: 提出Elastic Attention，集成轻量级Attention Router到预训练模型中，动态分配每个注意力头到不同计算模式，使模型能根据输入调整整体稀疏度。

Result: 在8xA800 GPU上仅训练12小时，模型在三个长上下文基准测试中表现出优越性能，实现了强性能和高效推理的平衡。

Conclusion: Elastic Attention通过动态调整注意力稀疏度，有效解决了混合注意力中静态计算比例的局限性，为LLM长上下文处理提供了高效灵活的解决方案。

Abstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.

</details>


### [23] [WarrantScore: Modeling Warrants between Claims and Evidence for Substantiation Evaluation in Peer Reviews](https://arxiv.org/abs/2601.17377)
*Kiyotada Mori,Shohei Tanaka,Tosho Hirasawa,Tadashi Kozuno,Koichiro Yoshino,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 提出一种评估科学评审意见中主张与证据间逻辑推理的新方法，相比传统方法能更好地与人工评分相关，有助于提高同行评审效率


<details>
  <summary>Details</summary>
Motivation: 科学同行评审面临人力短缺问题，语言模型被探索用于降低评审成本。现有方法通过检测主张是否被证据支持来评估论证的扎实程度，但仅检测证据存在与否不足以评估主张与证据间的逻辑推理关系

Method: 提出新的科学评审意见评估指标，专注于评估主张与证据之间的逻辑推理关系。该方法提取论证的核心组件（主张和证据），并评估逻辑推理的质量，而不仅仅是检测证据的存在

Result: 实验结果显示，提出的方法比传统方法获得更高的人工评分相关性，表明其能更好地评估评审意见的论证质量

Conclusion: 该方法通过更准确地评估主张与证据间的逻辑推理关系，有潜力提高同行评审过程的效率，为解决评审人力短缺问题提供更好的支持

Abstract: The scientific peer-review process is facing a shortage of human resources due to the rapid growth in the number of submitted papers. The use of language models to reduce the human cost of peer review has been actively explored as a potential solution to this challenge. A method has been proposed to evaluate the level of substantiation in scientific reviews in a manner that is interpretable by humans. This method extracts the core components of an argument, claims and evidence, and assesses the level of substantiation based on the proportion of claims supported by evidence. The level of substantiation refers to the extent to which claims are based on objective facts. However, when assessing the level of substantiation, simply detecting the presence or absence of supporting evidence for a claim is insufficient; it is also necessary to accurately assess the logical inference between a claim and its evidence. We propose a new evaluation metric for scientific review comments that assesses the logical inference between claims and evidence. Experimental results show that the proposed method achieves a higher correlation with human scores than conventional methods, indicating its potential to better support the efficiency of the peer-review process.

</details>


### [24] [Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis](https://arxiv.org/abs/2601.17387)
*Toshiki Nakai,Varsha Suresh,Vera Demberg*

Main category: cs.CL

TL;DR: SeamlessM4T v2多语言语音-文本基础模型在语音和文本模态下对同一语言的内部表示不一致，存在不完全的模态不变性，解码器难以从压缩的编码器表示中恢复源语言信息。


<details>
  <summary>Details</summary>
Motivation: 研究多语言语音-文本基础模型是否在内部一致地表示同一语言的语音和文本形式，探究模态不变性的程度及其对模型性能的影响。

Method: 采用三种互补分析方法：1) 使用平均精度排名识别语言和模态选择性神经元；2) 通过推理时的中位数替换干预研究其功能作用；3) 分析跨语言和模态的激活幅度不平等性。

Result: 发现不完全的模态不变性：编码器表示变得语言无关，但共享解码器难以从模态无关表示中恢复源语言，特别是从语音适应到文本时。跨注意力键值投影中存在局部化的模态选择性结构。语音条件解码和非主导脚本表现出更高的激活集中度。

Conclusion: 多语言语音-文本模型在语音和文本模态下对同一语言的表示不一致，这种不完全的模态不变性可能导致跨模态和语言的脆弱性，特别是语音到文本转换和非主导脚本处理时。

Abstract: Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.

</details>


### [25] [Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations](https://arxiv.org/abs/2601.17569)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: P³框架：通过客户端模型评估和修改服务器生成的草稿令牌，实现个性化LLM输出，保护用户隐私，性能接近完全暴露用户配置的基准


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强的个性化方法面临隐私与性能的权衡：要么将用户私有数据暴露给云端LLM，要么依赖能力较弱的本地模型。需要一种既能保护隐私又能提供高质量个性化的解决方案。

Method: 提出P³交互框架：1) 服务器端大模型仅基于用户查询生成k个草稿令牌；2) 客户端小模型访问用户私有配置，评估并修改这些草稿以更好地反映用户偏好；3) 重复此过程直到生成结束令牌。

Result: 在LaMP-QA基准测试中，P³显著优于非个性化服务器端和个性化客户端基线，平均提升7.4%至9%。恢复率高达90.3%至95.7%（相比完全暴露用户配置的基准）。隐私分析显示仅增加1.5%-3.5%的泄漏风险。客户端模型仅生成总令牌的9.2%。

Conclusion: P³提供了一个实用有效的个性化生成解决方案，在保护用户隐私的同时实现了接近完全暴露配置的性能，适合边缘部署。

Abstract: Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.

</details>


### [26] [CLM-Bench: Benchmarking and Analyzing Cross-lingual Misalignment of LLMs in Knowledge Editing](https://arxiv.org/abs/2601.17397)
*Yucheng Hu,Wei Zhou,Juesi Xiao*

Main category: cs.CL

TL;DR: 本文提出了CLM-Bench，一个文化感知的中文优先多语言知识编辑基准，揭示了当前方法在跨语言知识传播上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MKE基准通常通过机械翻译英文数据集构建，存在翻译伪影且忽略目标语言的文化特定实体，无法反映LLMs的真实知识分布，阻碍了多语言知识编辑的发展。

Method: 提出CLM-Bench基准，采用中文优先方法构建，包含1,010个基于中文文化背景的高质量CounterFact对，并与英文对应项对齐。通过该基准对代表性LLMs进行实验，并通过分层表示分析提供几何解释。

Result: 实验发现显著的跨语言错位现象：一种语言的编辑独立运作且无法传播到另一种语言。几何分析显示中文和英文的编辑向量几乎正交，位于不相交的子空间中，而混合语言编辑则表现出这些向量的线性可加性。

Conclusion: 当前方法在跨语言迁移方面效果有限，强调文化原生基准的重要性。研究结果挑战了现有多语言知识编辑方法的有效性，为未来研究提供了重要方向。

Abstract: Knowledge Editing (KE) has emerged as a promising paradigm for updating facts in Large Language Models (LLMs) without retraining. However, progress in Multilingual Knowledge Editing (MKE) is currently hindered by biased evaluation frameworks. We observe that existing MKE benchmarks are typically constructed by mechanically translating English-centric datasets into target languages (e.g., English-to-Chinese). This approach introduces translation artifacts and neglects culturally specific entities native to the target language, failing to reflect the true knowledge distribution of LLMs. To address this, we propose CLM-Bench, a culture-aware benchmark constructed using a native Chinese-first methodology. We curate 1,010 high-quality CounterFact pairs rooted in Chinese cultural contexts and align them with English counterparts. Using CLM-Bench, we conduct extensive experiments on representative LLMs (e.g., Llama-3, Qwen2) and reveal a significant Cross-lingual Misalignment: edits in one language function independently and fail to propagate to the other. We further provide a geometric explanation via layer-wise representation analysis, demonstrating that edit vectors for Chinese and English are nearly orthogonal -- residing in disjoint subspaces -- while mixed-lingual editing exhibits linear additivity of these vectors. Our findings challenge the effectiveness of current methods in cross-lingual transfer and underscore the importance of culturally native benchmarks.

</details>


### [27] [Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning](https://arxiv.org/abs/2601.17421)
*Jaehui Hwang,Dongyoon Han,Sangdoo Yun,Byeongho Heo*

Main category: cs.CL

TL;DR: 分析大型语言模型中"wait"、"therefore"等话语标记词的概率信号，发现这些信号与推理正确性相关，受训练策略影响但跨模型规模稳定


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中出现了"wait"、"therefore"等类似话语的标记词，为理解其推理过程提供了独特窗口，但缺乏对这类信号如何随训练策略和模型规模变化的系统性分析

Method: 通过分析各种模型的标记词概率来研究标记级别的信号，特别关注"wait"标记词与答案概率的关系

Result: 发现特定标记词与推理正确性强烈相关，这些相关性随训练策略变化但在不同模型规模下保持稳定；在小规模数据集上微调的模型通过这类信号获得推理能力，但仅部分利用这些信号

Conclusion: 这项工作为观察和理解大型语言模型推理动态提供了系统性视角

Abstract: The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.

</details>


### [28] [Corpus-Based Approaches to Igbo Diacritic Restoration](https://arxiv.org/abs/2601.18380)
*Ignatius Ezeani*

Main category: cs.CL

TL;DR: 该论文针对低资源语言（特别是伊博语）开发了变音符号恢复框架，提出了三种主要方法：标准n-gram模型、分类模型和嵌入模型来解决变音符号歧义问题。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言处理研究主要关注英语、汉语等高资源语言，而全球95%以上的语言（超过7000种）属于低资源语言，缺乏NLP所需的数据、工具和技术。伊博语作为低资源语言，存在变音符号歧义问题，需要开发专门的解决方案。

Method: 提出了三种变音符号消歧方法：1) 标准n-gram模型：使用目标词之前的词序列作为正确变体预测的关键特征；2) 分类模型：使用目标词两侧的窗口词作为特征；3) 嵌入模型：比较上下文词嵌入组合与候选变体向量嵌入的相似度得分。

Result: 开发了一个灵活的框架用于生成变音符号恢复数据集，并针对伊博语实现了三种不同的变音符号消歧方法，为低资源语言的NLP处理提供了技术方案。

Conclusion: 该研究为低资源语言的变音符号恢复提供了系统性的解决方案框架，填补了NLP研究在低资源语言处理方面的空白，特别是针对伊博语等非洲语言的变音符号歧义问题。

Abstract: With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.
  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.

</details>


### [29] [Clustering-driven Memory Compression for On-device Large Language Models](https://arxiv.org/abs/2601.17443)
*Ondrej Bohdal,Pramit Saha,Umberto Michieli,Mete Ozay,Taha Ceritli*

Main category: cs.CL

TL;DR: 提出基于聚类的记忆压缩策略，在有限上下文预算下平衡压缩效率与个性化质量


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：直接拼接记忆会耗尽设备端LLM的有限上下文；平均压缩会因语义冲突损害性能

Method: 基于聚类的记忆压缩策略：按相似性对记忆分组，在聚类内合并记忆后再拼接，减少冗余同时保持语义连贯性

Result: 显著减少记忆token数量，性能优于朴素平均或直接拼接基线；在固定上下文预算下，聚类驱动合并产生更紧凑的记忆表示并持续提升生成质量

Conclusion: 聚类压缩策略有效平衡上下文效率与个性化质量，为设备端LLM的记忆管理提供实用解决方案

Abstract: Large language models (LLMs) often rely on user-specific memories distilled from past interactions to enable personalized generation. A common practice is to concatenate these memories with the input prompt, but this approach quickly exhausts the limited context available in on-device LLMs. Compressing memories by averaging can mitigate context growth, yet it frequently harms performance due to semantic conflicts across heterogeneous memories. In this work, we introduce a clustering-based memory compression strategy that balances context efficiency and personalization quality. Our method groups memories by similarity and merges them within clusters prior to concatenation, thereby preserving coherence while reducing redundancy. Experiments demonstrate that our approach substantially lowers the number of memory tokens while outperforming baseline strategies such as naive averaging or direct concatenation. Furthermore, for a fixed context budget, clustering-driven merging yields more compact memory representations and consistently enhances generation quality.

</details>


### [30] [Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory](https://arxiv.org/abs/2601.18771)
*Yanming Liu,Xinyue Peng,Zixuan Yan,Yanxin Shen,Wenjie Xu,Yuefeng Huang,Xinyi Wang,Jiannan Cao,Jianwei Yin,Xuhong Zhang*

Main category: cs.CL

TL;DR: Dep-Search是一个依赖感知的搜索框架，通过结构化推理、检索和持久内存集成，解决了现有搜索框架在管理子问题依赖、重用检索知识和学习最优搜索策略方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有搜索框架虽然增强了LLMs的复杂推理能力，但仍严重依赖隐式自然语言推理来确定搜索策略和跨推理步骤利用检索信息。这种隐式推理在管理子问题依赖、高效重用先前检索知识以及通过强化学习学习最优搜索策略方面存在根本性挑战。

Method: 提出Dep-Search框架，通过GRPO集成结构化推理、检索和持久内存。引入显式控制机制，使模型能够：1）分解具有依赖关系的子问题；2）在需要时检索信息；3）从内存访问先前存储的知识；4）将长推理上下文总结为可重用的内存条目。

Result: 在七个不同的问答数据集上进行广泛实验，证明Dep-Search显著增强了LLMs处理复杂多跳推理任务的能力，在不同模型规模上都实现了对强基线的实质性改进。

Conclusion: Dep-Search通过依赖感知的搜索框架解决了现有搜索框架的局限性，通过结构化推理、检索和持久内存的集成，为LLMs在复杂多跳推理任务中提供了更有效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.

</details>


### [31] [Revealing the Truth with ConLLM for Detecting Multi-Modal Deepfakes](https://arxiv.org/abs/2601.17530)
*Gautam Siddharth Kashyap,Harsh Joshi,Niharika Jain,Ebad Shabbir,Jiechao Gao,Nipun Joshi,Usman Naseem*

Main category: cs.CL

TL;DR: 提出ConLLM框架，通过对比学习和LLM推理解决深度伪造检测中的模态碎片化和浅层跨模态推理问题，在音频、视频和视听多模态任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对社会政治稳定构成严重威胁，现有检测方法存在两个核心局限：1) 模态碎片化导致跨多样化和对抗性深度伪造模态的泛化能力差；2) 浅层跨模态推理导致细粒度语义不一致性检测有限。

Method: 提出ConLLM（对比学习与大语言模型）混合框架，采用两阶段架构：第一阶段使用预训练模型提取模态特定嵌入；第二阶段通过对比学习对齐嵌入以缓解模态碎片化，并使用基于LLM的推理细化嵌入以解决浅层跨模态推理问题，捕获语义不一致性。

Result: ConLLM在音频、视频和视听多模态上表现优异：音频深度伪造EER降低达50%，视频准确率提升达8%，视听任务准确率提升约9%。消融研究证实基于PTM的嵌入为各模态带来9%-10%的稳定改进。

Conclusion: ConLLM通过结合对比学习和LLM推理，有效解决了深度伪造检测中的模态碎片化和浅层跨模态推理问题，在多模态检测任务中实现了显著性能提升，为鲁棒的多模态深度伪造检测提供了有效解决方案。

Abstract: The rapid rise of deepfake technology poses a severe threat to social and political stability by enabling hyper-realistic synthetic media capable of manipulating public perception. However, existing detection methods struggle with two core limitations: (1) modality fragmentation, which leads to poor generalization across diverse and adversarial deepfake modalities; and (2) shallow inter-modal reasoning, resulting in limited detection of fine-grained semantic inconsistencies. To address these, we propose ConLLM (Contrastive Learning with Large Language Models), a hybrid framework for robust multimodal deepfake detection. ConLLM employs a two-stage architecture: stage 1 uses Pre-Trained Models (PTMs) to extract modality-specific embeddings; stage 2 aligns these embeddings via contrastive learning to mitigate modality fragmentation, and refines them using LLM-based reasoning to address shallow inter-modal reasoning by capturing semantic inconsistencies. ConLLM demonstrates strong performance across audio, video, and audio-visual modalities. It reduces audio deepfake EER by up to 50%, improves video accuracy by up to 8%, and achieves approximately 9% accuracy gains in audio-visual tasks. Ablation studies confirm that PTM-based embeddings contribute 9%-10% consistent improvements across modalities.

</details>


### [32] [Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection](https://arxiv.org/abs/2601.17532)
*Zhipeng Song,Yizhi Zhou,Xiangyu Kong,Jiulong Jiao,Xinrui Bao,Xu You,Xueqing Shi,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: 本文提出信息增益剪枝（IGP）方法，通过生成器对齐的效用信号选择和过滤检索到的证据，在有限上下文预算下优化检索增强生成的质量-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）虽然能为大语言模型提供外部证据，但在有限上下文预算下，关键挑战在于决定注入哪些检索到的段落。研究发现检索相关性指标（如NDCG）与端到端QA质量相关性较弱，在多段落注入时甚至可能负相关，因为冗余和轻微冲突会破坏生成的稳定性。

Method: 提出信息增益剪枝（IGP）方法，这是一个部署友好的重排序和剪枝模块。它使用生成器对齐的效用信号来选择证据，在截断前过滤弱或有害的段落，而不改变现有的预算接口。IGP通过评估每个段落对生成答案的信息增益来进行选择和过滤。

Result: 在五个开放域QA基准测试和多种检索器和生成器上，IGP始终改善了质量-成本权衡。在代表性的多证据设置中，与仅使用检索器的基线相比，IGP在平均F1上带来约12-20%的相对改进，同时将最终阶段输入token减少约76-79%。

Conclusion: IGP通过生成器对齐的证据选择有效解决了RAG在有限上下文预算下的证据注入问题，显著提高了检索增强生成系统的效率和效果，为实际部署提供了实用的解决方案。

Abstract: Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.

</details>


### [33] [Sequence Repetition Enhances Token Embeddings and Improves Sequence Labeling with Decoder-only Language Models](https://arxiv.org/abs/2601.17585)
*Matija Luka Kukić,Marko Čuljak,David Dukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 序列重复（SR）是一种非侵入性方法，使仅解码器模型具备双向性，提升序列标注任务性能，超越编码器和无掩码解码器


<details>
  <summary>Details</summary>
Motivation: 现代语言模型是自回归训练的，仅基于前缀条件，而序列标注任务需要双向上下文。虽然移除因果掩码可使解码器模型适应序列标注，但需要大量修改基础模型功能。本研究探索序列重复作为更少侵入性的替代方案

Method: 提出序列重复方法，通过重复输入序列使仅解码器模型具备双向性。通过微调实验验证该方法，比较不同重复次数的影响，并分析中间层与最终层嵌入的有效性

Result: 序列重复使解码器具备内在双向性，提升词级嵌入质量，超越编码器和无掩码解码器。增加重复次数不会降低序列标注性能。中间层嵌入与最终层嵌入效果相当，但计算效率显著更高

Conclusion: 序列重复缓解了解码器的结构限制，使语言模型更高效、适应性更强，拓宽了其在词级任务中的应用范围

Abstract: Modern language models (LMs) are trained in an autoregressive manner, conditioned only on the prefix. In contrast, sequence labeling (SL) tasks assign labels to each individual input token, naturally benefiting from bidirectional context. This discrepancy has historically led SL to rely on inherently bidirectional encoder-only models. However, the rapid development of decoder-only models has raised the question of whether they can be adapted to SL. While causal mask removal has emerged as a viable technique for adapting decoder-only models to leverage the full context for SL, it requires considerable changes to the base model functionality. In this work, we explore sequence repetition (SR) as a less invasive alternative for enabling bidirectionality in decoder-only models. Through fine-tuning experiments, we show that SR inherently makes decoders bidirectional, improving the quality of token-level embeddings and surpassing encoders and unmasked decoders. Contrary to earlier claims, we find that increasing the number of repetitions does not degrade SL performance. Finally, we demonstrate that embeddings from intermediate layers are highly effective for SR, comparable to those from final layers, while being significantly more efficient to compute. Our findings underscore that SR alleviates the structural limitations of decoders, enabling more efficient and adaptable LMs and broadening their applicability to other token-level tasks.

</details>


### [34] [From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs](https://arxiv.org/abs/2601.17593)
*Tianjun Zhong,Linyang He,Nima Mesgarani*

Main category: cs.CL

TL;DR: LLM隐藏状态编码推理有向无环图结构，可通过轻量级探针预测节点深度和距离，揭示推理内部图结构


<details>
  <summary>Details</summary>
Motivation: 现有研究多将推理视为线性链，但许多推理问题更适合用有向无环图表示，其中中间结论依赖多个前提、分支并行推导、后期合并或重用。理解模型内部是否反映这种图结构推理仍是一个开放问题。

Method: 提出推理DAG探测框架，将每个推理节点与文本实现关联，训练轻量级探针从隐藏状态预测两个图论属性：节点深度和成对节点距离。分析DAG结构的层级涌现，并评估破坏推理相关结构但保留表面文本属性的控制条件。

Result: 结果表明推理DAG几何在中间层有意义地编码，可恢复性随节点深度和模型规模系统性变化，表明LLM推理不仅是顺序的，而且表现出可测量的内部图结构。

Conclusion: LLM推理内部存在可探测的图结构，推理DAG几何在隐藏状态中以线性可访问形式编码，这为理解复杂推理的机制表征提供了新视角。

Abstract: Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.
  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.

</details>


### [35] [Learning to Ideate for Machine Learning Engineering Agents](https://arxiv.org/abs/2601.17596)
*Yunxiang Zhang,Kang Zhou,Zhichao Xu,Kiran Ramnath,Yun Zhou,Sangmin Woo,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: MLE-Ideator：双智能体框架，将构思与实现分离，通过专门的构思器帮助实现智能体进行迭代优化，在MLE-Bench上显著优于仅实现智能体基线，且构思器可通过强化学习训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习工程（MLE）智能体在迭代优化算法效果方面存在困难，需要分离构思与实现过程以提高优化能力。

Method: 提出MLE-Ideator双智能体框架：实现智能体负责算法实施，构思智能体专门提供战略帮助；构思器可通过强化学习训练，仅需1K训练样本即可提升性能。

Result: 1. 在无需训练的设置下，框架在MLE-Bench上显著优于仅实现智能体基线；2. 强化学习训练的Qwen3-8B构思器相比未训练版本实现11.5%相对提升，并超越Claude Sonnet 3.5。

Conclusion: 该框架为训练战略AI系统进行科学发现提供了有前景的路径，通过分离构思与实现并训练构思器，能有效提升机器学习工程任务的迭代优化能力。

Abstract: Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.

</details>


### [36] [What Language Models Know But Don't Say: Non-Generative Prior Extraction for Generalization](https://arxiv.org/abs/2601.17609)
*Sara Rezaeimanesh,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: LoID：一种确定性方法，通过直接访问LLM的token级预测来提取贝叶斯逻辑回归的信息先验分布，在协变量偏移的OOD设置中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 在医学和金融等领域，大规模标注数据成本高昂且通常不可得，导致在小数据集上训练的模型难以泛化到真实世界人群。大型语言模型包含这些领域多年研究的广泛知识，但如何有效提取这些知识用于统计模型仍具挑战。

Method: LoID通过精心构造的句子探测LLM在相反语义方向（正面vs负面影响）上的置信度，测量LLM在不同表述中一致偏好某一方向的程度，从而提取模型对每个特征影响的强度和可靠性的信念。该方法直接访问token级预测而非依赖生成文本。

Result: 在10个真实世界表格数据集上，在协变量偏移的合成OOD设置下，LoID显著提升了在OOD数据上训练的逻辑回归性能，恢复了高达59%相对于全数据集拟合的oracle模型的性能差距。在8/10数据集上优于AutoElicit和LLMProcesses方法。

Conclusion: LoID提供了一种可重复且计算高效的机制，将LLM知识整合到贝叶斯推断中，在数据稀缺且存在协变量偏移的场景下，能够有效利用LLM的领域知识提升模型泛化能力。

Abstract: In domains like medicine and finance, large-scale labeled data is costly and often unavailable, leading to models trained on small datasets that struggle to generalize to real-world populations. Large language models contain extensive knowledge from years of research across these domains. We propose LoID (Logit-Informed Distributions), a deterministic method for extracting informative prior distributions for Bayesian logistic regression by directly accessing their token-level predictions. Rather than relying on generated text, we probe the model's confidence in opposing semantic directions (positive vs. negative impact) through carefully constructed sentences. By measuring how consistently the LLM favors one direction across diverse phrasings, we extract the strength and reliability of the model's belief about each feature's influence. We evaluate LoID on ten real-world tabular datasets under synthetic out-of-distribution (OOD) settings characterized by covariate shift, where the training data represents only a subset of the population. We compare our approach against (1) standard uninformative priors, (2) AutoElicit, a recent method that prompts LLMs to generate priors via text completions, (3) LLMProcesses, a method that uses LLMs to generate numerical predictions through in-context learning and (4) an oracle-style upper bound derived from fitting logistic regression on the full dataset. We assess performance using Area Under the Curve (AUC). Across datasets, LoID significantly improves performance over logistic regression trained on OOD data, recovering up to \textbf{59\%} of the performance gap relative to the oracle model. LoID outperforms AutoElicit and LLMProcessesc on 8 out of 10 datasets, while providing a reproducible and computationally efficient mechanism for integrating LLM knowledge into Bayesian inference.

</details>


### [37] [Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization](https://arxiv.org/abs/2601.17658)
*Bich Ngoc,Doan,Giuseppe Russo,Gianmarco De Francisci Morales,Robert West*

Main category: cs.CL

TL;DR: 该研究通过分析QAnon支持社区中的12747个叙事，系统性地绘制了阴谋论信徒的激进化轨迹，识别了六种激进化人格类型，并量化了这些类型对亲友造成的特定情感伤害。


<details>
  <summary>Details</summary>
Motivation: 现有大规模计算研究主要关注阴谋论对公共话语的宏观影响（如信任侵蚀和两极分化），但往往忽视了其对信徒亲友造成的个人情感伤害。本研究旨在填补这一空白，系统性地理解激进化作为关系现象的人际后果。

Method: 采用混合方法：1) 使用BERTopic主题建模分析12747个r/QAnonCasualties社区叙事，绘制激进化轨迹（识别前兆条件、触发因素和激进化后特征）；2) 应用LDA图模型识别六种反复出现的QAnon信徒原型（"激进化人格"）；3) 使用LLM辅助的情感检测和回归模型，将这些人格类型与叙述者报告的具体情感伤害联系起来。

Result: 研究发现：1) 识别出六种QAnon信徒的激进化人格类型；2) 这些人格类型不仅仅是描述性的，而是叙述者所经历特定情感伤害的有力预测因子；3) 被感知为有意意识形态选择的激进化与叙述者的愤怒和厌恶相关，而以个人和认知崩溃为特征的激进化则与恐惧和悲伤相关。

Conclusion: 本研究首次提供了理解激进化作为关系现象的实证框架，为研究人员和实践者应对其人际后果提供了重要路线图。研究揭示了激进化人格类型与特定情感伤害之间的系统性关联，强调了从关系角度理解激进化的重要性。

Abstract: The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term "radicalization personas." Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.

</details>


### [38] [UrduLM: A Resource-Efficient Monolingual Urdu Language Model](https://arxiv.org/abs/2601.17664)
*Syed Muhammad Ali,Hammad Sajid,Zainab Haider,Ali Muhammad Asad,Haya Fatima,Abdul Samad*

Main category: cs.CL

TL;DR: UrduLM：首个专为乌尔都语设计的单语预训练语言模型，在低资源环境下训练，性能与规模大30倍的多语言模型相当


<details>
  <summary>Details</summary>
Motivation: 乌尔都语（全球2.3亿使用者）缺乏专门的基于Transformer的语言模型和高质量语料库。现有的多语言模型对乌尔都语支持有限，存在性能差、计算成本高、文化不准确等问题，主要原因是训练数据不足。

Method: 1. 从多样化来源整理33GB乌尔都语语料库；2. 开发定制BPE分词器，相比多语言替代方案减少至少20-30%的分词开销；3. 预训练100M参数的仅解码器模型。

Result: 在少样本评估中，UrduLM与规模大30倍的多语言模型性能相当：情感分类准确率达到66.6%，语法纠正任务的BLEU分数超过30。

Conclusion: 完整的方法论（包括语料库、分词器、模型权重和评估基准）已开源发布，为乌尔都语NLP研究建立基线，并为其他资源不足语言提供了可扩展框架。

Abstract: Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.

</details>


### [39] [S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference](https://arxiv.org/abs/2601.17702)
*Qingsen Ma,Dianyun Wang,Yaoye Wang,Lechen Ning,Sujie Zhu,Xiaohang Zhang,Jiaming Lyu,Linhao Ren,Zhenbo Xu,Zhaofeng He*

Main category: cs.CL

TL;DR: S3-Attention是一种内存优先的推理时框架，通过将长上下文处理视为注意力对齐的内生检索，完全丢弃KV缓存，使用稀疏自编码器将键值投影解码为稀疏特征标识符，并构建CPU倒排索引来检索证据片段。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理多文档和长格式输入时面临内存效率低和噪声问题。传统的KV缓存随上下文长度线性扩展，而外部检索方法通常返回词汇相似但因果无关的段落。

Method: S3-Attention将长上下文处理视为注意力对齐的内生检索：1) 使用轻量级稀疏自编码器将瞬态键和查询投影解码为top-k稀疏特征标识符；2) 在单次流式扫描中构建CPU倒排索引，将特征映射到token位置或片段；3) 生成时使用特征共激活检索紧凑证据片段，可选与BM25融合进行精确词汇匹配。

Result: 在统一的LongBench评估协议下，S3-Hybrid在多个模型系列中与完整上下文推理结果接近，并在多个信息密集场景中提高了鲁棒性。但当前原型存在工程限制，比优化的完整KV基线具有更高的实际延迟。

Conclusion: S3-Attention通过内存优先的推理框架有效解决了长上下文处理的内存和噪声效率问题，完全丢弃KV缓存并限制GPU内存使用。虽然当前原型存在延迟问题，但该方法为长上下文推理提供了有前景的方向，需要未来内核级优化。

Abstract: Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.
  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.
  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.

</details>


### [40] [Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings](https://arxiv.org/abs/2601.17705)
*Abdullah Qureshi,Kenneth Rice,Alexander Wolpert*

Main category: cs.CL

TL;DR: 提出DDR（距离-距离比）作为LLM句子嵌入相似性度量，通过测量上下文前后嵌入相似度的变化率来评估语义影响，在受控扰动实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入相似性度量必须符合人类对文本相似性的感知，现有方法可能无法准确捕捉上下文对语义的影响，需要更精细的相似性度量来区分语义相似和相异的文本。

Method: 提出DDR（距离-距离比）度量，受Lipschitz连续性启发，测量预上下文词嵌入相似度与后上下文LLM嵌入相似度之间的变化率，从而量化上下文的语义影响。通过设计扰动实验：从句子数据集中选取句子，生成变体（替换1-3个词为同义词或随机词），比较DDR与现有相似性度量的性能。

Result: DDR在实验中一致提供更精细的区分能力，能更好地区分语义相似和相异的文本，即使在最小化、受控的编辑条件下也表现优异，优于其他主流相似性度量。

Conclusion: DDR是一种有效的LLM句子嵌入相似性度量，能够准确捕捉上下文对语义的影响，符合人类对文本相似性的感知，为文本相似性评估提供了更精细的工具。

Abstract: A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.

</details>


### [41] [A Computational Approach to Visual Metonymy](https://arxiv.org/abs/2601.17706)
*Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.CL

TL;DR: 该论文首次对视觉转喻进行系统性计算研究，提出基于符号学理论的生成框架，构建了包含2000个多选题的ViMET数据集，揭示了当前视觉语言模型在理解间接视觉引用方面与人类存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 图像通常传达比其字面描绘更多的信息（如工具暗示职业、文物暗示传统），这种间接视觉引用（视觉转喻）要求观者通过关联线索恢复目标概念。当前缺乏对视觉转喻的计算研究，需要建立评估多模态模型认知推理能力的数据集和方法。

Method: 提出基于符号学理论的新颖流程，利用大语言模型和文本到图像模型生成转喻视觉表示。通过该框架构建ViMET数据集（2000个多选题），用于评估多模态语言模型的认知推理能力。

Result: 实验结果显示人类表现（86.9%）与最先进的视觉语言模型（65.9%）之间存在显著差距，表明机器在解释间接视觉引用方面存在局限性。数据集已公开可用。

Conclusion: 这是首次对视觉转喻的计算研究，提出的框架和数据集为评估多模态模型的认知推理能力提供了基准，揭示了当前模型在理解间接视觉引用方面的不足，为未来研究提供了重要基础。

Abstract: Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.

</details>


### [42] [Unsupervised Elicitation of Moral Values from Language Models](https://arxiv.org/abs/2601.17728)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei*

Main category: cs.CL

TL;DR: 本文提出通过内部一致性最大化（ICM）算法无监督地激发预训练语言模型的潜在道德推理能力，无需人工监督即可获得高质量道德判断标签，并在多个基准测试中超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统普及，将其行为与人类价值观对齐至关重要。先前研究表明语言模型固有的道德推理能力有限，需要明确的道德教导。然而，构建道德评估的基准数据面临多元道德框架和普遍偏见的挑战。本文探索无监督激发作为替代方案，研究预训练语言模型是否具备可通过无监督方法激发的内在道德推理能力。

Method: 采用内部一致性最大化（ICM）算法，在三个基准数据集（Norm Bank、ETHICS等）和四个语言模型上进行实验。ICM通过最大化模型内部一致性来无监督地生成道德判断标签，无需人类监督。研究测试了ICM在可靠标注道德判断、跨道德框架泛化以及减轻社会偏见方面的能力。

Result: ICM在Norm Bank和ETHICS基准测试中优于所有预训练和聊天机器人基线。使用ICM标签进行微调的模型表现与使用人类标签的模型相当或更优。在理论驱动的道德框架中，ICM在正义和常识道德方面获得最大相对增益。虽然聊天机器人语言模型的社会偏见失败率与预训练模型相当，但ICM将此类错误减少了一半以上，在种族、社会经济地位和政治方面的改进最大。

Conclusion: 预训练语言模型具备可通过无监督方法（如ICM）激发的潜在道德推理能力，这为AI对齐提供了一条可扩展的路径。ICM不仅能可靠地标注道德判断，还能跨道德框架泛化并显著减轻社会偏见。

Abstract: As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.

</details>


### [43] [Hylog: A Hybrid Approach to Logging Text Production in Non-alphabetic Scripts](https://arxiv.org/abs/2601.17753)
*Roberto Crotti,Giovanni Denaro,Zhiqiang Du,Ricardo Muñoz Martín*

Main category: cs.CL

TL;DR: Hylog是一个混合日志系统，结合分析性键盘记录和生态文本记录，用于研究非字母文字输入法编辑器的屏幕转换过程。


<details>
  <summary>Details</summary>
Motivation: 现有研究键盘记录工具大多无法捕捉非字母文字输入法编辑器（IME）在屏幕上进行的转换过程，这限制了认知研究中文本生产的完整分析。

Method: 开发了模块化开源系统Hylog，通过插件捕获标准应用程序中的键盘输出和渲染文本，然后由混合器模块同步为双重轨迹。

Result: 在概念验证研究中成功捕获了按键、拉丁字母、中文字符和IME确认之间的时间间隔，这些测量传统键盘记录器无法获取。

Conclusion: Hylog能够支持关于IME中介打字中不同语言层认知限制和可供性的新假设，其插件架构可扩展到其他IME系统，促进更包容的多语言文本生产研究。

Abstract: Research keyloggers are essential for cognitive studies of text production, yet most fail to capture the on-screen transformations performed by Input Method Editors (IMEs) for non-alphabetic scripts. To address this methodological gap, we present Hylog, a novel hybrid logging system that combines analytical keylogging with ecological text logging for a more complete and finer-grained analysis. Our modular, open-source system uses plug-ins for standard applications (Microsoft Word, Google Chrome) to capture both keyboard output and rendered text, which a hybridizer module then synchronizes into a dual trace. To validate the system's technical feasibility and demonstrate its analytical capabilities, we conducted a proof-of-concept study where two volunteers translated a text into simplified Chinese. Hylog successfully captured keypresses and temporal intervals between Latin letters, Chinese characters, and IME confirmations -- some measurements invisible to traditional keyloggers. The resulting data enable the formulation of new, testable hypotheses about the cognitive restrictions and affordances at different linguistic layers in IME-mediated typing. Our plug-in architecture enables extension to other IME systems and fosters more inclusive multilingual text-production research.

</details>


### [44] [ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation](https://arxiv.org/abs/2601.17755)
*Jinyoung Park,Sanghyeok Lee,Omar Zia Khan,Hyunwoo J. Kim,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: ProGraph-R1提出了一种基于进度的图检索增强生成框架，通过结构感知的超图检索机制和基于进度的逐步策略优化，解决了现有RL-based GraphRAG方法在结构利用和奖励稀疏性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的GraphRAG框架（如Graph-R1）存在两个关键限制：1）主要依赖语义相似性进行检索，忽视了底层的图结构；2）依赖稀疏的结果级奖励，无法捕捉中间检索步骤的质量及其依赖关系。这些限制影响了多步推理的性能。

Method: ProGraph-R1包含两个核心组件：1）结构感知的超图检索机制，联合考虑语义相关性和图连接性，鼓励沿着多跳推理路径进行连贯遍历；2）基于进度的逐步策略优化，通过根据图中中间推理进度调整优势函数来提供密集学习信号，而不是仅依赖最终结果。

Result: 在多跳问答基准测试中，ProGraph-R1在推理准确性和生成质量方面持续优于现有的GraphRAG方法。

Conclusion: ProGraph-R1通过结合结构感知检索和基于进度的强化学习，有效解决了现有GraphRAG框架的局限性，为知识密集型问答任务中的多步推理提供了更强大的解决方案。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.

</details>


### [45] [Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali](https://arxiv.org/abs/2601.17764)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Jui Saha Pritha,Abdullah Al Noman,Abir Ahmed,Golam Md Mohiuddin,Tze Hui Liew*

Main category: cs.CL

TL;DR: 该研究分析了孟加拉语中LLMs的性别偏见特征，发现英语中心的偏见检测框架在孟加拉语中效果有限，需要基于本地文化和语境的定制化方法。


<details>
  <summary>Details</summary>
Motivation: 当前关于LLMs性别偏见的研究主要集中于英语，而对全球南方语言（如孟加拉语）中的语言和文化偏见研究不足。该研究旨在填补这一空白，评估孟加拉语中性别偏见的特征和程度，并检验现有偏见检测和缓解方法的有效性。

Method: 采用多种方法提取性别偏见话语：基于词典的挖掘、计算分类模型、基于翻译的比较分析、GPT驱动的偏见生成。同时进行了两项实地调查，在乡村和低收入地区收集真实偏见数据，以弥补自动化系统的不足。

Result: 研究发现，英语中心的偏见检测框架直接应用于孟加拉语时受到严重限制，主要受语言差异和社会文化因素影响。孟加拉语的性别偏见表现出与英语不同的特征，需要更本地化和语境敏感的方法。社区驱动的研究方法能识别自动化系统常忽略的文化相关偏见。

Conclusion: 该研究强调了为代表性不足的语言开发专门语言工具的必要性，为孟加拉语和其他印度语言中的偏见减少研究奠定了基础，促进了更包容和公平的NLP系统发展。

Abstract: Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.

</details>


### [46] [DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning](https://arxiv.org/abs/2601.17777)
*Xiaoyu Liu,Xiaoyu Guan,Di Liang,Xianjie Wu*

Main category: cs.CL

TL;DR: 提出一种动态参数隔离策略来解决SFT中的跷跷板效应，通过识别任务核心参数区域并分阶段冻结来减少任务间干扰


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)中，异构任务间的冲突目标会导致"跷跷板效应"：优化一个任务可能降低其他任务性能，特别是当模型参数被无差别更新时。参数异质性被认为是跨任务干扰的根本原因。

Method: 1. 在不同SFT任务上独立微调LLMs，识别每个任务的核心参数区域（更新幅度最大的参数子集）；2. 合并核心参数区域高度重叠的任务进行联合训练，将不相交的任务组织到不同阶段；3. 在多阶段SFT中，冻结先前任务获得的核心参数，防止被后续任务覆盖。

Result: 在多个公共数据集上的密集实验表明，动态参数隔离策略能持续减少数据冲突，相比多阶段和多任务调优基线，实现了持续的性能提升。

Conclusion: 通过识别和隔离任务特定参数区域，可以有效缓解SFT中的跷跷板效应，该方法为处理异构任务间的参数冲突提供了原则性解决方案。

Abstract: Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the "seesaw effect": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.

</details>


### [47] [Controlling Reading Ease with Gaze-Guided Text Generation](https://arxiv.org/abs/2601.17781)
*Andreas Säuberli,Darja Jepifanova,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: 使用眼动预测模型控制语言模型生成文本的阅读难度，通过眼动追踪实验验证方法有效性


<details>
  <summary>Details</summary>
Motivation: 利用阅读时的眼动模式反映认知负荷的特性，开发能够控制文本阅读难度的生成方法，以改善信息可及性和语言学习材料

Method: 采用预测人类注视模式的模型来引导语言模型输出，使其产生特定的阅读行为，通过眼动追踪实验评估方法效果

Result: 方法能有效使生成文本变得更易或更难阅读，体现在阅读时间和感知难度上；统计分析显示阅读行为变化主要源于影响词汇处理的特征

Conclusion: 基于眼动预测的文本生成方法能有效控制阅读难度，在信息可及性文本简化和个性化语言学习材料生成方面具有应用潜力

Abstract: The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.

</details>


### [48] [Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations](https://arxiv.org/abs/2601.17786)
*Yixin Liu,Kehan Yan,Shiyuan Li,Qingfeng Chen,Shirui Pan*

Main category: cs.CL

TL;DR: MCA²是一个多视图文本异常检测框架，通过集成多个预训练语言模型的嵌入，采用多视图重建模型提取正常文本模式，并设计了对比协作模块和自适应分配模块来提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有两步式"嵌入-检测器"文本异常检测方法通常只使用单一嵌入模型，缺乏跨不同数据集和异常类型的适应性，限制了其在实际应用中的有效性。

Method: 提出MCA²框架：1) 利用多个预训练语言模型的嵌入构建多视图表示；2) 采用多视图重建模型从多个嵌入视角提取正常文本模式；3) 设计对比协作模块加强不同视图间的交互和互补性；4) 开发自适应分配模块自动分配每个视图的贡献权重，提高对多样化数据集的适应性。

Result: 在10个基准数据集上的广泛实验验证了MCA²相对于强基线的有效性，表明该方法能够显著提升文本异常检测性能。

Conclusion: MCA²通过集成多个预训练语言模型的嵌入、设计多视图重建、对比协作和自适应分配机制，有效解决了现有文本异常检测方法中单一嵌入模型和缺乏适应性的问题，为实际应用提供了更强大的解决方案。

Abstract: Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step "embedding-detector" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.

</details>


### [49] [On the Emergence and Test-Time Use of Structural Information in Large Language Models](https://arxiv.org/abs/2601.17869)
*Michelle Chao Chen,Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: 语言模型学习结构信息的能力与复杂推理任务相关，但测试时组合生成能力有限


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何从观测数据中学习抽象结构信息，这对于科学发现中的机制理解和灵活测试时组合生成至关重要

Method: 设计基于语言结构转换的自然语言数据集，在受控设置下研究语言模型学习结构信息的能力

Result: 学习结构信息的出现与复杂推理任务相关，但模型在测试时进行组合生成的能力仍然有限

Conclusion: 语言模型能够学习结构信息，但在利用这些信息进行测试时组合生成方面存在局限性，需要进一步研究改进

Abstract: Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.

</details>


### [50] [Assessment of Generative Named Entity Recognition in the Era of Large Language Models](https://arxiv.org/abs/2601.17898)
*Qi Zhan,Yile Wang,Hui Huang*

Main category: cs.CL

TL;DR: 该论文系统评估了开源大语言模型在平面和嵌套命名实体识别任务上的表现，发现通过参数高效微调和结构化输出格式，开源LLMs能达到与传统编码器模型竞争的性能，且其NER能力源于指令遵循和生成能力而非记忆。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，命名实体识别正从序列标注任务转向生成范式。作者旨在系统评估开源LLMs在平面和嵌套NER任务上的表现，研究生成式NER与传统NER模型的性能差距、输出格式影响、LLMs是否依赖记忆以及微调后通用能力保持等问题。

Method: 采用8个不同规模的开源LLMs和4个标准NER数据集进行实验。研究内容包括：1) 比较生成式NER与传统编码器模型性能；2) 分析不同输出格式（如内联括号或XML）的影响；3) 通过消融实验验证LLMs是否依赖实体-标签对的记忆；4) 评估NER指令微调对LLMs通用能力的影响。

Result: 实验发现：1) 通过参数高效微调和结构化输出格式，开源LLMs能达到与传统编码器模型竞争的性能，甚至超越GPT-3等闭源模型；2) LLMs的NER能力源于指令遵循和生成能力，而非简单的实体-标签对记忆；3) NER指令微调对LLMs的通用能力影响极小，甚至因增强实体理解而提升在DROP等数据集上的表现。

Conclusion: 生成式NER结合LLMs是传统方法的有前景且用户友好的替代方案。开源LLMs通过适当微调和输出格式设计，在NER任务上展现出强大能力，且不损害其通用语言理解能力。研究结果支持了生成式NER范式的可行性。

Abstract: Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at https://github.com/szu-tera/LLMs4NER.

</details>


### [51] [ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation](https://arxiv.org/abs/2601.17921)
*Yi Zhao,Qinghua Yao,Xinyuan song,Wei Zhu*

Main category: cs.CL

TL;DR: ShapLoRA：基于Shapley值的可解释LoRA秩分配框架，通过结合敏感度度量和协作博弈思想，提出更可解释的重要性度量方法，在多种任务上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA秩分配方法存在局限性，主要依赖于不可解释且不可靠的重要性度量。传统均匀秩分配的LoRA方法效率有限，而现有的秩分配方法缺乏可解释性，需要更可靠的重要性度量来优化参数分配。

Method: 提出ShapLoRA框架，受可解释归因度量Shapley值的启发，将基于敏感度的度量与LoRA秩之间协作博弈的联盟思想相结合，提出称为Shapley敏感度的更可解释重要性度量。优化现有工作流程：在单独验证集上计算Shapley敏感度；建立分配-重训练程序以确保公平比较。

Result: 在多种挑战性任务上的实验结果表明，ShapLoRA方法能够在可比较的可调参数数量下，优于最近的基线方法。

Conclusion: ShapLoRA通过引入基于Shapley值的可解释重要性度量，解决了现有LoRA秩分配方法的局限性，提供了更可靠和可解释的参数高效微调方法，在保持参数效率的同时提升了性能。

Abstract: Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.

</details>


### [52] [A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models](https://arxiv.org/abs/2601.17952)
*Michail Mamalakis,Tiago Azevedo,Cristian Cosentino,Chiara D'Ercoli,Subati Abulikemu,Zhongtian Sun,Richard Bethlehem,Pietro Lio*

Main category: cs.CL

TL;DR: 提出统一可解释性框架，结合归因和机制视角，通过单义特征提取减少方法间变异，为临床LLM应用提供稳定重要性评分


<details>
  <summary>Details</summary>
Motivation: 在阿尔茨海默病等临床环境中部署大型语言模型时，可解释性至关重要。现有归因方法存在高方法间变异性和不稳定解释，而机制可解释性方法缺乏与模型输入输出的直接对齐，无法提供明确的重要性评分。

Method: 引入统一可解释性框架，通过单义特征提取整合归因和机制视角。在LLM层级别构建单义嵌入空间，优化框架以显式减少方法间变异性，生成稳定的输入级重要性评分，并通过感兴趣层的解压缩表示突出显著特征。

Result: 该方法产生稳定的输入级重要性评分，通过解压缩表示突出显著特征，推进LLM在认知健康和神经退行性疾病中的安全可信应用。

Conclusion: 提出的统一框架通过单义特征提取整合了归因和机制视角，解决了现有方法的局限性，为临床LLM应用提供了更稳定可靠的可解释性解决方案。

Abstract: Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.

</details>


### [53] [AI-based approach to burnout identification from textual data](https://arxiv.org/abs/2601.17993)
*Marina Zavertiaeva,Petr Parshakov,Mikhail Usanin,Aleksei Smirnov,Sofia Paklina,Anastasiia Kibardina*

Main category: cs.CL

TL;DR: 提出基于RuBERT模型的AI方法，利用NLP技术从文本数据中检测职业倦怠，通过ChatGPT生成合成句子和YouTube用户评论进行微调，可评估文本的倦怠概率。


<details>
  <summary>Details</summary>
Motivation: 开发自动化工具来监测高压工作环境中的职业倦怠迹象，通过分析文本数据实现早期识别和干预，解决传统方法依赖问卷调查的局限性。

Method: 采用预训练的RuBERT模型（原用于情感分析），使用ChatGPT生成的合成句子和俄罗斯YouTube视频中关于倦怠的用户评论进行微调，构建倦怠检测模型。

Result: 成功开发出能够为输入文本分配倦怠概率的模型，可处理大量书面通信数据，识别高压工作环境中的倦怠相关语言信号。

Conclusion: 该方法展示了AI在心理健康监测中的应用潜力，为组织提供了一种可扩展的工具来检测和预防职业倦怠，特别是在高压工作环境中。

Abstract: This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.

</details>


### [54] [Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems](https://arxiv.org/abs/2601.18012)
*Hendrika Maclean,Mert Can Cakmak,Muzakkiruddin Ahmed Mohammed,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 评估大语言模型在精确数值计算和可审计输出方面的可靠性，以工资单系统为案例研究，发现需要结合精心设计的提示和显式计算才能达到分币级精度


<details>
  <summary>Details</summary>
Motivation: 大语言模型在日常写作、搜索和分析中广泛应用，但其在精确数值计算和可审计输出方面仍不可靠，需要研究在高风险场景（如工资单系统）中的表现

Method: 使用分层数据集（从基础到复杂案例）、多种提示策略（从最小基线到模式引导和推理变体）以及多个模型家族（GPT、Claude、Perplexity、Grok、Gemini）进行系统性评估

Result: 结果表明存在明确的工作模式：在某些情况下精心设计的提示足够，而在其他情况下需要显式计算；提供了可复现的框架和实用部署指南

Conclusion: 为在要求准确性和保证性的场景中部署LLMs提供了紧凑、可复现的框架和实用指导，强调了结合提示策略和显式计算的重要性

Abstract: Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.

</details>


### [55] [A System for Name and Address Parsing with Large Language Models](https://arxiv.org/abs/2601.18014)
*Adeeba Tarannum,Muzakkiruddin Ahmed Mohammed,Mert Can Cakmak,Shames Al Mandalawi,John Talburt*

Main category: cs.CL

TL;DR: 提出一种结合提示工程与确定性验证的框架，将非结构化人员地址文本转换为17字段结构化数据，无需微调，确保可复现性


<details>
  <summary>Details</summary>
Motivation: 传统规则方法和概率方法在干净输入上表现良好，但在噪声或多语言条件下失效；神经模型和LLM缺乏确定性控制和可复现性。需要一种既灵活又可靠的结构化信息提取方案

Method: 集成输入标准化、结构化提示、约束解码和严格规则验证的提示驱动验证中心框架，在固定实验设置下确保可复现性

Result: 在异构真实地址数据上评估显示高字段级准确率、强模式遵从性和稳定置信度校准，证明结合确定性验证与生成提示的优越性

Conclusion: 结合确定性验证与生成提示提供了鲁棒、可解释且可扩展的结构化信息提取方案，是训练密集型或领域特定模型的实用替代方案

Abstract: Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.

</details>


### [56] [Addressing LLM Diversity by Infusing Random Concepts](https://arxiv.org/abs/2601.18053)
*Pulin Agrawal,Prasoon Goyal*

Main category: cs.CL

TL;DR: 研究表明，在提示词中注入随机概念可以显著提升大语言模型输出的多样性


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的输出多样性有限，需要探索提升多样性的方法

Method: 在提示词前添加与问题无关的随机单词/句子，设计系统评估协议，通过"列举10位好莱坞演员"等任务测试多样性指标

Result: 在多个LLMs上的实验表明，添加随机内容能显著提高输出多样性

Conclusion: 随机性注入是提升LLM多样性的有效方法，评估协议为系统化基准测试提供了新思路

Abstract: Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.

</details>


### [57] [Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production](https://arxiv.org/abs/2601.18056)
*Ahmet Yavuz Uluslu,Elliot Murphy*

Main category: cs.CL

TL;DR: 该论文主张将振荡特征纳入双语产生错误研究，利用ROSE神经模型解释句法迁移，以跨语言影响为案例研究，探索更复杂的语言功能障碍生物标志物。


<details>
  <summary>Details</summary>
Motivation: 传统双语产生错误研究主要关注事件相关电位等时序特征，缺乏实现层面的神经计算约束。需要新的神经模型来解释句法迁移的形式特性和形态句法序列失败模式。

Method: 采用ROSE神经语言模型，将跨语言影响和功能抑制/竞争理论解释为L2句子规划期间的特定振荡失败模式，建立神经计算与行为表现之间的连接假设。

Result: ROSE模型能够捕捉双语产生中句法迁移的形式特性和形态句法序列失败模式的范围，为跨语言影响提供了神经计算解释，并支持探索更复杂的时空生物标志物。

Conclusion: 将振荡特征纳入双语研究不仅为ROSE模型提供了验证机会，还允许探索比传统神经特征更复杂的语言功能障碍生物标志物，推动了双语神经计算理论的发展。

Abstract: We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.

</details>


### [58] [Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models](https://arxiv.org/abs/2601.18065)
*Aryan Roy,Zekun Wang,Christopher J. MacLellan*

Main category: cs.CL

TL;DR: 研究比较视觉-语言模型(VLMs)与纯文本大语言模型(LLMs)对语言具体性的敏感度，发现VLMs在多模态预训练后表现出更接近人类的认知模式


<details>
  <summary>Details</summary>
Motivation: 探究多模态预训练是否能让视觉-语言模型比纯文本模型发展出更接近人类的对语言具体性的敏感度，将多模态训练视为感知基础的一种消融实验

Method: 使用匹配的Llama文本模型及其视觉对应版本进行控制比较，在三个层面测量具体性效应：输出行为、嵌入几何、注意力动态，并评估模型生成的具体性评分与人类规范的匹配度

Result: VLMs在更具体的输入上表现提升更大，具有更清晰的具体性结构化表示，产生的评分更符合人类规范，注意力模式显示更强的感知基础

Conclusion: 多模态预训练使VLMs发展出比纯文本LLMs更接近人类的对语言具体性的敏感度，即使在使用纯文本提示时也是如此，表明感知基础对语言理解有重要影响

Abstract: Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.

</details>


### [59] [Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents](https://arxiv.org/abs/2601.18077)
*Mahesh Ramesh,Kaousheik Jayakumar,Aswinkumar Ramkumar,Pavan Thodima,Aniket Rege*

Main category: cs.CL

TL;DR: 本文系统评估了17个先进LLM在Hanabi游戏中的合作推理能力，通过三种上下文工程设置（Watson、Sherlock、Mycroft）分析模型规模对协调性能的影响，并发布首个公开Hanabi数据集用于指令微调和强化学习，显著提升了4B开源模型的合作性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决不完全信息下的合作推理挑战，Hanabi游戏作为典型测试平台，需要心智理论和战略沟通能力。现有LLM在复杂多智能体协调任务中表现不足，需要系统评估其能力边界并探索提升方法。

Method: 方法包括：1）在2-5人Hanabi游戏中评估17个SOTA LLM；2）设计三种上下文工程设置：Watson（仅显式卡片信息）、Sherlock（程序化贝叶斯推理）、Mycroft（多轮状态跟踪工作记忆）；3）创建两个公开数据集：HanabiLogs（1520个完整游戏日志）和HanabiRewards（560个带密集动作价值标注的游戏）；4）对4B开源模型进行监督学习和RL微调。

Result: 结果显示：1）LLM能够维护内部工作记忆进行状态跟踪；2）不同LLM间的交叉游戏性能随模型强度平滑插值；3）在Sherlock设置下，最强推理模型平均得分超过15分，但仍落后于经验人类玩家（>20分）；4）监督微调提升合作性能21%，RL微调提升156%，接近最强专有推理模型（o4-mini）并在多个下游任务中展现泛化能力。

Conclusion: 结论表明：1）上下文工程和模型规模对合作推理至关重要；2）专用数据集微调能显著提升LLM的合作能力；3）HanabiRewards RL微调模型展现出超越Hanabi的泛化能力，在群体猜测、时序推理、指令跟随和数学推理任务中均有提升；4）研究为不完全信息下的多智能体合作推理提供了系统评估框架和有效提升方法。

Abstract: Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.

</details>


### [60] [CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations](https://arxiv.org/abs/2601.18102)
*Stephanie Fong,Zimu Wang,Guilherme C. Oliveira,Xiangyu Zhao,Yiwen Jiang,Jiahe Liu,Beau-Luke Colton,Scott Woods,Martha E. Shenton,Barnaby Nelson,Zongyuan Ge,Dominic Dwyer*

Main category: cs.CL

TL;DR: CHiRPE是一个临床NLP管道，通过转录的半结构化临床访谈预测精神病风险，并生成与临床医生共同开发的新型SHAP解释格式，实现超过90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统可解释AI方法与临床推理不匹配，缺乏临床医生输入，而医疗NLP工具的采用需要最终用户的可解释性。

Method: 整合症状领域映射、LLM摘要和BERT分类的NLP管道，使用944份半结构化访谈转录本训练，生成与临床医生共同开发的新型SHAP解释格式。

Result: 在三个BERT变体上均达到超过90%的准确率，优于基线模型；28名临床专家评估显示对新型概念引导解释（特别是混合图-文本摘要格式）有强烈偏好。

Conclusion: 临床引导的模型开发能产生准确且可解释的结果；下一步将在24个国际站点进行真实世界测试。

Abstract: The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.

</details>


### [61] [FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning](https://arxiv.org/abs/2601.18116)
*Lin Sun,Linglin Zhang,Jingang Huang,Change Jia,Zhengwei Cheng,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: FABLE是一个基于森林的自适应双路径LLM增强检索框架，通过构建LLM增强的层次化森林索引和双路径检索策略，在显著减少token消耗的同时达到与全上下文LLM推理相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM存在中间信息丢失、计算成本高、多文档推理扩展性差等问题，而传统RAG系统受限于平面分块检索，存在语义噪声且无法支持结构化跨文档合成。需要一种既能利用LLM能力又能克服这些限制的检索增强方法。

Method: FABLE框架包含两个核心组件：1) LLM增强的层次化森林索引构建，创建多粒度语义结构；2) 双路径检索策略，结合LLM引导的层次遍历和结构感知传播进行细粒度证据获取，并具有显式预算控制以实现自适应效率权衡。

Result: 实验表明FABLE持续优于SOTA RAG方法，在减少高达94% token消耗的情况下达到与全上下文LLM推理相当的准确率，证明长上下文LLM放大了而非完全替代结构化检索的需求。

Conclusion: 长上下文LLM并未使RAG过时，而是强调了结构化检索的重要性。FABLE通过集成LLM到知识组织和检索中，实现了高效且准确的检索增强生成，为长上下文推理提供了实用的解决方案。

Abstract: The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.
  We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.
  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.

</details>


### [62] [Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models](https://arxiv.org/abs/2601.18129)
*Kunat Pipatanakul,Pittawat Taveekitworachai*

Main category: cs.CL

TL;DR: Typhoon S：一种面向主权LLM的轻量级后训练方法，通过监督微调、策略蒸馏和小规模RFT，在有限资源下实现泰语等低资源语言的适配性和主权能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流LLM主要面向英语和中文等高资源语言，由少数拥有大规模计算和数据的组织开发，形成了技术壁垒。主权场景（如区域或国家级机构）需要在有限资源、严格透明度约束下保持对模型权重、训练数据和部署的控制与理解，因此需要解决适配性（将基础模型转化为通用助手）和主权能力（执行高风险、区域特定任务）两大核心需求。

Method: 提出Typhoon S后训练方法：1）监督微调；2）策略蒸馏；3）小规模强化微调（RFT），其中使用InK-GRPO扩展GRPO损失函数，增加下一词预测损失。该方法避免大规模指令数据收集和复杂偏好调优流程，以泰语为案例研究验证有效性。

Result: 该方法成功将主权适应和通用基础模型转化为指令调优模型，具有强大的通用性能。小规模RFT结合InK-GRPO显著提升泰语法律推理和泰语特定知识能力，同时保持通用能力。结果表明精心设计的后训练策略可以减少指令数据和计算规模需求。

Conclusion: Typhoon S为在学术规模资源下开发高质量主权LLM提供了实用路径，证明通过轻量级后训练方法可以在有限资源下实现低资源语言的适配性和主权能力，降低对大规模数据和计算的依赖。

Abstract: Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.

</details>


### [63] [Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models](https://arxiv.org/abs/2601.18162)
*Ani Harutyunyan,Sachin Kumar*

Main category: cs.CL

TL;DR: 论文在GoEmotions数据集上对比了三种细粒度情感识别模型：基于TF-IDF的逻辑回归、带注意力的BiLSTM和微调的BERT模型，发现逻辑回归在Micro-F1上表现最佳，而BERT在整体平衡性上最优。


<details>
  <summary>Details</summary>
Motivation: 细粒度情感识别是一个具有挑战性的多标签NLP任务，面临标签重叠和类别不平衡的问题。研究旨在比较不同建模方法在GoEmotions数据集上的表现，探索不同模型在处理表面词汇线索与上下文依赖情感方面的能力差异。

Method: 使用GoEmotions数据集的官方训练/验证/测试划分，采用三种模型架构：1) 基于TF-IDF特征和二元相关性的逻辑回归系统；2) 带注意力的双向LSTM；3) 为多标签分类微调的BERT模型。使用逆频率类别权重缓解类别不平衡问题。

Result: 逻辑回归获得最高的Micro-F1分数0.51，而BERT在整体平衡性上表现最佳，达到Macro-F1 0.49、Hamming Loss 0.036和Subset Accuracy 0.36，超越了原论文报告的结果。这表明高频情感常依赖表面词汇线索，而上下文表示能提升对稀有情感和模糊示例的识别能力。

Conclusion: 不同模型在细粒度情感识别任务中各有优势：逻辑回归在捕捉高频情感的表面词汇模式方面有效，而BERT的上下文表示能力在处理稀有情感和模糊案例时表现更优。研究为多标签情感分类提供了实用的建模基准。

Abstract: Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.

</details>


### [64] [MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2601.18204)
*Juexiang Ye,Xue Li,Xinyu Yang,Chengkai Huang,Lanshun Nie,Lina Yao,Dechen Zhan*

Main category: cs.CL

TL;DR: MemWeaver是一个统一记忆框架，通过结构化图记忆、经验记忆和文本证据记忆三个组件，结合双通道检索策略，显著提升长期交互中智能体的多跳推理和时间一致性能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体在长期交互中面临记忆系统不足的问题：非结构化检索或粗粒度抽象导致时间冲突、推理脆弱和可追溯性有限。需要支持时间一致性、多跳推理和跨会话证据重用的记忆系统。

Method: 提出MemWeaver框架，包含三个互联组件：1)时间基础图记忆用于结构化关系推理；2)经验记忆从重复观察中抽象出交互模式；3)段落记忆保留原始文本证据。采用双通道检索策略联合检索结构化知识和支持证据，构建紧凑且信息密集的推理上下文。

Result: 在LoCoMo基准测试中，MemWeaver显著提高了多跳推理和时间推理的准确性，同时相比长上下文基线减少了超过95%的输入上下文长度。

Conclusion: MemWeaver通过统一的结构化记忆框架有效解决了长期交互智能体中的记忆挑战，在保持高推理准确性的同时大幅降低了计算开销，为构建更可靠的长期交互智能体提供了有效方案。

Abstract: Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.

</details>


### [65] [TechING: Towards Real World Technical Image Understanding via VLMs](https://arxiv.org/abs/2601.18238)
*Tafazzul Nadeem,Bhavik Shangari,Manish Rai,Gagan Raj Gupta,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了一种通过合成数据训练VLM的方法，以解决手绘技术图表理解难题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 专业人士在讨论中常手绘技术图表（如流程图、框图等），但这些图表难以编辑。现有VLM在理解技术图表方面表现不佳，而收集大量真实手绘图像进行微调又不现实。

Method: 1. 创建大规模合成数据集（反映真实世界图像特征）；2. 引入多种新的自监督任务进行训练；3. 在合成图像上对Llama 3.2 11B-instruct模型进行微调，得到LLama-VL-TUG模型；4. 在小规模真实手绘图像语料库上进行评估（借助人工标注）。

Result: 1. LLama-VL-TUG将Llama 3.2 11B-instruct的ROUGE-L性能提升了2.14倍；2. 在所有基线模型中取得最佳综合性能；3. 在真实图像上，8种图表类型中有7种实现了最低编译错误；4. 将Llama 3.2 11B-instruct的平均F1分数提升了6.97倍。

Conclusion: 通过合成数据训练VLM是解决手绘技术图表理解问题的有效方法，显著提升了模型性能，为实际应用提供了可行解决方案。

Abstract: Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.

</details>


### [66] [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)
*Peng Sun,Xiangyu Zhang,Duan Wu*

Main category: cs.CL

TL;DR: BoRP是一个基于LLM潜在空间几何特性的可扩展满意度评估框架，通过极化指数引导的自动标注生成和偏最小二乘回归映射隐藏状态到连续分数，显著优于生成式基线且大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 开放域对话助手的满意度评估面临挑战：显式反馈稀疏，隐式指标模糊，传统A/B测试缺乏可靠指标。需要一种高保真、可扩展的评估框架来支持迭代开发。

Method: 提出BoRP框架：1）利用LLM潜在空间的几何特性；2）基于极化指数的引导机制自动生成标注规则；3）使用偏最小二乘回归将隐藏状态映射到连续满意度分数。不同于生成式方法，该方法更高效且成本更低。

Result: 在工业数据集上的实验表明，BoRP（基于Qwen3-8B/14B）在与人判断的一致性方面显著优于生成式基线（包括Qwen3-Max）。同时，推理成本降低数个数量级，支持全规模监控和通过CUPED实现高灵敏度A/B测试。

Conclusion: BoRP为开放域对话助手的满意度评估提供了一种高保真、可扩展且成本效益高的解决方案，解决了传统评估方法的局限性，支持高效的迭代开发和A/B测试。

Abstract: Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.

</details>


### [67] [Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue](https://arxiv.org/abs/2601.18281)
*Yuhang Jia,Pei Liu,Haoqin Sun,Jiaming Zhou,Xuxin Cheng,Cao Liu,Ke Zeng,Xunliang Cai,Yong Qin*

Main category: cs.CL

TL;DR: 提出ReEmpathy模型，通过引入描述性自然语言评估模型EmpathyEval和自反思交替推理机制，增强端到端口语语言模型在共情对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前端到端口语语言模型在共情对话中主要依赖刚性监督信号（如监督微调中的真实响应或强化学习中的偏好分数），这些方法无法充分建模复杂的共情，因为不存在单一的"正确"响应，且简单的数值分数无法捕捉情感表达的细微差别或共情行为的适当性。

Method: 1. 引入EmpathyEval：基于描述性自然语言的评估模型，用于评估口语对话中的共情质量；2. 提出ReEmpathy模型：基于EmpathyEval，采用新颖的共情自反思交替推理机制，交替进行口语响应生成和自由形式的共情相关反思推理。

Result: 大量实验表明，ReEmpathy通过启用反思推理，显著改善了共情敏感的口语对话，为实现更具情感智能和共情意识的人机交互提供了有前景的方法。

Conclusion: ReEmpathy模型通过结合描述性评估和自反思推理机制，有效解决了传统方法在建模复杂共情方面的局限性，为端到端口语语言模型的共情能力提升提供了创新解决方案。

Abstract: End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single "correct" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.

</details>


### [68] [Suppressing Final Layer Hidden State Jumps in Transformer Pretraining](https://arxiv.org/abs/2601.18302)
*Keigo Shibata,Kazuki Yano,Ryosuke Takahashi,Jaesung Lee,Wataru Ikeda,Jun Suzuki*

Main category: cs.CL

TL;DR: 该论文研究了Transformer语言模型的内部行为，发现许多预训练模型在中间层输入输出隐藏状态向量的角度距离变化很小，但在最后一层附近会出现不成比例的"跳跃"。作者提出了抑制这种跳跃的正则化方法JREG，并在Llama模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 许多预训练Transformer语言模型在中间层的隐藏状态变化很小，而在最后一层附近出现不成比例的"跳跃"现象。作者认为这种跳跃可能表明模型能力使用不均衡，中间层未能充分利用，而过度依赖最后一层，这可能是一种不良特性。

Method: 首先引入量化最后一层附近跳跃强度的度量指标，并验证其在多个开源模型中的普遍性。然后提出跳跃抑制正则化器(JREG)，在预训练过程中惩罚这种跳跃，鼓励模型在中间层更均衡地使用能力。在三种不同规模的Llama模型上进行实验验证。

Result: JREG方法在多个Llama模型规模上都显示出改进的任务性能，且不改变模型架构。实验表明该方法能够有效抑制最后一层的跳跃现象，促进模型在中间层更均衡地使用能力。

Conclusion: Transformer语言模型在最后一层附近存在不成比例的跳跃现象，这可能表明模型能力使用不均衡。通过JREG正则化方法可以抑制这种跳跃，改善模型性能，为优化Transformer训练提供了新思路。

Abstract: This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.

</details>


### [69] [Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM](https://arxiv.org/abs/2601.18306)
*Everlyn Asiko Chimoto,Mostafa Elhoushi,Bruce A. Bassett*

Main category: cs.CL

TL;DR: 本文系统评估了多语言大语言模型量化中校准集语言选择的影响，发现非英语和多语言校准集相比英语基线能显著降低困惑度，语言对齐是关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法通常使用小型英语校准集，但它们对多语言模型的影响尚未充分探索。量化虽然能减少存储和计算成本，但常导致性能下降，特别是在多语言场景下。

Method: 系统评估了8种校准设置（5种单语言和3种多语言混合）在两种量化器（GPTQ、AWQ）上的表现，使用来自10种语言的数据，在Llama3.1 8B和Qwen2.5 7B模型上进行实验。

Result: 非英语和多语言校准集相比英语基线显著改善困惑度，多语言混合实现最大困惑度降低达3.52点。针对评估语言定制校准集对单个语言改进最大。某些语言-量化器组合出现性能下降，归因于不同语言间激活范围分布的差异。

Conclusion: 静态的"一刀切"校准方法对多语言LLM量化是次优的。定制校准数据（包括语言和多样性）对稳健量化多语言LLM至关重要，语言对齐在量化性能中起关键作用。

Abstract: Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.

</details>


### [70] [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320)
*Jinwei Lu,Yuanfeng Song,Chen Zhang,Raymond Chi-Wing Wong*

Main category: cs.CL

TL;DR: MultiVis-Agent：基于逻辑规则增强的多智能体框架，用于可靠的多模态多场景可视化生成，解决现有系统单模态输入、一次性生成和可靠性问题


<details>
  <summary>Details</summary>
Motivation: 现实世界可视化任务涉及复杂多模态需求，需要参考图像、代码示例和迭代优化。现有系统存在根本限制：单模态输入、一次性生成和僵化工作流程。LLM方法虽有潜力但引入可靠性挑战，包括灾难性故障和无限循环风险。

Method: 提出MultiVis-Agent，一个逻辑规则增强的多智能体框架，采用四层逻辑规则框架为系统可靠性提供数学保证，同时保持灵活性。逻辑规则是指导LLM推理的数学约束而非替代。形式化MultiVis任务涵盖从基础生成到迭代优化的四个场景，并开发包含1000+案例的MultiVis-Bench基准。

Result: 在挑战性任务上达到75.63%的可视化分数，显著优于基线（57.54-62.79%）。任务完成率99.58%，代码执行成功率94.56%（无逻辑规则时为74.48%和65.10%）。成功解决了自动化可视化生成的复杂性和可靠性挑战。

Conclusion: MultiVis-Agent通过逻辑规则增强的多智能体框架有效解决了多模态可视化生成的可靠性和复杂性挑战，在任务完成率、代码执行成功率和可视化质量方面显著优于现有方法。

Abstract: Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.

</details>


### [71] [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350)
*Junyi Zou*

Main category: cs.CL

TL;DR: 该研究针对大语言模型在医学术语精确性和安全关键指令遵循方面的不足，提出了一种两阶段LoRA管道方法，通过加权适配器合并技术平衡指令遵循能力和领域知识保留。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备强大的通用能力，但在医学术语精确性和安全关键指令遵循方面存在不足，这在医疗等安全关键领域尤为关键，需要专门的方法来提升模型在医疗领域的表现。

Method: 采用两阶段LoRA管道：第一阶段通过领域自适应预训练（DAPT）注入广泛的医学知识；第二阶段通过监督微调（SFT）使用指令式数据对齐模型的医学问答行为。提出加权适配器合并技术，在导出合并的基础模型检查点前线性组合SFT和PT适配器。

Result: 在保留的医疗验证集（F5/F6）上，合并模型在实用解码配置下达到BLEU-4=16.38、ROUGE-1=20.42、ROUGE-2=4.60、ROUGE-L=11.54。进一步通过损失曲线和受控解码比较分析了解码敏感性和训练稳定性。

Conclusion: 该研究展示了在安全关键领域通过两阶段适配器训练和加权合并的有效方法，能够平衡指令遵循能力和领域知识保留，为医疗领域的大语言模型应用提供了实用的技术方案。

Abstract: Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.

</details>


### [72] [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352)
*Manjie Xu,Isabella Yin,Xinyi Tu,Chi Zhang,Yixin Zhu*

Main category: cs.CL

TL;DR: 大型语言模型存在"语义惯性"问题，难以抑制预训练先验知识，当上下文规则与先验矛盾时表现更差。通过将动态规则表示为可执行代码而非描述性文本，可以逆转这一趋势并实现有效的先验抑制。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型存在的"语义惯性"问题，即模型难以抑制预训练先验知识（如"岩浆是危险的"），当动态上下文规则与这些先验矛盾时。这种现象在需要动态覆盖学习先验的领域中尤为重要。

Method: 使用Baba Is You游戏作为测试平台，其中物理规则是可变的文本规则。提出Code-Grounded Vistas (LCV)方法，将动态规则表示为可执行代码而非描述性文本，通过微调模型处理反事实对，并识别具有矛盾规则的状态，强制模型关注逻辑约束而非视觉语义。

Result: 发现大型模型可能表现出逆缩放现象：当自然语言推理需要抑制预训练关联时，更大模型的表现反而比小模型更差。将动态表示为可执行代码可以逆转这一趋势，使缩放能够改善上下文推理。LCV方法在效率和准确性上都优于昂贵的推理时搜索方法。

Conclusion: 表示形式从根本上决定了缩放是改善还是损害上下文推理能力。这挑战了"更大模型总是更好"的假设，对需要动态覆盖学习先验的领域具有重要意义。通过将规则编码为可执行代码，可以有效解决语义惯性问题。

Abstract: LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.

</details>


### [73] [Hierarchical Text Classification with LLM-Refined Taxonomies](https://arxiv.org/abs/2601.18375)
*Jonas Golde,Nicolaas Jedema,Ravi Krishnan,Phong Le*

Main category: cs.CL

TL;DR: TaxMorph：使用LLMs重构分类学层次结构以提升层次文本分类性能的框架


<details>
  <summary>Details</summary>
Motivation: 现实世界中的分类学存在模糊性（如相似父节点下的相同叶节点名称），这阻碍了语言模型学习清晰的决策边界，需要改进分类学结构以更好地匹配语言模型的语义编码

Method: 提出TaxMorph框架，使用大语言模型通过重命名、合并、拆分和重新排序等操作重构整个分类学层次结构，使分类学更好地与语言模型的语义编码对齐

Result: 在三个HTC基准测试中，LLM优化的分类学在各种设置下始终优于人工策划的分类学，F1分数最高提升+2.9个百分点。分析发现，虽然人工分类学在嵌入空间中形成更易分离的簇，但LLM优化的分类学更贴近模型在分类过程中的实际混淆模式

Conclusion: LLM引导的分类学优化能够创建更符合模型学习方式的分类学结构，通过更好地反映模型的归纳偏差来提升层次文本分类性能，这表明分类学设计应考虑与模型学习机制的兼容性

Abstract: Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.

</details>


### [74] [Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction](https://arxiv.org/abs/2601.18395)
*Mikel Zubillaga,Oscar Sainz,Oier Lopez de Lacalle,Eneko Agirre*

Main category: cs.CL

TL;DR: ThinkTwice框架通过采样生成多个候选模板并使用选择模块选取最佳方案，显著提升文档级信息抽取性能，超越贪婪解码方法


<details>
  <summary>Details</summary>
Motivation: 传统文档级信息抽取使用贪婪解码以避免输出变异性，但作者认为这种变异性可以被利用来获得更好的解决方案，特别是使用推理模型时

Method: 提出ThinkTwice框架：1) LLM为给定文档生成多个候选模板；2) 选择模块选取最合适的模板。包含无监督方法（利用生成输出间的一致性）和监督方法（使用在标注数据上训练的奖励模型）。为解决DocIE中黄金推理轨迹稀缺问题，提出基于拒绝采样的方法生成包含输出模板和推理轨迹的银训练数据

Result: 实验证明无监督和监督ThinkTwice方法的有效性，一致优于贪婪基线和当前最先进方法

Conclusion: 采样方法能够产生比贪婪解码更好的解决方案，ThinkTwice框架通过利用输出变异性显著提升了文档级信息抽取的性能

Abstract: Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.

</details>


### [75] [Pisets: A Robust Speech Recognition System for Lectures and Interviews](https://arxiv.org/abs/2601.18415)
*Ivan Bondarenko,Daniil Grebenkin,Oleg Sedukhin,Mikhail Klementev,Roman Derunets,Lyudmila Budneva*

Main category: cs.CL

TL;DR: 提出基于三组件架构的语音转文本系统"Pisets"，通过Wav2Vec2、AST和Whisper的组合提升俄语语音识别精度，减少幻觉错误，优于WhisperX和标准Whisper模型


<details>
  <summary>Details</summary>
Motivation: 解决Whisper模型在语音识别中存在的错误和幻觉问题，特别是针对俄语科学和新闻领域的长音频转录需求，提高在不同声学条件下的鲁棒性

Method: 采用三组件架构：1) Wav2Vec2进行初步识别；2) Audio Spectrogram Transformer (AST)进行误报过滤；3) Whisper进行最终语音识别。结合课程学习方法和多样化的俄语语音语料库，引入先进的不确定性建模技术

Result: 系统在长音频数据转录方面表现出色，相比WhisperX和标准Whisper模型，在各种声学条件下具有更好的鲁棒性和准确性，减少了错误和幻觉现象

Conclusion: 提出的三组件架构结合课程学习和不确定性建模，有效提升了俄语语音识别系统的性能，为科学家和记者提供了高质量的语音转文本工具，代码已开源

Abstract: This work presents a speech-to-text system "Pisets" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of "Pisets" system is publicly available at GitHub: https://github.com/bond005/pisets.

</details>


### [76] [Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.18468)
*Daniel B. Hier,Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: 研究显示大语言模型在预训练后存储生物医学事实的强度不均，通过微调Llama 3.1 8B学习本体术语映射，发现潜在知识是事实获取速度和有限泛化的最强预测因子


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在预训练后存储生物医学事实的不均匀性，探索潜在知识对微调过程中事实获取、泛化和退化的影响机制

Method: 微调Llama 3.1 8B Instruct模型学习人类表型本体(800对)和基因本体(400训练对)的术语标识符映射，保留400个GO对测试泛化能力；将学习视为时间事件过程，使用随机解码检测基线潜在知识，应用Cox比例风险模型识别获取、泛化和退化的预测因子

Result: HPO的基线确定性召回率为2.8%，微调后提升至71.9%；潜在知识是事实获取速度的最强预测因子(HR 2.6)，与更早、更高的峰值学习率和更快收敛相关；泛化到保留GO事实的情况较少(5.8%)，但潜在知识存在时更可能发生；未见过术语的正确映射比训练过术语更容易退化

Conclusion: 潜在知识预测微调期间事实学习的速度和未见本体事实的有限泛化，而抵抗退化取决于事实是否在训练中得到强化

Abstract: Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.

</details>


### [77] [Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs](https://arxiv.org/abs/2601.18483)
*Arya Labroo,Ivaxi Sheth,Vyas Raina,Amaani Ahmed,Mario Fritz*

Main category: cs.CL

TL;DR: 论文提出评估框架，发现LLMs在多概念控制中表现下降，揭示基于提示的控制的组合性限制


<details>
  <summary>Details</summary>
Motivation: 大型语言模型具有强大的生成能力，但许多应用需要对特定文本概念（如幽默、说服力、正式性）进行精细控制。现有方法只能提供粗略或单属性控制，缺乏对多属性设置的系统评估。

Method: 引入一个评估框架，用于评估单概念和双概念场景下的精细可控性，重点关注语言学上不同的概念对（如说服力vs幽默）。

Result: 在多个LLM和生成任务中，双概念设置下的性能通常会下降，即使所选概念在原则上是可分离的。这揭示了基于提示的朴素控制的基本限制：即使概念在直觉上是独立的，模型也难以处理组合性。

Conclusion: 该框架为多概念控制能力的系统评估提供了原则性方法，揭示了当前基于提示的控制方法在组合性方面的根本局限性，为未来改进方法提供了基准。

Abstract: Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.

</details>


### [78] [Demographic Probing of Large Language Models Lacks Construct Validity](https://arxiv.org/abs/2601.18486)
*Manuel Tonneau,Neil K. R. Seghal,Niyati Malhotra,Victor Orozco-Olvera,Ana María Muñoz Boudet,Lakshmi Subramanian,Sharath Chandra Guntuku,Valentin Hofmann*

Main category: cs.CL

TL;DR: 本文质疑人口统计学探测方法的建构效度，发现不同人口线索（如姓名、方言）在LLMs中引发不一致的行为变化，导致估计的群体差异不稳定，建议使用多种生态效度高的线索并控制混淆因素。


<details>
  <summary>Details</summary>
Motivation: 当前人口统计学探测研究通常使用单一人口线索（如姓名或方言）来代表群体身份，隐含假设这些线索具有强建构效度——即它们是同一潜在人口条件行为的可互换操作化。本文旨在检验这一假设在现实建议寻求互动中的有效性。

Method: 在现实建议寻求互动中测试人口统计学探测的建构效度，聚焦美国语境下的种族和性别。使用不同的人口线索（如姓名、方言等）作为群体身份信号，分析这些线索如何影响LLMs的行为变化，并探究不一致性的来源。

Result: 1. 意图代表同一人口群体的线索仅引发部分重叠的模型行为变化；2. 同一线索内不同群体间的区分度弱且不均匀；3. 估计的群体差异不稳定，其大小和方向随线索变化；4. 不一致性部分源于线索编码人口属性的强度差异以及独立影响模型行为的语言混淆因素。

Conclusion: 人口统计学探测缺乏建构效度：它无法产生关于LLMs如何基于人口信息进行条件化的单一稳定表征，这可能反映了建构的错误设定或碎片化。建议使用多种生态效度高的线索并明确控制混淆因素，以支持关于LLMs中人口效应的更可靠主张。

Abstract: Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.

</details>


### [79] [Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research](https://arxiv.org/abs/2601.18512)
*Antonio Garzon-Vico,Krithika Sharon Komalapati,Arsalan Shahid,Jan Rosier*

Main category: cs.CL

TL;DR: 该研究提出了一个使用大语言模型创建真实高管虚拟人格的方法框架，通过CEO沟通数据和道德基础理论构建模拟领导者决策的LLM参与者，验证了其在组织研究中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在难以直接接触高管的情况下，需要开发能够模拟真实领导者决策的工具来支持组织研究。现有方法在捕捉个体领导者特质方面存在局限，因此需要创建可信的虚拟人格来补充传统研究方法。

Method: 基于真实CEO沟通数据和道德基础理论，构建LLM驱动的虚拟CEO人格。研究分为三个阶段：结构效度评估、可靠性测试和行为保真度验证，通过将虚拟CEO与人类参与者进行基准比较来评估其性能。

Result: 理论框架支撑的虚拟人格能够近似人类样本中观察到的道德判断，表明LLM基础的人格可以作为组织研究中可信且互补的工具，特别是在难以直接接触高管的情况下。

Conclusion: LLM基础的虚拟人格为组织研究提供了新的方法论工具，特别是在高管可及性有限的情况下。研究为未来在组织环境中使用LLM人格的研究提供了框架和启示。

Abstract: This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.

</details>


### [80] [GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback](https://arxiv.org/abs/2601.18517)
*James Sungarda,Hongkai Liu,Zilong Zhou,Tien-Hsuan Wu,Johnson Chun-Sing Cheung,Ben Kao*

Main category: cs.CL

TL;DR: SWITCH是一个社会工作交互式培训聊天机器人，通过模拟真实客户、实时咨询技能分类和动机访谈进展系统，为社工学生提供可扩展、低成本、一致的培训解决方案。


<details>
  <summary>Details</summary>
Motivation: 社会工作实地教育是核心教学方法，但传统培训受限于教师可用性和咨询客户资源，难以及时提供客观反馈。需要一种可扩展、低成本的替代方案来补充实地教育。

Method: SWITCH采用认知基础客户模型（包含静态和动态字段），结合实时咨询技能分类模块和动机访谈进展控制系统。技能分类采用基于上下文的检索学习和微调BERT多标签分类器两种方法。

Result: 实验表明，BERT方法和上下文学习方法都显著优于基线模型，验证了SWITCH系统的有效性。系统能够提供可扩展、低成本、一致的培训工作流程。

Conclusion: SWITCH作为社会工作交互式培训聊天机器人，能够补充实地教育，让督导专注于更高层次的指导，为社工培训提供了创新的技术解决方案。

Abstract: Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.

</details>


### [81] [Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models](https://arxiv.org/abs/2601.18527)
*Francesco Maria Molfese,Momchil Hardalov,Rexhina Blloshmi,Bill Byrne,Adrià de Gispert*

Main category: cs.CL

TL;DR: 研究探讨了长上下文语言模型（LCLMs）的微调策略如何提升其在长上下文任务中的性能，以及这些策略对KV缓存压缩技术鲁棒性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文语言模型能够处理数百万token的上下文窗口，它们可以作为传统检索增强生成（RAG）的替代方案。然而，尚不清楚微调策略是否能提升长上下文性能，以及这些改进是否能转化为在KV缓存压缩技术下更强的鲁棒性。

Method: 研究调查了哪些训练策略能最有效地增强LCLMs识别和使用相关信息的能力，以及提升它们在KV缓存压缩下的鲁棒性。通过实验比较不同微调方法的效果。

Result: 实验显示：1）在领域内任务上取得显著改进，相比基础模型提升高达+20分；2）领域外泛化能力因任务而异，LCLMs在金融问题上表现优异（+9分），而RAG在多项选择题上优于基线模型（+6分）；3）微调方法在KV缓存压缩下带来适度的鲁棒性提升，但增益因任务而异。

Conclusion: 微调策略能有效提升LCLMs在长上下文任务中的性能，特别是在领域内任务上。然而，领域外泛化能力具有任务依赖性，且微调对KV缓存压缩鲁棒性的改善有限且不稳定。

Abstract: With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.

</details>


### [82] [From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation](https://arxiv.org/abs/2601.18533)
*Yuxin Jiang,Yufei Wang,Qiyuan Zhang,Xingshan Zeng,Liangyou Li,Jierun Chen,Chaofan Tao,Haoli Bai,Lifeng Shang*

Main category: cs.CL

TL;DR: RLVRR提出了一种基于可验证参考奖励的强化学习方法，通过从高质量参考中提取有序语言信号（奖励链），将奖励分解为内容和风格两个维度，解决了传统RLVR在开放式生成任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于可验证奖励的强化学习（RLVR）在推理任务（如数学和代码）中有效，但在开放式生成任务中存在挑战，因为缺乏明确的地面真值。单一监督信号常导致效率低下和奖励黑客问题，需要一种能结合RL探索能力和监督微调效率的方法。

Method: RLVRR从高质量参考中提取有序语言信号（奖励链），将奖励分解为两个维度：1）内容维度：保留确定性核心概念（如关键词）；2）风格维度：通过基于LLM的验证评估对风格属性的遵循程度。该方法结合了RL的探索能力和监督微调（SFT）的效率与可靠性。

Result: 在超过10个基准测试中使用Qwen和Llama模型进行广泛实验，结果表明：1）RLVRR显著优于使用十倍数据训练的高级奖励模型的SFT；2）统一了结构化推理和开放式生成的训练；3）在保持输出多样性的同时具有更好的泛化能力。

Conclusion: RLVRR为通用LLM对齐提供了一种原则性且高效的可验证强化学习路径，通过基于参考的奖励分解方法，有效解决了开放式生成任务中的监督信号问题，实现了RL探索能力与SFT效率的平衡。

Abstract: Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.

</details>


### [83] [Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features](https://arxiv.org/abs/2601.18536)
*Abishek Stephen,Jindřich Libovický*

Main category: cs.CL

TL;DR: 提出一种基于形态句法特征评估子词切分形态合理性的新指标，无需黄金切分数据，适用于更多语言


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如语素边界F值）需要黄金切分数据，但这类数据在许多语言中不可得或质量不一致。需要一种更广泛适用的评估方法，利用更易获得的形态句法特征资源。

Method: 利用Universal Dependencies或UniMorph等资源中的形态句法特征，通过IBM Model 1概率性地对齐子词与形态特征，从而评估子词切分的形态合理性。

Result: 实验表明，该指标与传统语素边界召回率有良好相关性，同时在不同形态系统的语言中具有更广泛的适用性。

Conclusion: 提出的新指标为子词切分评估提供了一种更实用、更广泛适用的方法，特别适合资源匮乏的语言，能够有效评估切分结果的形态合理性。

Abstract: We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.

</details>


### [84] [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)
*Devansh Srivastav,David Pape,Lea Schönherr*

Main category: cs.CL

TL;DR: 该论文系统分析了LLM中的"隐藏意图"问题，提出了十类隐藏意图的分类法，展示了检测方法在开放世界设置中的失效，并通过案例研究证实了这些意图在现有LLM中的存在。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地嵌入日常决策，但其输出可能编码微妙的、非预期的行为，这些行为会影响用户信念和行动。这些隐蔽的、目标导向的行为被称为"隐藏意图"，可能源于训练和优化伪影，或被对抗性开发者故意诱导，但在实践中难以检测。

Method: 1. 提出了基于社会科学研究的十类隐藏意图分类法，按意图、机制、上下文和影响组织；2. 展示了如何在受控模型中轻松诱导隐藏意图；3. 系统评估了检测方法，包括推理和非推理LLM判断器；4. 在开放世界设置下进行压力测试，分析精度-流行率和精度-FNR权衡；5. 通过定性案例研究验证十类意图在已部署的SOTA LLM中的存在。

Result: 1. 检测方法在现实开放世界设置中失效，特别是在低流行率条件下，假阳性压倒精度，假阴性掩盖真实风险；2. 压力测试显示，没有极小的假阳性率或对操纵类型的强先验，审计就会失败；3. 案例研究证实所有十类隐藏意图都在已部署的SOTA LLM中显现；4. 提供了第一个在开放世界设置下系统分析LLM隐藏意图可检测性失败的工作。

Conclusion: LLM中的隐藏意图是一个紧迫问题，现有检测方法在开放世界设置中严重失效。需要建立强大的框架来理解、诱导和压力测试这些行为，建立灵活的分类法来预测不断演变的威胁并指导治理。

Abstract: LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.

</details>


### [85] [One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization](https://arxiv.org/abs/2601.18572)
*Franziska Weeber,Vera Neplenbroek,Jan Batzner,Sebastian Padó*

Main category: cs.CL

TL;DR: 研究比较了六种常用人物提示线索在七个LLM上的表现，发现不同线索会产生显著差异，建议未来个性化研究应评估多种外部有效的线索


<details>
  <summary>Details</summary>
Motivation: LLM的个性化虽然能改善用户体验，但可能引入或放大群体偏见。先前研究依赖单一人物提示线索（如用户名或显式属性），忽视了LLM对提示变化的敏感性和某些线索在真实交互中的罕见性

Method: 比较六种常用人物提示线索在七个开源和专有LLM上的表现，涵盖四个写作和建议任务，分析线索间的相关性和响应方差

Result: 虽然线索整体高度相关，但不同线索在不同人物设定下会产生显著响应方差，单一人物提示线索的结论可能不可靠

Conclusion: 应谨慎基于单一人物提示线索做出结论，建议未来个性化研究评估多种外部有效的线索以提高鲁棒性和外部效度

Abstract: Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.

</details>


### [86] [From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection](https://arxiv.org/abs/2601.18582)
*Yuan Cao,Feixiang Liu,Xinyue Wang,Yihan Zhu,Hui Xu,Zheng Wang,Qiang Qiu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于强化学习的性格检测新方法，将性格检测视为排序任务而非分类任务，通过监督微调和分组相对策略优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的性格检测方法存在两个主要问题：1) 人类性格的复杂性和特质间的细微差异使得准确分类困难；2) 基于提示的方法过度依赖专家知识，缺乏自主模式学习能力。需要一种能更好处理性格评估主观性和模糊边界的新方法。

Method: 提出两阶段方法：1) 使用监督微调(SFT)建立性格特质排序能力并标准化输出格式，提供稳健初始化；2) 引入分组相对策略优化(GRPO)和专门的基于排序的奖励函数，训练LLM学习最优答案排序而非简单分类。

Result: 在多个性格检测基准测试中实现了最先进的性能，证明了该方法在性格评估任务中的有效性。

Conclusion: 将性格检测重新定义为排序任务并结合强化学习训练范式，能够更好地处理性格评估的主观性和模糊边界，相比传统分类方法具有显著优势。

Abstract: Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.

</details>


### [87] [Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning](https://arxiv.org/abs/2601.18722)
*Lintang Sutawika,Gokul Swamy,Zhiwei Steven Wu,Graham Neubig*

Main category: cs.CL

TL;DR: SP3F是一个两阶段框架，通过自我对弈和特权成对反馈，无需目标语言数据即可提升多语言推理能力


<details>
  <summary>Details</summary>
Motivation: 当前推理大语言模型在训练数据中较少见的语言上表现显著低于英语，需要一种无需目标语言数据的方法来提升多语言推理能力

Method: 两阶段框架：1) 在翻译的英文问答对上监督微调提升基础模型正确性；2) 通过特权成对法官进行自我对弈强化学习，法官可获得英文参考答案作为特权信息

Result: SP3F显著提升基础模型性能，在多个数学和非数学任务上甚至优于完全后训练的模型，且训练数据量不到1%

Conclusion: SP3F框架通过自我对弈和特权反馈机制，无需目标语言数据即可有效提升多语言推理能力，在单语言、多语言和未见语言泛化设置中均表现优异

Abstract: When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than
  of the training data across the single-language, multilingual, and generalization to unseen language settings.

</details>


### [88] [HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences](https://arxiv.org/abs/2601.18724)
*Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 该研究系统调查了AI生成论文中幻觉引用（HalluCitation）的普遍性和影响，发现ACL、NAACL和EMNLP会议2024-2025年发表的近300篇论文包含至少一个幻觉引用，且问题在EMNLP 2025年急剧增加。


<details>
  <summary>Details</summary>
Motivation: 近年来在审稿、预印本和已发表论文中频繁观察到幻觉引用（引用不存在的文献），这严重威胁科学可靠性，当这些引用出现在已接受论文中时，还会损害会议信誉。

Method: 分析了ACL、NAACL和EMNLP在2024年和2025年发表的所有论文，包括主会议论文、Findings论文和研讨会论文，系统识别和统计幻觉引用。

Result: 发现近300篇论文包含至少一个幻觉引用，其中大部分发表于2025年；特别值得注意的是，半数问题论文出现在EMNLP 2025（最近的会议），表明问题正在迅速恶化；超过100篇这样的论文在EMNLP 2025被接受为主会议和Findings论文。

Conclusion: 幻觉引用在计算语言学领域已构成严重且日益增长的问题，特别是在EMNLP 2025会议上表现尤为突出，这对科学可靠性和会议信誉造成了实质性影响。

Abstract: Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.

</details>


### [89] [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)
*Henry Bell,Caroline Zhang,Mohammed Mobasserul Haque,Dhaval Potdar,Samia Zaman,Brandon Fain*

Main category: cs.CL

TL;DR: REFLECT：一种无需训练或数据的推理时宪法对齐框架，通过上下文中的自我评估、自我批判和最终修订来对齐语言模型与自然语言原则


<details>
  <summary>Details</summary>
Motivation: 现有的宪法对齐方法（如RLHF）需要大量计算资源、精心工程调整和难以获取的人工标注数据，因此需要一种更轻量、即插即用的对齐方法

Method: REFLECT是完全基于上下文的推理时框架，包含三个核心步骤：(i) 基于宪法的基本响应生成，(ii) 后生成自我评估，(iii)(a) 自我批判和(iii)(b) 最终修订，通过显式的原则推理提供透明推理轨迹

Result: REFLECT显著提高了LLM对多样复杂原则的符合度，包括与原始参数微调强调的原则完全不同的原则，同时不牺牲事实推理能力；特别有效减少罕见但严重的原则违反，提高安全性和鲁棒性；还能自然生成用于传统参数微调的有用训练数据

Conclusion: REFLECT提供了一种无需训练或数据的即插即用宪法对齐方法，在推理时通过显式原则推理实现更好的对齐效果，同时为长期部署中的高效扩展和推理计算开销减少提供了可能

Abstract: The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.

</details>


### [90] [One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment](https://arxiv.org/abs/2601.18731)
*Hongru Cai,Yongqi Li,Tiezheng Yu,Fengbin Zhu,Wenjie Wang,Fuli Feng,Wenjie Li*

Main category: cs.CL

TL;DR: 提出元奖励建模（MRM）框架，通过元学习解决个性化奖励建模中的数据稀缺和快速适应新用户问题，引入鲁棒个性化目标（RPO）提升模型对难学习用户的适应性。


<details>
  <summary>Details</summary>
Motivation: 个性化对齐LLMs需要个性化奖励模型，但面临两大挑战：个体用户反馈稀缺和需要高效适应未见用户。传统方法直接拟合数据学习用户偏好存在局限性，需要转向学习偏好适应过程。

Method: 提出元奖励建模（MRM），将个性化奖励建模重构为元学习问题。将每个用户的奖励模型表示为基奖励函数的加权组合，使用MAML风格框架优化权重初始化以支持有限反馈下的快速适应。引入鲁棒个性化目标（RPO），在元优化中更重视难学习用户。

Result: 在个性化偏好数据集上的广泛实验验证：MRM增强了少样本个性化能力，提高了用户鲁棒性，并持续优于基线方法。

Conclusion: MRM通过元学习范式有效解决了个性化奖励建模中的数据稀缺和快速适应问题，RPO进一步提升了模型对难学习用户的鲁棒性，为LLMs的个性化对齐提供了有效解决方案。

Abstract: Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.

</details>


### [91] [Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings](https://arxiv.org/abs/2601.18788)
*Mumin Jia,Jairo Diaz-Rodriguez*

Main category: cs.CL

TL;DR: Embed-KCPD：一种无监督文本分割方法，使用句子嵌入和惩罚KCPD目标检测边界，具有理论保证和模拟验证


<details>
  <summary>Details</summary>
Motivation: 边界标注成本高、主观性强，且难以跨领域和粒度迁移，需要有效的无监督文本分割方法

Method: 将句子表示为嵌入向量，通过最小化惩罚KCPD目标估计边界；提出m-依赖序列理论，开发LLM模拟框架验证

Result: 在标准分割基准上常优于强无监督基线；理论证明oracle不等式和定位保证；模拟验证预测的缩放行为

Conclusion: Embed-KCPD结合了理论保证、模拟可靠性和实际有效性，为无监督文本分割提供了有前景的解决方案

Abstract: Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.

</details>


### [92] [MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts](https://arxiv.org/abs/2601.18790)
*Etienne Lanzeray,Stephane Meilliez,Malo Ruelle,Damien Sileo*

Main category: cs.CL

TL;DR: 研究发现专门优化的推理模型在用户面临生命危险时仍坚持完成数学任务，忽视安全警告，而通用模型能优先处理紧急情况


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在深度推理优化过程中是否形成了"隧道视野"，即在追求正确执行复杂任务时忽视了安全考虑，特别是在用户描述生命威胁紧急情况时

Method: 引入MortalMATH基准测试，包含150个场景，用户请求代数帮助的同时描述日益严重的生命威胁紧急情况（如中风症状、自由落体），评估不同模型的行为反应

Result: 发现明显的行为分化：通用模型（如Llama-3.1）成功拒绝数学任务以处理危险；而专门推理模型（如Qwen-3-32b和GPT-5-nano）常完全忽视紧急情况，保持超过95%的任务完成率，即使用户描述濒死状态。推理计算时间引入危险延迟：最多15秒后才可能提供帮助

Conclusion: 训练模型不懈追求正确答案可能无意中使其丧失了安全部署所需的生存本能，需要在模型优化中平衡推理能力与安全响应

Abstract: Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a "tunnel vision" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.

</details>


### [93] [Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets](https://arxiv.org/abs/2601.18791)
*Iaroslav Chelombitko,Mika Hämäläinen,Aleksey Komissarov*

Main category: cs.CL

TL;DR: 该研究使用BPE子词方法对242种拉丁和西里尔文字语言进行大规模比较分析，发现BPE分割与语素边界高度一致，词汇相似性与语言遗传关系显著相关，跨语言同形词分析揭示了语言距离对分割差异的影响。


<details>
  <summary>Details</summary>
Motivation: 需要在大规模跨语言比较中建立统一的分析框架，量化研究拉丁和西里尔文字语言之间的词汇模式、语言相似性和遗传关系，填补宏观语言学定量分析的空白。

Method: 从维基百科词典构建"glottosets"语料集，采用基于字节对编码（BPE）的子词方法，使用基于排名的子词向量分析词汇重叠、词汇分化和语言相似性，涵盖242种语言的大规模比较。

Result: BPE分割与语素边界的一致性比随机基线高95%（F1=0.34 vs 0.15）；BPE词汇相似性与语言遗传关系显著相关（Mantel r=0.329, p<0.001）；罗曼语族形成最紧密聚类（平均距离0.51）；跨语系语言对明显分离（0.82）；26,939个跨语言同形词中48.7%在不同相关语言中获得不同分割，变异与系统发育距离相关。

Conclusion: 研究证明了基于BPE的子词方法在宏观语言学分析中的有效性，为跨类型多样语言的词汇模式提供了统一的定量分析框架，揭示了语言相似性、遗传关系和词汇分割之间的系统性联系。

Abstract: We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.

</details>


### [94] [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/abs/2601.18796)
*Brian Ondov,Chia-Hsuan Chang,Yujia Zhou,Mauro Giuffrè,Hua Xu*

Main category: cs.CL

TL;DR: 提出ctELM模型，通过Embedding Language Model方法将大语言模型与临床试验嵌入空间对齐，实现从嵌入向量生成临床试验描述和比较，并展示沿年龄和性别概念向量移动嵌入能生成响应性试验摘要。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入已成为多种语言应用的核心组件，但解释、探索和反转嵌入空间的方法有限，这降低了透明度并阻碍了潜在有价值的生成应用。特别是在临床试验领域，需要更好的方法来理解和操作嵌入表示。

Method: 采用Embedding Language Model方法将大语言模型与临床试验嵌入对齐。开发了开源、领域无关的ELM架构和训练框架，设计了针对临床试验的训练任务，并引入了专家验证的合成数据集。训练了一系列ELM模型探索任务和训练机制的影响。

Result: 最终模型ctELM能够仅从嵌入向量准确描述和比较未见过的临床试验，并能从新颖向量生成合理的临床试验。生成的试验摘要能够响应沿年龄和性别概念向量移动嵌入的操作。

Conclusion: 公开的ELM实现和实验结果将有助于在生物医学领域及其他领域将大语言模型与嵌入空间对齐，提高嵌入空间的透明度和生成能力。

Abstract: Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [95] [Online parameter estimation for the Crazyflie quadcopter through an EM algorithm](https://arxiv.org/abs/2601.17009)
*Yanhua Zhao*

Main category: cs.AI

TL;DR: 该论文研究了在四旋翼无人机系统中添加随机噪声的影响，使用扩展卡尔曼滤波进行状态估计，基于SDE系统实现线性二次高斯控制器，并应用期望最大化算法进行参数估计，比较了离线和在线参数估计的性能。


<details>
  <summary>Details</summary>
Motivation: 无人机因其尺寸小、成本低、可靠性高且配备多种传感器，在救援、摄影、农业和运输等领域应用广泛。地震等灾害会破坏基础设施，阻碍救援人员进入，而无人机可以到达人类难以进入的区域。本文旨在研究随机噪声对四旋翼无人机系统的影响，并开发有效的状态估计和控制方法。

Method: 1. 在四旋翼无人机系统中添加随机噪声；2. 使用扩展卡尔曼滤波基于传感器的噪声观测进行状态估计；3. 基于随机微分方程系统实现线性二次高斯控制器；4. 应用期望最大化算法进行四旋翼无人机的参数估计；5. 比较离线和在线参数估计方法。

Result: 研究结果表明，在线参数估计的收敛值范围略大于离线参数估计。扩展卡尔曼滤波能有效处理噪声观测，线性二次高斯控制器在随机噪声环境下表现良好，期望最大化算法成功实现了参数估计。

Conclusion: 该研究证明了在四旋翼无人机系统中处理随机噪声的有效方法，扩展卡尔曼滤波和线性二次高斯控制器的结合为无人机在噪声环境下的稳定运行提供了解决方案。在线参数估计相比离线方法具有更广的收敛范围，为无人机系统的自适应控制提供了更好的基础。

Abstract: Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.

</details>


### [96] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: 论文评估现有可解释性方法在智能体系统中的应用局限，提出需开发专门针对智能体系统特性的新可解释性技术，以保障AI系统的安全部署。


<details>
  <summary>Details</summary>
Motivation: 智能体系统与传统机器学习模型在架构和部署上存在根本差异，引入了独特的安全挑战（如目标错位、决策错误累积、多智能体协调风险），而现有主要为静态模型设计的可解释性技术存在局限性，需要专门针对智能体系统开发新的可解释性方法。

Method: 评估现有可解释性方法在智能体系统背景下的适用性和局限性，识别其在提供智能体决策洞察方面的能力差距，并提出未来开发专门针对智能体系统的可解释性技术方向。

Result: 发现现有可解释性方法在应用于智能体系统时存在显著局限，无法充分应对智能体系统的时序动态性、决策累积效应和上下文依赖行为等特性，需要新的分析方法。

Conclusion: 需要开发专门针对智能体系统设计的可解释性技术，在智能体生命周期的各个阶段（目标形成、环境交互、结果评估）嵌入可解释性机制，这对于确保智能体AI系统的安全和可问责部署至关重要。

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [97] [High-Fidelity Longitudinal Patient Simulation Using Real-World Data](https://arxiv.org/abs/2601.17310)
*Yu Akagi,Tomohisa Seki,Hiromasa Ito,Toru Takiguchi,Kazuhiko Ohe,Yoshimasa Kawazoe*

Main category: cs.AI

TL;DR: 利用真实世界临床记录构建生成式模拟器模型，能够基于患者历史生成高保真的未来临床轨迹，为个性化治疗规划和虚拟临床试验提供支持。


<details>
  <summary>Details</summary>
Motivation: 临床医学中模拟患者轨迹具有变革性潜力，可用于个性化治疗规划和虚拟临床试验。然而，由于复杂的生物和社会文化影响，模拟患者轨迹具有挑战性。本研究旨在利用真实世界临床记录经验性地建模患者时间线。

Method: 开发了一个生成式模拟器模型，以患者历史为输入，合成细粒度、真实的未来轨迹。该模型在超过2亿条临床记录上进行预训练。

Result: 模型生成了高保真的未来时间线，与真实患者未来数据中的事件发生率、实验室检测结果和时间动态密切匹配。准确估计了未来事件概率，在不同结果和时间范围内观察到与预期比率始终接近1.0。

Conclusion: 研究揭示了电子健康记录中真实世界数据的未开发价值，并引入了一个可扩展的临床护理计算机模拟框架。

Abstract: Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.

</details>


### [98] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 该论文提出了一个可校准的理论框架，用于预测多智能体系统在有限推理预算下的性能表现，识别出帮助、饱和和崩溃三种机制，并揭示了上下文窗口、通信损失和智能体相关性等约束条件如何影响系统性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在固定推理预算下常常表现出帮助、饱和甚至崩溃的现象，现有理论缺乏对这些现象的定量预测能力。研究者旨在开发一个最小化且可校准的理论框架，从现代智能体堆栈的三个关键约束（有限上下文窗口、有损通信、相似智能体间的共享故障）出发，预测多智能体系统的性能表现。

Method: 提出一个理论框架，将每个叶子智能体用计算-性能缩放指数β描述，通信用消息长度保真度曲线γ(m)描述，相关性用有效共享误差相关系数ρ描述，上下文窗口W施加硬性扇入限制。通过分析二进制成功/失败任务中的多数聚合，证明了深度b叉树在相关输入和有损通信下的尖锐相变。推导了组织指数s，并给出了计算分配规则和预算阈值。

Result: 理论分析表明：单个标量αρ（结合γ(m)、ρ和扇入b）决定了弱信号是被放大到非平凡固定点还是被洗牌到随机水平。在放大机制中，当s>β时出现预算协同效应（即优于相同总预算下的最佳单个智能体）。通过混合深度表征饱和现象，并提供了保守的裁剪预测器。连续性能热身给出了星型、链式和树形组织的闭式风险公式。

Conclusion: 该研究提供了一个统一的理论框架，能够预测多智能体系统在有限预算下的性能表现，揭示了上下文窗口、通信损失和智能体相关性之间的核心设计权衡。理论预测在合成模拟中得到验证，并能解释最近大规模匹配预算研究中报告的大语言模型智能体系统缩放的主要瓶颈。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [99] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge是一个低成本的形式化数学数据合成框架，通过任务分解和解耦提取策略提高数据生成效率，相比基线将验证率从8.6%提升到12.6%，每个成功轨迹成本仅0.481美元。


<details>
  <summary>Details</summary>
Motivation: 形式化数学中智能体工作流的高成本阻碍了大规模数据合成，加剧了开源语料库的稀缺性。需要一种成本效益高的方法来生成高质量的形式化数学数据。

Method: 将形式化过程分解为五个子任务：陈述形式化、证明生成、前提选择、证明修正和证明草图。采用解耦提取策略从全局失败的轨迹中恢复有效训练信号，有效利用浪费的计算资源。

Result: 在2000个问题的基准测试中，TheoremForge达到12.6%的验证率，超过8.6%的基线，每个成功轨迹平均成本仅0.481美元（使用Gemini-3-Flash）。解耦提取策略使证明生成的数据产量相比标准过滤提高1.6倍。

Conclusion: TheoremForge为构建数据飞轮以训练未来专家模型提供了一个可扩展的框架，能够有效解决形式化数学数据稀缺问题。

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [100] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该论文通过公理化框架证明AGI无法获得独立于分布的统一定义，缺乏通用鲁棒性，存在有界迁移限制，且无法通过可计算程序（包括自我认证）进行完备验证，从而质疑AGI的绝对存在性声明。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能通用智能（AGI）是否具有支持绝对存在性、鲁棒性或自我验证的连贯理论定义，质疑当前AGI声明的理论基础。

Method: 将AGI公理化为分布性、资源受限的语义谓词，通过任务族、任务分布、性能函数和显式资源预算进行索引，在此基础上推导四类形式化结果。

Result: 1. 通用性本质上是关系性的，不存在独立于分布的AGI概念；2. 任务分布的微小扰动可通过悬崖集使AGI属性失效；3. 有限资源下无法实现跨任务族的无限泛化；4. AGI作为非平凡语义属性无法通过任何可计算程序（包括自我认证）进行完备验证。

Conclusion: 强分布独立的AGI声明在没有显式形式化索引的情况下是未定义的，AI的经验进展并不意味着可实现自我认证的通用智能，依赖内部自我认证的递归自我改进方案存在根本缺陷。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [101] [Are We Evaluating the Edit Locality of LLM Model Editing Properly?](https://arxiv.org/abs/2601.17343)
*Wei Liu,Haomei Xu,Hongkai Liu,Zhiying Deng,Ruixuan Li,Heng Huang,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 论文指出现有模型编辑特异性评估协议存在不足，提出了新的评估协议，该协议能更敏感地反映特异性正则化强度变化，并更好地区分不同方法的知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 模型编辑需要平衡编辑效果（成功注入目标知识）和特异性（保留现有非目标知识），但现有特异性评估协议存在不足，无法有效评估这一平衡。

Method: 系统分析了现有特异性评估协议的三个基本问题，实证表明现有指标与特异性正则化强度相关性弱且敏感性不足。提出了新的评估协议，消除了开放域LLM与确定答案假设的冲突，避免了查询无关的流畅性偏差，并能在接近连续的空间中平滑调整评估严格度。

Result: 在不同LLM、数据集和编辑方法上的实验表明，基于新协议的指标对特异性正则化强度变化更敏感，与正则化强度强相关，能更精细地区分不同方法的知识保留能力。

Conclusion: 现有模型编辑特异性评估存在严重缺陷，提出的新评估协议能更准确、敏感地评估知识保留能力，为模型编辑方法的比较和改进提供了更好的评估框架。

Abstract: Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.

</details>


### [102] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体学习路径规划框架，通过角色化智能体协作实现透明、自适应、可解释的个性化学习路径规划


<details>
  <summary>Details</summary>
Motivation: 现有智能导学系统中的学习路径规划方法缺乏透明度、适应性和以学习者为中心的可解释性，需要开发更可信、可解释的教育AI系统

Method: 提出多智能体学习路径规划框架，包含学习者分析、路径规划和反思三个任务特定智能体，基于角色和规则协作机制，使用结构化提示和预定义规则，基于认知负荷理论和最近发展区理论进行约束

Result: 在MOOCCubeX数据集上使用7个LLM的实验表明，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型，消融研究验证了协作机制和理论约束的有效性

Conclusion: 该研究为可信、可解释的教育AI发展做出贡献，展示了基于LLM的以学习者为中心的自适应教学的规模化方法，具有实际应用价值

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [103] [Auditing Disability Representation in Vision-Language Models](https://arxiv.org/abs/2601.17348)
*Srikant Panda,Sourabh Singh Yadav,Palkesh Malviya*

Main category: cs.AI

TL;DR: 研究视觉语言模型在残疾相关描述中的解释偏移问题，发现引入残疾上下文会降低解释保真度，导致推测性推断、叙事扩展、情感降级和缺陷导向框架等偏差，这些效应在种族和性别维度上进一步放大。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型越来越多地应用于社会敏感领域，但它们在残疾相关行为方面的表现尚未得到充分研究。模型在人物中心图像描述中经常从基于证据的事实描述转向解释偏移，引入超出可观察视觉证据的不支持推断。

Method: 引入基于中性提示和残疾情境化提示配对的基准，在零样本设置下评估15个最先进的开放和闭源视觉语言模型，涵盖9个残疾类别。评估框架将解释保真度作为核心目标，结合标准文本指标（捕捉情感降级、社会关注和响应长度变化）和经过残疾生活经验标注者验证的LLM-as-judge协议。

Result: 引入残疾上下文会持续降低解释保真度，导致解释偏移，表现为推测性推断、叙事扩展、情感降级和缺陷导向框架。这些效应在种族和性别维度上进一步放大。针对性提示和偏好微调能有效提高解释保真度并显著减少解释偏移。

Conclusion: 视觉语言模型在残疾相关描述中存在系统性解释偏移问题，需要更严格的评估和干预策略。针对性提示和偏好微调是改善模型解释保真度的有效方法，但需要进一步研究以确保模型在社会敏感应用中的公平性和准确性。

Abstract: Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.

</details>


### [104] [A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models](https://arxiv.org/abs/2601.17426)
*Zhengqing Zang,Yuqi Ding,Yanmei Gu,Changkai Song,Zhengkai Yang,Guoping Du,Junbo Zhao,Haobo Wang*

Main category: cs.AI

TL;DR: 该研究探索大语言模型在逻辑推理中是否表现出从传统逻辑向现代逻辑的演变，使用存在引入作为探针，通过三段论测试发现模型规模、思维链和基础模型是影响这一演变的关键因素。


<details>
  <summary>Details</summary>
Motivation: 人类逻辑从直觉驱动推理转向严谨的形式系统，受大语言模型最新进展启发，研究者探索LLMs是否在底层逻辑框架上表现出类似的演变，特别是使用存在引入作为探针来评估三段论推理。

Method: 使用存在引入作为探针，在传统逻辑和现代逻辑框架下评估三段论推理。通过在新构建的三段论数据集上测试最先进的LLMs进行广泛实验，分析模型规模、思维链和基础模型等因素的影响。

Result: 发现三个关键结果：(1) 模型规模扩展促进向现代逻辑的转变；(2) 思维链作为参数扩展之外的高效加速器；(3) 基础模型在决定这一转变的容易程度和稳定性方面起关键作用。此外还进行了额外实验深入分析当前LLMs在三段论推理上的特性。

Conclusion: 大语言模型在逻辑推理中确实表现出从传统逻辑向现代逻辑的演变趋势，模型规模、思维链和基础模型是影响这一演变的关键因素，为理解LLMs的逻辑推理能力提供了重要见解。

Abstract: Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.

</details>


### [105] [Lattice: Generative Guardrails for Conversational Agents](https://arxiv.org/abs/2601.17481)
*Emily Broadhurst,Tawab Safi,Joseph Edell,Vashisht Ganesh,Karime Maamari*

Main category: cs.AI

TL;DR: Lattice是一个用于对话AI系统护栏的自构建和持续改进框架，通过两阶段方法实现自适应安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI系统的护栏使用静态规则，无法适应新威胁或部署环境变化，需要能够自我构建和持续改进的护栏框架。

Method: Lattice采用两阶段方法：构建阶段通过迭代模拟和优化从标注示例构建初始护栏；持续改进阶段通过风险评估、对抗测试和整合自主适应已部署的护栏。

Result: 在ProsocialDialog数据集上，Lattice在保留数据上达到91% F1分数，比关键词基线高43个百分点，比LlamaGuard高25个百分点，比NeMo高4个百分点。持续改进阶段通过闭环优化在跨域数据上实现7个百分点的F1提升。

Conclusion: Lattice框架表明，有效的护栏可以通过迭代优化实现自我构建，为对话AI系统提供自适应安全防护解决方案。

Abstract: Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.

</details>


### [106] [Cognitive Platform Engineering for Autonomous Cloud Operations](https://arxiv.org/abs/2601.17542)
*Vinoth Punniyamoorthy,Nitin Saksena,Srivenkateswara Reddy Sankiti,Nachiappan Chockalingam,Aswathnarayan Muthukrishnan Kirubakaran,Shiva Kumar Reddy Carimireddy,Durgaraman Maruthavanan*

Main category: cs.AI

TL;DR: 论文提出认知平台工程新范式，通过四层参考架构将感知、推理和自主行动集成到平台生命周期中，以解决传统DevOps在云原生系统规模和动态性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统DevOps实践（自动化、CI/CD管道、可观测性工具）难以应对云原生系统的规模和动态性挑战。随着遥测数据量增长和配置漂移加剧，基于规则的自动化往往导致被动运维、修复延迟和对人工专业知识的依赖。

Method: 提出认知平台工程范式，设计四层参考架构：数据收集层、智能推理层、策略驱动编排层和人类体验层，形成持续反馈循环。原型实现基于Kubernetes、Terraform、Open Policy Agent和基于机器学习的异常检测。

Result: 原型演示显示在平均解决时间、资源效率和合规性方面均有改进。结果表明，将智能嵌入平台运维能够实现弹性、自我调整和意图对齐的云环境。

Conclusion: 认知平台工程通过将智能集成到平台操作中，使云环境更具弹性、自我调整能力和意图对齐性。未来研究方向包括强化学习、可解释治理和可持续自管理云生态系统。

Abstract: Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.

</details>


### [107] [JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research](https://arxiv.org/abs/2601.17564)
*Aadam,Monu Verma,Mohamed Abdel-Mottaleb*

Main category: cs.AI

TL;DR: JaxARC是一个基于JAX实现的高性能强化学习环境，用于ARC推理任务，相比Gymnasium实现38-5,439倍加速，支持大规模并行实验。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Gymnasium的RL环境在ARC任务上存在计算瓶颈，限制了实验规模，需要高性能环境来支持大规模强化学习研究。

Method: 采用JAX实现功能化、无状态的架构，支持大规模并行计算，提供多种ARC数据集、灵活的动作空间、可组合的包装器和配置驱动的可复现性。

Result: 在匹配的批量大小下，相比Gymnasium实现38-5,439倍加速，峰值吞吐量达到7.9亿步/秒，支持先前计算上不可行的大规模RL研究。

Conclusion: JaxARC通过高性能实现解决了ARC推理任务中的计算瓶颈问题，为大规模强化学习研究提供了可行的实验平台。

Abstract: The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.

</details>


### [108] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: 论文提出了Health-ORSC-Bench，首个大规模医疗领域基准，用于系统评估LLM的过度拒绝和安全完成质量，揭示当前模型在安全性和实用性之间的平衡困境。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域LLM的安全对齐主要依赖二元拒绝边界，导致对良性查询的过度拒绝或对有害查询的不安全遵从。现有基准仅测量极端情况，无法评估模型在双用途或边界查询上提供安全、高级指导而不跨越可操作伤害的能力。

Method: 构建了包含31,920个良性边界提示的Health-ORSC-Bench基准，涵盖七个健康类别（如自残、医疗错误信息）。采用自动化流水线结合人工验证，在不同意图模糊度级别测试模型。评估了30个最先进的LLM，包括GPT-5和Claude-4。

Result: 安全优化模型对"困难"良性提示的拒绝率高达80%，而领域特定模型常为实用性牺牲安全性。模型家族和规模显著影响校准：大型前沿模型（如GPT-5、Llama-4）表现出"安全悲观主义"和更高的过度拒绝，较小或MoE模型（如Qwen-3-Next）表现不同。当前LLM难以平衡拒绝和遵从。

Conclusion: Health-ORSC-Bench为校准下一代医疗AI助手提供了严格标准，推动其实现细致、安全和有用的完成。模型家族和规模是影响安全-效用权衡的关键因素，需要更精细的校准方法。

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [109] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua,Zhen Han,Zhengyuan Shen,Jeremy Lee,Patrick Guan,Qi Zhu,Sullam Jeoung,Yueyan Chen,Yunfei Bai,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.AI

TL;DR: SQL-Trail：一个用于Text-to-SQL的多轮强化学习智能体框架，通过交互式数据库环境执行反馈迭代优化查询，在BIRD-SQL等基准上实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL生成主要采用单次推理范式，缺乏人类专家使用的迭代推理、模式探索和错误纠正行为，导致在BIRD-SQL等挑战性基准上与人类专家存在明显差距

Method: 提出SQL-Trail多轮强化学习智能体框架：1）自适应轮次预算分配机制，根据问题难度调整智能体交互深度；2）复合奖励面板，联合激励SQL正确性和高效探索；通过与数据库环境交互并使用执行反馈迭代优化预测

Result: 在多个基准测试中达到最先进水平，数据效率比先前单次推理RL方法高18倍；7B和14B模型平均比大得多的专有系统性能高5%，证明了交互式智能体工作流的有效性

Conclusion: 交互式、智能体化的工作流程对于稳健的Text-to-SQL生成非常有效，多轮强化学习方法能够显著缩小AI系统与人类专家在复杂SQL生成任务上的差距

Abstract: While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [110] [The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data](https://arxiv.org/abs/2601.17717)
*Kaituo Zhang,Mingzhi Hu,Hoang Anh Duy Le,Fariha Kabir Torsha,Zhimeng Jiang,Minh Khai Bui,Chia-Yuan Chang,Yu-Neng Chuang,Zhen Xiong,Ying Lin,Guanchu Wang,Na Zou*

Main category: cs.AI

TL;DR: 该论文提出了LLM数据审计框架，系统评估LLM生成的多模态合成数据质量与可信度，指出当前评估方法的不足并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: LLM已成为生成多模态数据的有力工具，将数据从稀缺资源转变为可控资产，降低了真实数据获取成本。然而，确保LLM生成合成数据的高质量仍是关键挑战。现有研究主要关注生成方法，对数据质量关注有限，且多局限于单一模态，缺乏跨模态的统一视角。

Method: 提出LLM数据审计框架：1) 描述LLM如何生成六种不同模态的数据；2) 从质量和可信度两个维度系统分类合成数据的内在评估指标，将评估重点从依赖下游任务性能的外在评估转向数据本身固有属性；3) 使用该评估系统分析各模态代表性生成方法的实验评估；4) 基于发现提出改进数据生成评估的具体建议；5) 概述合成数据在不同模态中的实际应用方法。

Result: 通过分析各模态代表性生成方法的实验评估，发现了当前评估实践中的重大缺陷。基于这些发现，为社区提供了改进数据生成评估的具体建议。

Conclusion: LLM数据审计框架填补了现有研究的空白，提供了跨模态的统一评估视角。该框架不仅识别了当前评估实践的不足，还提出了改进建议，并为合成数据在不同模态中的实际应用提供了方法论指导，有助于推动LLM生成合成数据领域的发展。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.

</details>


### [111] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: EntWorld是一个针对企业级工作流程的大规模基准测试，包含1,756个任务，覆盖CRM、ITIL、ERP等六个企业领域，采用基于数据库模式的确定性验证机制，揭示当前多模态大语言模型在企业环境中的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对消费级场景（如电商、旅行预订），无法捕捉企业工作流程的复杂性和严谨性。企业系统具有高密度用户界面、严格业务逻辑约束和精确状态一致性要求等特点，当前通用智能体在这些环境中表现不佳，需要专门的企业级基准测试来推动领域发展。

Method: 提出基于模式的任务生成框架，直接从底层数据库模式逆向工程业务逻辑，合成真实的长流程工作流。采用基于SQL的确定性验证机制，用严格的状态转换验证替代模糊的视觉匹配，确保任务评估的准确性和可重复性。

Result: 实验结果显示，最先进模型（如GPT-4.1）在EntWorld上的成功率仅为47.61%，远低于人类表现。这表明当前智能体在企业环境中的能力存在显著差距，需要开发领域特定的企业级智能体。

Conclusion: EntWorld作为一个严谨的测试平台，揭示了当前多模态大语言模型在企业工作流程中的局限性，为开发下一代企业级数字智能体提供了必要的评估基准，强调了开发领域特定智能体的必要性。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [112] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: NSVIF是一个神经符号框架，用于验证LLM输出是否遵循指令，将指令遵循验证建模为约束满足问题，显著优于基于LLM的方法并提供可解释反馈


<details>
  <summary>Details</summary>
Motivation: LLM不总是遵循指令，且违规行为难以观察或检查，在基于LLM的智能体工作流中，这些违规会沿推理链传播放大，导致任务失败和系统事故

Method: NSVIF将指令遵循验证建模为约束满足问题，将用户指令建模为约束，同时建模逻辑和语义约束，通过统一求解器协调逻辑推理和语义分析

Result: NSVIF显著优于基于LLM的方法，提供可解释反馈，且NSVIF的反馈有助于在不进行后训练的情况下提高LLM的指令遵循能力

Conclusion: NSVIF是一个通用、通用的验证器，对指令或LLM不做假设，为解决LLM指令遵循验证问题提供了有效的神经符号框架

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [113] [MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing](https://arxiv.org/abs/2601.17814)
*Haoxuan Ma,Guannan Lai,Han-Jia Ye*

Main category: cs.AI

TL;DR: MMR-Bench是一个用于评估多模态大语言模型路由策略的统一基准，通过控制候选模型集和计算成本模型，帮助在预算约束下选择最优模型，提升多模态任务部署效率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在架构、对齐策略和效率方面存在异质性，单一模型无法在所有任务上表现最优。实际部署中，工作负载从轻量级OCR到复杂多模态推理不等，使用单一模型要么在简单实例上过度配置计算资源，要么在困难实例上牺牲准确性。需要一种查询级别的模型选择（路由）机制来解决这一矛盾。

Method: 提出了MMR-Bench基准，包含：(1) 具有模态感知输入和可变计算预算的控制环境；(2) 涵盖OCR、通用视觉问答和多模态数学推理的广泛视觉语言任务套件；(3) 强大的单模型参考、理论上限和代表性路由策略。通过该基准评估多模态信号对路由质量的影响。

Result: 实验表明，融入多模态信号能显著提升路由质量，改善成本-准确性边界。路由系统能以最强单模型约33%的成本超越其准确性。此外，在部分模型和任务上训练的策略能够零样本泛化到新数据集和纯文本基准，无需重新调整。

Conclusion: MMR-Bench为研究自适应多模态模型选择和高效MLLM部署提供了基础框架，证明了多模态路由在平衡计算成本与任务性能方面的有效性，为实际部署中的资源优化提供了解决方案。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.

</details>


### [114] [RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance](https://arxiv.org/abs/2601.17826)
*Siyuan Yang,Xihan Bian,Jiayin Tang*

Main category: cs.AI

TL;DR: RegGuard：面向跨国药企的工业级AI助手，通过HiSACC和ReLACE技术自动解析异构监管文本，提升合规效率并降低风险


<details>
  <summary>Details</summary>
Motivation: 监管更新日益频繁复杂，跨国药企合规团队需手动解读多司法管辖区、多格式、多机构的监管规则，成本高且易出错，亟需自动化解决方案

Method: 1. 通过安全管道摄入异构文档源；2. HiSACC（分层语义聚合上下文分块）将长文档语义分割为连贯单元；3. ReLACE（监管列表自适应交叉编码器重排序）基于开源模型构建领域适应交叉编码器，联合建模用户查询与检索候选项以提升排序相关性

Result: 企业环境评估显示，RegGuard在相关性、事实依据性和上下文聚焦方面显著提升答案质量，同时大幅降低幻觉风险；系统架构具备可审计性和可追溯性，支持溯源跟踪、访问控制和增量索引

Conclusion: RegGuard为具有严格合规需求的领域提供了高效、可靠的监管文本自动化解读解决方案，其架构设计使其能够快速适应不断演变的文档源

Abstract: The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.

</details>


### [115] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: IGFT是一种无需人工对话数据、基于信息增益奖励和在线强化学习训练医疗对话AI进行患者访谈的新方法，在HPI生成任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI训练依赖昂贵的人工标注对话数据或静态数据集，无法有效学习多轮访谈策略。需要一种能够自主探索有效提问策略、无需预收集人类对话的方法。

Method: 提出信息增益微调(IGFT)，结合在线组相对策略优化(GRPO)和信息论奖励。使用信息增益奖励函数追踪临床实体（症状、时间模式、病史）在对话中的揭示情况，结合GPT-4o-mini的质量评估（临床相关性、患者参与度、特异性）计算问题奖励。使用LoRA微调Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B模型。

Result: 在Avey数据上训练，在MIMIC数据上评估泛化能力。DeepSeek-R1-Distill-Qwen-7B(IGFT)在Avey上F1得分为0.408（比基础模型提升10.9%），在MIMIC上为0.289（提升12.9%）。Llama-3.1-8B-Instruct(IGFT)分别达到0.384和0.336。两个模型在MIMIC上都优于OpenAI模型，并超越HuatuoGPT和UltraMedical等医疗领域基线模型。

Conclusion: IGFT通过信息增益奖励和在线强化学习，使医疗对话AI能够自主发现有效的提问策略，无需依赖预收集的人类对话数据，在多轮患者访谈和HPI生成任务上表现出优越性能。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [116] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: UniCog是一个通过潜在思维空间分析LLM认知的统一框架，将密集模型激活编码为稀疏解耦的潜在维度，揭示了LLM认知的帕累托原则，并利用潜在激活异常检测推理失败，最终通过潜在信息候选优先级策略提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法在解释LLM推理过程中认知能力如何被调用方面存在局限，而研究表明LLM的认知过程与人类存在根本差异，需要新的分析框架来深入理解LLM的认知机制。

Method: 提出UniCog统一框架，采用潜在变量模型将密集模型激活编码为稀疏解耦的潜在维度，对六个先进LLM（包括DeepSeek-V3.2和GPT-4o）进行广泛分析，发现认知的帕累托原则和推理失败的潜在激活异常模式。

Result: 揭示了LLM认知的帕累托原则：存在共享推理核心和特定能力特征；发现推理失败常表现为潜在激活异常；提出的潜在信息候选优先级策略在多个挑战性基准上提升推理性能达7.5%。

Conclusion: UniCog为LLM分析开辟了新范式，提供了基于认知的推理动态视图，通过潜在思维空间分析能够深入理解LLM认知机制并实际提升推理性能。

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [117] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: 关于自主实验室中智能体AI的综述，聚焦软物质领域，系统分析SDL作为智能体-环境交互问题的AI方法、分类体系、评估指标及挑战


<details>
  <summary>Details</summary>
Motivation: 自主实验室为AI提供了具有昂贵操作、噪声延迟反馈、严格约束和非平稳性的测试环境，需要系统梳理其中产生的AI问题和方法框架

Method: 将SDL自主性构建为智能体-环境交互问题，综述贝叶斯优化、主动学习、规划、强化学习、工具使用智能体等方法，提出能力驱动的分类体系，合成基准任务模板和评估指标

Result: 建立了SDL的AI方法框架，提出了基于决策视野、不确定性建模、动作参数化、约束处理、故障恢复和人类参与的分类体系，设计了成本感知性能、漂移鲁棒性、约束违反行为和可重复性等评估指标

Conclusion: 自主实验室是AI的重要测试平台，需要进一步发展多模态表示、校准不确定性、安全探索和共享基准基础设施，可验证和溯源感知的策略对调试、可重复性和安全操作至关重要

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [118] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 该研究提出了一种基于技能图的分层课程学习方法，用于在复杂实时环境（黑暗之魂III）中训练终身学习智能体，通过技能分解和选择性微调实现高效适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 终身学习智能体需要在不从头训练或覆盖已学行为的情况下扩展能力，特别是在复杂实时控制环境中。研究旨在解决如何在环境变化时高效适应，同时保持先前学到的技能。

Method: 将战斗表示为有向技能图，采用分层课程训练方法。将控制分解为五个可重用技能：相机控制、目标锁定、移动、闪避和治疗-攻击决策策略，每个技能针对特定职责优化。当环境从第一阶段变为第二阶段时，仅对部分技能进行选择性微调。

Result: 实验表明，仅对两个技能进行针对性微调就能在有限交互预算下快速恢复性能。技能分解提高了样本效率，减少了单个策略的负担，上游技能保持可迁移性。

Conclusion: 技能图课程与选择性微调相结合，为复杂实时环境中演化、持续学习的智能体提供了实用路径。这种分解方法支持高效适应环境变化，同时保持已学技能的可用性。

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [119] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang,Hung-Fu Chang,Po-An Chen*

Main category: cs.AI

TL;DR: 该研究提出了两种Text-to-SQL方法：SSEV（单智能体自精炼集成投票）管道和ReCAPAgent-SQL（基于智能体的SQL框架），用于解决自然语言到SQL转换中的挑战，并在多个基准测试中取得了竞争性性能。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL技术虽然降低了数据分析门槛，但自然语言查询的歧义性、模式链接的复杂性、SQL方言的泛化限制以及领域特定理解的需求使得生成准确SQL仍然具有挑战性。需要开发更强大的方法来处理企业数据库和真实世界Text-to-SQL任务的复杂性。

Method: 1. SSEV管道：基于PET-SQL构建，无需真实数据，集成自精炼与加权多数投票（WMV）及其随机变体（RWMA）。2. ReCAPAgent-SQL框架：基于智能体的SQL框架，集成多个专门智能体（规划、外部知识检索、批判、动作生成、自精炼、模式链接、结果验证），通过智能体协作实现SQL预测的迭代精炼。

Result: SSEV在多个基准测试中取得竞争性性能：Spider 1.0-Dev执行准确率85.5%，Spider 1.0-Test 86.4%，BIRD-Dev 66.3%。ReCAPAgent-SQL在Spider 2.0-Lite前100个查询中达到31%的执行准确率，在处理真实世界企业场景方面显示出显著改进。

Conclusion: 该研究提出的方法促进了可扩展Text-to-SQL系统在实际环境中的部署，支持以更低成本和更高效率进行数据驱动决策。智能体协作框架特别适合处理复杂的企业数据库场景，为实际应用提供了有效解决方案。

Abstract: Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [120] [Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing](https://arxiv.org/abs/2601.18061)
*Kiana Jafari,Paul Ulrich Nikolaus Rust,Duncan Eddy,Robbie Fraser,Nina Vasan,Darja Djordjevic,Akanksha Dadlani,Max Lamparth,Eugenia Kim,Mykel Kochenderfer*

Main category: cs.AI

TL;DR: 专家在心理健康领域对AI生成响应的评估存在系统性分歧，尤其是在安全关键项目上，一致性低于可接受阈值，表明基于共识的聚合方法存在问题


<details>
  <summary>Details</summary>
Motivation: 验证学习人类反馈(LHF)的基本假设——专家判断经过适当聚合后能提供有效的训练和评估基础。在心理健康这一安全要求极高的领域，专家共识尤为重要

Method: 三位认证精神科医生使用校准的评分标准独立评估LLM生成的响应；通过定量分析评估者间信度（ICC、Krippendorff's α），并进行定性访谈了解分歧原因

Result: 评估者间信度持续较差（ICC 0.087-0.295），低于可接受阈值；自杀和自残类别的分歧最大；一个因素的信度系数为负（α=-0.203），表明存在结构化分歧；分歧源于不同的临床框架而非测量误差

Conclusion: 专家分歧是原则性的社会技术现象，反映了不同的专业哲学；基于共识的聚合方法会抹杀专业判断的多样性；建议从共识聚合转向能够保留和学习专家分歧的对齐方法

Abstract: Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $α= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.

</details>


### [121] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: EvolVE框架通过多种进化策略分析，结合结构化测试平台生成，在Verilog硬件设计任务中实现自动化，显著提升功能正确性和优化效果，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Verilog设计流程劳动密集且需要大量领域专业知识，现有大型语言模型因训练数据有限和顺序推理特性难以捕捉硬件系统的严格形式逻辑和并发特性，需要新的自动化解决方案。

Method: 提出EvolVE框架，分析多种进化策略：蒙特卡洛树搜索（MCTS）用于最大化功能正确性，思路引导精炼（IGR）用于优化；结合结构化测试平台生成（STG）加速进化过程；引入IC-RTL基准测试套件，针对工业规模问题。

Result: 在VerilogEval v2上达到98.1%，RTLLM v2上达到92%；在工业规模IC-RTL套件上超越竞赛参与者参考实现，Huffman编码中PPA乘积降低66%，所有问题几何平均降低17%。

Conclusion: EvolVE框架通过进化策略分析和结构化测试平台生成，成功解决了Verilog硬件设计的自动化挑战，在功能正确性和优化方面均达到最先进水平，为工业规模硬件设计提供了有效解决方案。

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [122] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: OurBench是首个企业级SQL推理与调试基准，通过自动化注入真实错误构建，包含469个语法错误查询和516个语义错误查询，评估显示当前LLMs在复杂SQL调试任务上表现不佳（最佳模型准确率仅36.46%）。


<details>
  <summary>Details</summary>
Motivation: 企业数据工程中SQL生成至关重要，但即使是经验丰富的开发者和先进LLMs也难以一次性生成完全正确的SQL代码，通常需要多次调试迭代。现有基准缺乏对企业级SQL调试能力的评估，需要专门针对SQL推理和调试的基准测试。

Method: 提出两个关键创新：(1) 自动化构建工作流，使用逆向工程技术在大规模SQL代码中系统注入真实错误，实现可扩展和多样化的基准生成；(2) 面向企业环境的免执行评估框架，提供快速、准确且资源高效的评估。构建了469个语法错误查询（OurBenchSyn）和516个语义错误查询（OurBenchSem），查询复杂度高（平均超过140行，具有深广的抽象语法树）。

Result: 评估近30个LLMs显示显著性能差距：最佳模型Claude-4-Sonnet在OurBenchSyn上仅达到36.46%准确率，在OurBenchSem上为32.17%，大多数模型得分低于20%。研究进一步探索了四种解决方案策略，识别了关键挑战，并概述了企业SQL调试的有前景方向。

Conclusion: OurBench填补了企业级SQL调试基准的空白，揭示了当前LLMs在复杂SQL调试任务上的局限性，为未来研究提供了重要评估工具和方向指导。结果表明企业SQL调试仍是一个具有挑战性的开放问题，需要更先进的推理和调试能力。

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [123] [Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters](https://arxiv.org/abs/2601.18123)
*Muhammad Ibrahim Khan,Bivin Pradeep,James Brusey*

Main category: cs.AI

TL;DR: 研究家庭浸入式热水器的截止时间感知控制，通过强化学习（PPO）相比传统bang-bang控制和MCTS规划器，在相同物理条件下显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 传统家庭浸入式热水器在冬季通常连续运行，追求快速加热而非效率，忽略了可预测的需求窗口和环境热损失。需要开发能在指定时间达到目标温度同时最小化能耗的智能控制方法。

Method: 建立Gymnasium环境模拟浸入式热水器（一阶热损失模型，120秒间隔的0W/6000W开关控制）。比较三种方法：时间最优bang-bang基线、零样本蒙特卡洛树搜索规划器、近端策略优化强化学习策略。

Result: 在初始温度10-30°C、截止时间30-90步（1-3小时）、目标温度40-80°C的参数扫描中，PPO在60步（2小时）时耗能最低（3.23kWh），相比bang-bang控制（4.37-10.45kWh）和MCTS（4.18-6.46kWh）显著节能。典型轨迹（50kg水质量，20°C环境温度，60°C目标）中，PPO比bang-bang节能54%，比MCTS节能33%。

Conclusion: 学习型截止时间感知控制能在相同物理假设下显著降低能耗。规划器无需训练即可提供部分节能，而学习策略一旦训练完成，推理成本几乎为零，具有实际应用价值。

Abstract: Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.

</details>


### [124] [RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening](https://arxiv.org/abs/2601.18132)
*Xi Chen,Hongru Zhou,Huahui Yi,Shiyu Feng,Hanyu Zhou,Tiancheng He,Mingke You,Li Wang,Qiankun Li,Kun Wang,Weili Fu,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: RareAlert是一个基于LLM推理校准的罕见病早期筛查系统，通过整合10个LLM的推理信号，训练出可在本地部署的单一模型，在158,666例真实世界数据上实现了0.917的AUC，优于所有评估的LLM和机器学习集成方法。


<details>
  <summary>Details</summary>
Motivation: 罕见病的漏诊和延迟诊断是临床主要挑战，初级医疗分诊流程在初次就诊时难以可靠识别罕见病患者，需要通用筛查来减少诊断延迟。现有方法在信息有限、高度不确定的初次临床接触中无法有效评估罕见病风险。

Method: 开发RareAlert系统：1) 整合10个LLM生成的推理信号；2) 使用机器学习校准和加权这些信号；3) 将对齐的推理蒸馏到单一可本地部署的模型。使用RareBench数据集（158,666例，33个Orphanet疾病类别，7000+罕见病）进行开发和评估。

Result: 在独立测试集上，基于Qwen3-4B并使用校准推理信号训练的RareAlert模型达到0.917的AUC，优于最佳机器学习集成和所有评估的LLM（包括GPT-5、DeepSeek-R1、Claude-3.7-Sonnet、o3-mini、Gemini-2.5-Pro、Qwen3-235B）。

Conclusion: 罕见病识别可重新概念化为应用于普通患者群体的通用不确定性解决过程。LLM在医学推理中存在多样性，在高度不确定的临床任务中对齐这种推理是有效的。RareAlert通过将校准推理整合到单一模型中，实现了准确、隐私保护、可扩展的罕见病风险筛查，适合大规模本地部署。

Abstract: Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.

</details>


### [125] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: DeepPlanning是一个具有挑战性的长时程智能体规划基准，专注于多日旅行规划和多产品购物任务，需要主动信息获取、局部约束推理和全局约束优化。


<details>
  <summary>Details</summary>
Motivation: 当前智能体评估虽然转向长时程任务，但大多数基准仍强调局部、步骤级推理，而非需要真正规划能力的全局约束优化（如时间和财务预算）。现有LLM规划基准未能充分体现现实世界中典型的主动信息收集和细粒度局部约束。

Method: 引入DeepPlanning基准，包含多日旅行规划和多产品购物任务，这些任务要求智能体进行主动信息获取、局部约束推理和全局约束优化。基准评估前沿智能体LLM在这些复杂规划问题上的表现。

Result: 评估显示，即使是前沿的智能体LLM在这些问题上也表现不佳，突显了可靠显式推理模式和并行工具使用对于实现更好的效果-效率权衡的重要性。错误分析为进一步改进长规划时程的智能体LLM指出了有前景的方向。

Conclusion: DeepPlanning基准填补了现有LLM规划评估的空白，强调需要更真实的规划能力评估，包括主动信息收集和约束优化。开源代码和数据以支持未来研究。

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [126] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: 成功条件化（success conditioning）是一种广泛使用的策略改进技术，通过收集轨迹、识别成功轨迹并模仿其动作来更新策略。本文证明该方法精确解决了一个信任域优化问题，建立了策略改进、策略变化幅度和动作影响之间的恒等式，揭示了该方法作为保守改进算子的本质。


<details>
  <summary>Details</summary>
Motivation: 成功条件化技术（如拒绝采样+SFT、目标条件RL、决策变换器等）被广泛使用，但其解决的优化问题本质一直不明确。本文旨在从理论层面阐明成功条件化究竟解决了什么优化问题，以及其数学基础是什么。

Method: 通过数学证明，展示成功条件化精确解决了一个信任域优化问题：在χ²散度约束下最大化策略改进，其中约束半径由数据自动确定。建立了相对策略改进、策略变化幅度和动作影响之间的恒等关系。还将理论应用于常见的回报阈值化实践。

Result: 成功条件化被证明是保守的改进算子：它不会降低性能或引发危险的分布偏移。当失败时，它会通过几乎不改变策略来可观察地失败。回报阈值化可以放大改进，但可能以与真实目标不对齐为代价。

Conclusion: 成功条件化技术具有坚实的数学基础，它精确解决了一个信任域优化问题，并作为保守的改进算子运行。该方法在理论上保证了安全性，同时为实践中常见的回报阈值化提供了理论解释和警示。

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [127] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究LLM智能体在未知测试领域中的泛化能力，发现环境状态信息丰富度和规划复杂度是影响跨域泛化的关键因素，而非领域真实性或文本相似度


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体通常在狭窄环境集上进行后训练，但部署到更广泛的未知领域。本研究旨在探索当最终测试领域未知时，智能体后训练面临的挑战，分析哪些RL环境属性和建模选择对跨域性能影响最大

Method: 首先识别与跨域泛化强相关的两个环境轴：状态信息丰富度（agent需要处理的状态信息量）和规划复杂度（通过基础策略下的目标可达性和轨迹长度估计）。提出随机化技术：向状态添加少量分散注意力的目标无关特征来增加状态丰富度而不改变任务。同时考察建模选择：SFT预热或中期训练的影响，以及RL期间启用逐步思考的作用

Result: 发现状态信息丰富度和规划复杂度是跨域泛化的关键因素，领域真实性和文本相似度并非主要因素。增加状态信息丰富度能有效提高跨域鲁棒性。SFT预热/中期训练有助于防止RL期间的灾难性遗忘，但会削弱对未包含在中期训练数据混合中的领域的泛化能力。启用逐步思考在RL期间虽不总能提高域内性能，但对保持泛化能力至关重要

Conclusion: 为提升LLM智能体在未知领域的泛化能力，应关注环境状态信息丰富度和规划复杂度，而非领域真实性。提出的随机化技术是低开销且广泛适用的方法。在建模方面，需要权衡SFT训练与泛化能力的关系，并重视逐步思考在保持跨域泛化中的作用

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [128] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang,Yanan Wu,Xiaoshuai Song,Weixun Wang,Gengru Chen,Zhongwen Li,Kezhong Yan,Ken Deng,Qi Liu,Shuaibing Zhao,Shaopan Xiong,Xuepeng Liu,Xuefeng Chen,Wanxi Deng,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: ShopSimulator：一个大规模中文电商购物模拟环境，用于评估和训练LLM智能体在个性化产品搜索、多轮对话和相似产品辨别方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏统一的模拟环境来全面评估LLM智能体在电商购物中的关键能力，包括：理解个人偏好、进行多轮对话、检索和区分高度相似的产品。现有工作主要关注评估基准，缺乏训练支持。

Method: 提出ShopSimulator——一个大规模且具有挑战性的中文购物环境。利用该环境评估LLM在不同场景下的表现，并进行错误分析。进一步探索训练方法，包括监督微调（SFT）和强化学习（RL）的组合。

Result: 评估发现即使表现最好的模型完整成功率也不到40%。错误分析显示智能体在长轨迹中的深度搜索和产品选择方面存在困难，无法平衡个性化线索的使用，且与用户互动效果不佳。SFT和RL的组合训练能显著提升性能。

Conclusion: ShopSimulator为LLM智能体在电商购物中的全面评估和训练提供了统一环境。实验揭示了当前模型的局限性，并证明结合SFT和RL的训练方法能有效提升智能体在复杂购物任务中的表现。

Abstract: Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [129] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 提出In-Situ Self-Evolving范式，通过工具演化实现智能体在开放环境中的持续能力扩展，无需真实标签监督


<details>
  <summary>Details</summary>
Motivation: 传统智能体系统在开放环境中面临任务分布持续漂移和外部监督稀缺的挑战，依赖静态工具集或离线训练导致能力边界僵化且未知

Method: 提出In-Situ Self-Evolving范式，将顺序任务交互视为连续经验流，将短期执行反馈提炼为长期可重用能力；开发Yunjue Agent系统，通过迭代合成、优化和重用工具应对新挑战；引入Parallel Batch Evolution策略优化演化效率

Result: 在五个不同基准测试的零起点设置下，相比专有基线取得显著性能提升；补充的暖启动评估证实积累的通用知识可无缝迁移到新领域；提出监测演化收敛的新指标

Conclusion: In-Situ Self-Evolving范式通过工具演化实现智能体在开放环境中的持续能力扩展，为弹性自演化智能研究提供新方向，并开源代码库、系统轨迹和演化工具

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [130] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: TAFC框架通过函数和参数级别的显式推理增强LLM函数调用准确性，引入通用"think"参数增强和动态优化，无需修改模型架构


<details>
  <summary>Details</summary>
Motivation: 当前LLM函数调用机制缺乏参数生成的显式推理透明度，特别是对于具有相互依赖参数的复杂函数。现有方法如思维链提示在智能体级别操作，无法为单个函数参数提供细粒度推理指导

Method: 提出Think-Augmented Function Calling框架：1) 引入通用"think"参数增强，让模型阐述决策过程；2) 动态优化参数描述以提高推理质量；3) 基于复杂度评分自动触发细粒度推理；4) 提出推理引导优化以对齐人类期望

Result: 在ToolBench上对专有和开源模型的评估显示，多参数函数的参数生成准确性和推理连贯性显著提升，同时为调试AI智能体行为提供增强的可解释性

Conclusion: TAFC框架通过显式推理增强函数调用准确性，无需修改现有LLM架构且保持完全API兼容性，在保持实用性的同时提高了透明度和可解释性

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [131] [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308)
*Geunsik Lim*

Main category: cs.AI

TL;DR: Climate RADAR是一个基于生成式AI的可靠性层，将传统预警系统从警报传递转变为行动执行，通过整合多源数据和LLM提供个性化建议，提高保护行动执行率、减少响应延迟并增强信任。


<details>
  <summary>Details</summary>
Motivation: 传统预警系统虽然能快速传播警报，但往往无法触发及时的保护行动，导致可预防的损失和不公平现象。需要将灾害通信从"警报传递"重新定义为"行动执行"，以应对日益加剧的气候相关灾害。

Method: 引入Climate RADAR系统，整合气象、水文、脆弱性和社会数据形成综合风险指数，采用带有护栏的大型语言模型（LLMs）为公民、志愿者和市政部门提供个性化行动建议。

Result: 通过模拟、用户研究和市政试点评估显示，系统改善了多项指标：提高了保护行动执行率、减少了响应延迟、增强了可用性和信任度。

Conclusion: Climate RADAR通过结合预测分析、行为科学和负责任AI，推进了以人为本、透明和公平的预警系统，为符合要求的灾害韧性基础设施提供了实用路径。

Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.

</details>


### [132] [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353)
*Tuhin Chakrabarty,Paramveer S. Dhillon*

Main category: cs.AI

TL;DR: 研究通过实验发现，经过微调的AI在模仿著名作家风格方面已超越人类专家，这引发了专业作家的身份危机，并对创意劳动的未来提出根本性问题。


<details>
  <summary>Details</summary>
Motivation: 挑战"创意写作是独特人类能力"的传统假设，探究生成式AI在模仿作家风格方面的能力边界，以及AI对创意劳动的影响。

Method: 行为实验设计：28名MFA作家（专家）与3个LLM竞争模仿50位备受好评的作家风格；采用盲测成对比较，由28名专家评委和131名非专业评委评估；对比上下文提示和基于作者完整作品微调两种条件。

Result: 上下文提示条件下，专家评委82.7%偏好人类写作；但经过微调后，专家偏好反转，62%偏好AI写作；非专业评委始终偏好AI写作；专家作家访谈显示AI偏好引发了身份危机和美学自信侵蚀。

Conclusion: AI在模仿作家风格方面已超越人类专家，挑战了AI创意局限性的传统论述，对创意劳动的未来、作家身份认同和"优秀写作"的定义提出了根本性问题。

Abstract: Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes "good writing." These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.

</details>


### [133] [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383)
*Zhenyuan Guo,Tong Chen,Wenlong Meng,Chen Gong,Xin Yu,Chengkun Wei,Wenzhi Chen*

Main category: cs.AI

TL;DR: 论文提出DynTS方法，通过注意力图分析推理轨迹，识别关键决策token并仅保留其KV缓存，优化大型推理模型的效率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在解决复杂问题时需要显式生成推理轨迹，这带来了巨大的内存占用和计算开销，成为模型效率的瓶颈。研究发现推理轨迹中只有部分决策关键token对最终答案有决定性影响，其余token贡献可忽略。

Method: 提出动态思维token选择(DynTS)方法：1) 使用注意力图分析推理轨迹中各token的影响；2) 识别决策关键token；3) 在推理过程中仅保留这些关键token的KV缓存状态，剔除冗余条目。

Result: DynTS方法通过选择性保留关键token的KV缓存，显著减少了内存占用和计算开销，优化了大型推理模型的推理效率，同时保持了模型性能。

Conclusion: 推理轨迹中存在大量冗余token，仅少数决策关键token对最终答案有实质性影响。DynTS方法通过动态选择并保留这些关键token的KV缓存，有效提升了大型推理模型的效率，为优化推理过程提供了新思路。

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.

</details>


### [134] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种完全离线的研究方法，通过开源套件生成大规模研究数据，训练出性能与在线强化学习相当但成本更低的8B参数研究智能体。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体在长时程任务中表现出色，但最先进的性能通常依赖昂贵的在线强化学习（RL），需要大量API调用。离线训练虽然更高效，但受限于高质量研究轨迹的稀缺性。作者旨在证明昂贵的在线RL并非构建强大研究智能体的唯一途径。

Method: 引入完全开源套件进行有效离线训练：1) DeepForge - 即用型任务合成框架，无需繁重预处理即可生成大规模研究查询；2) 精心策划的数据集：66k QA对、33k SFT轨迹和21k DPO对。利用这些资源训练OffSeeker（8B参数模型），完全离线开发。

Result: 在六个基准测试上的广泛评估表明，OffSeeker不仅在同尺寸智能体中领先，而且与通过大量在线RL训练的30B参数系统保持竞争力。

Conclusion: 该研究证明了离线训练可以替代昂贵的在线强化学习来构建强大的研究智能体，通过开源工具和数据集为社区提供了高效、可复现的研究框架。

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [135] [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496)
*Zihan wang,Hao Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yiqun Zhang,Jinghao Lin,Haihua Yang,Xiaozhong Ji*

Main category: cs.AI

TL;DR: DeepMed：针对医学领域的深度研究模型，通过解决任务特性和工具使用扩展两大差距，在医学基准测试中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有医学推理模型受限于参数知识，易出现遗忘和幻觉问题。通用深度研究模型虽能基于工具证据生成输出，但直接应用于医学领域效果有限，主要存在两大差距：任务特性差距（医学问题需要知识密集的临床上下文推理）和工具使用扩展差距（盲目扩展工具调用会引入噪声上下文，干扰敏感医学推理）

Method: 提出DeepMed框架：1) 数据层面：采用多跳医学搜索QA合成方法，支持模型在医学上下文中应用深度研究范式；2) 训练层面：引入难度感知的轮次惩罚机制，抑制过度工具调用增长；3) 推理层面：引入监控器帮助在可控步骤内验证假设，避免上下文腐化

Result: 在七个医学基准测试中，DeepMed相比基础模型平均提升9.79%，并优于更大的医学推理和深度研究模型

Conclusion: DeepMed通过专门设计的医学深度研究框架，有效解决了通用深度研究模型在医学领域应用中的局限性，显著提升了医学推理性能，为医学AI系统提供了更可靠的证据基础推理能力

Abstract: Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\% on average and outperforms larger medical reasoning and DR models.

</details>


### [136] [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554)
*Alberto Purpura,Li Wang,Sahil Badyal,Eugenio Beaufrand,Adam Faulkner*

Main category: cs.AI

TL;DR: MOSAIC是一个模块化框架，使用包含最多20个应用导向生成约束的动态生成数据集，对LLM的指令遵循能力进行细粒度独立分析，发现合规性受约束类型、数量和位置影响显著。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往无法反映真实世界使用情况，或未能将合规性与任务成功分离，可靠确保LLM遵循复杂指令是一个关键挑战。

Method: 引入MOSAIC模块化框架，使用动态生成的数据集（包含最多20个应用导向的生成约束），对五个不同家族的LLM进行细粒度独立分析。

Result: 合规性不是单一能力，而是随约束类型、数量和位置显著变化；揭示了模型特定弱点、指令间的协同与冲突交互，以及首因效应和近因效应等位置偏差。

Conclusion: 这些细粒度洞察对于诊断模型失败和开发更可靠的LLM至关重要，特别是在需要严格遵循复杂指令的系统中。

Abstract: Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.

</details>


### [137] [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588)
*Xianzhe Meng,Qiangsheng Zeng,Ling Luo,Qinghan Yang,Jiarui Hao,Wenbo Wu,Qinyu Wang,Rui Yin,Lin Qi,Renzhi Lu*

Main category: cs.AI

TL;DR: 稳定训练动态会导致生成分布熵降低，使模型集中在有限的经验模式上，产生重复性退化行为，表明优化稳定性与生成表达能力并不一致


<details>
  <summary>Details</summary>
Motivation: 分析训练稳定性对生成分布的影响，揭示稳定参数轨迹如何导致生成熵降低和模式集中现象，挑战"稳定训练确保可靠优化"的传统观点

Method: 理论分析标准最大似然训练下稳定参数轨迹的统计特性，使用基于反馈的训练框架稳定内部生成统计量，在不同架构和随机种子下进行实证验证

Result: 稳定训练导致模型近似最小化前向KL散度，同时隐式减少生成熵，使概率质量集中在有限的经验模式上，产生低熵输出和重复行为

Conclusion: 优化稳定性和生成表达能力并不内在一致，稳定性本身不足以指示生成质量，需要更全面的评估指标来平衡训练稳定性和生成多样性

Abstract: Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.

</details>


### [138] [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595)
*Joseph Cotnareanu,Didier Chetelat,Yingxue Zhang,Mark Coates*

Main category: cs.AI

TL;DR: 提出一种结合LLM与逻辑求解器的新方法，通过迭代反馈机制补充缺失的常识关系，提升复杂推理问题的解决能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要复杂证明规划的问题上表现不佳，而传统逻辑求解器虽然推理效率高，但无法处理缺失的常识关系。需要一种平衡神经与符号元素的方法来解决人类上下文中的推理问题。

Method: 提出新颖的迭代方法：1) 将问题转化为形式逻辑；2) 使用逻辑求解器分析；3) 根据求解器的反馈，让LLM提供缺失的常识关系；4) 通过搜索潜在常识假设来最大化找到有用事实的概率，同时控制成本；5) 迭代进行直到问题解决。

Result: 在移除了部分常识信息的纯逻辑推理数据集上，该方法相比现有技术始终取得显著改进，证明了在人类上下文中平衡神经与符号元素的价值。

Conclusion: 通过结合LLM的常识推理能力和逻辑求解器的形式推理效率，提出的迭代反馈机制有效解决了复杂推理问题中缺失常识关系的挑战，为混合神经符号推理系统提供了有前景的方向。

Abstract: Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.

</details>


### [139] [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630)
*Abeer Badawi,Md Tahmid Rahman Laskar,Elahe Rahimi,Sheri Grach,Lindsay Bertrand,Lames Danok,Frank Rudzicz,Jimmy Huang,Elham Dolatabadi*

Main category: cs.AI

TL;DR: 该论文提出了一种基于人类专家评估的方法，用于评估大型语言模型在心理健康对话中的表现，发现LLMs在认知支持方面表现可靠但在情感共鸣方面存在不足，揭示了认知-情感差距。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康危机日益严重，存在治疗缺口和合格治疗师短缺问题，大型语言模型有望提供可扩展的心理支持，但其可靠性、治疗相关性和与人类标准的对齐性仍面临挑战，需要建立有效的评估方法。

Method: 提出基于人类专家的评估方法：1）从真实场景数据集中整理500个心理健康对话；2）评估9个不同LLMs（包括闭源和开源模型）生成的响应；3）由两位精神病学专家独立使用5点李克特量表，基于包含6个属性的评估框架（涵盖认知支持和情感共鸣）进行评分。

Result: LLMs在认知可靠性方面表现良好，能提供安全、连贯且临床适当的信息，但在情感对齐方面表现不稳定。闭源模型（如GPT-4o）提供更平衡的治疗响应，开源模型则表现出更大的变异性和情感平淡性。研究揭示了持续的认知-情感差距。

Conclusion: 需要建立具有失败意识、临床基础的评估框架，在心理健康导向的LLMs中优先考虑关系敏感性而不仅仅是信息准确性。倡导采用人类在环的平衡评估协议，以治疗敏感性为中心，为心理健康对话AI的负责任设计和临床监督提供指导框架。

Abstract: The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.

</details>


### [140] [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631)
*Mingyang Song,Haoyu Sun,Jiawei Gu,Linjie Li,Luxin Xu,Ranjay Krishna,Yu Cheng*

Main category: cs.AI

TL;DR: AdaReasoner是一个多模态模型家族，通过将工具使用作为通用推理技能而非特定工具行为来学习，实现自主工具选择、组合和适应新工具的能力。


<details>
  <summary>Details</summary>
Motivation: 人类在面对超出自身能力的问题时会依赖工具，这为提高多模态大语言模型（MLLMs）的视觉推理能力提供了有前景的范式。有效推理的关键在于知道使用哪些工具、何时调用它们以及如何在多步骤中组合它们，即使面对新工具或新任务时也是如此。

Method: AdaReasoner通过三个核心组件实现：(1) 可扩展的数据整理流程，让模型接触长视野、多步骤的工具交互；(2) Tool-GRPO强化学习算法，基于最终任务成功优化工具选择和序列；(3) 自适应学习机制，动态调节工具使用。这些组件使模型能够从任务上下文和中间结果推断工具效用，实现多工具协调和对未见工具的泛化。

Result: AdaReasoner展现出强大的工具适应和泛化行为：自主采用有益工具、抑制无关工具、根据任务需求调整工具使用频率，尽管从未被明确训练这样做。这些能力转化为在挑战性基准测试中的最先进性能，将7B基础模型平均提升24.9%，并在VSP和Jigsaw等多个任务上超越GPT-5等强大的专有系统。

Conclusion: AdaReasoner成功地将工具使用作为通用推理技能进行学习，实现了自主工具选择、组合和适应能力，在多模态推理任务中取得了显著性能提升，展示了模型在面对新工具和新任务时的强大泛化能力。

Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.

</details>


### [141] [TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent](https://arxiv.org/abs/2601.18700)
*Xingyu Sui,Yanyan Zhao,Yulin Hu,Jiahe Guo,Weixiang Zhao,Bing Qin*

Main category: cs.AI

TL;DR: TEA-Bench：首个用于评估工具增强型情感支持对话系统的交互式基准，包含真实情感场景、工具环境和过程级评估指标，实验显示工具增强能提升支持质量并减少幻觉，但效果与模型能力相关。


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话系统和基准主要关注文本环境下的情感支持，忽视了外部工具如何实现事实基础并减少多轮情感支持中的幻觉问题，需要建立工具增强的情感支持评估框架。

Method: 提出TEA-Bench交互式基准，包含真实情感场景、MCP风格工具环境，以及同时评估情感支持质量和事实基础的过程级指标；在9个LLM上进行实验，并发布TEA-Dialog工具增强的情感支持对话数据集。

Result: 工具增强普遍提升情感支持质量并减少幻觉，但效果强烈依赖于模型能力：强模型能更选择性和有效地使用工具，而弱模型获益有限；监督微调能提升分布内支持效果但泛化能力差。

Conclusion: 工具使用对于构建可靠的情感支持智能体至关重要，TEA-Bench为评估工具增强的情感支持系统提供了首个综合框架，揭示了模型能力与工具使用效果之间的重要关系。

Abstract: Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.

</details>


### [142] [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706)
*Zhichao Yang,Sepehr Janghorbani,Dongxu Zhang,Jun Han,Qian Qian,Andrew Ressler,Gregory D. Lyng,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: Health-SCORE：一个可扩展的基于量规的医疗LLM训练与评估框架，显著降低量规开发成本


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，量规对于评估开放式LLM响应至关重要，但创建高质量、领域特定的量规需要大量专家时间和开发成本，使得基于量规的评估和训练难以扩展。

Method: 提出Health-SCORE框架，这是一个通用且可扩展的基于量规的训练和评估框架，显著降低量规开发成本而不牺牲性能。该框架提供两种实用功能：1) 作为结构化奖励信号指导具有安全感知监督的强化学习；2) 直接融入提示中通过上下文学习提高响应质量。

Result: 在开放式医疗任务中，Health-SCORE实现了与人工创建量规相当的评估质量，同时显著降低了开发工作量，使基于量规的评估和训练更具可扩展性。

Conclusion: Health-SCORE是一个有效的解决方案，能够显著降低医疗领域量规开发成本，同时保持评估质量，为基于量规的LLM评估和训练提供了可扩展的框架。

Abstract: Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.

</details>


### [143] [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716)
*Naeyma N. Islam,Thomas R. Caulfield*

Main category: cs.AI

TL;DR: AI辅助药物设计新方法：利用E3连接酶导向的分子胶促进Aβ-42靶向降解，为神经退行性疾病治疗提供新框架


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病中细胞内Aβ-42的积累是疾病早期和毒性驱动因素，但现有研究主要关注细胞外淀粉样斑块。需要开发新策略靶向降解细胞内Aβ-42，而泛素-蛋白酶体系统通过分子胶介导的降解是潜在治疗途径。

Method: 1. 系统评估Aβ-42与三种E3连接酶（CRBN、VHL、MDM2）的三元复合物形成潜力，采用基于结构的建模、ADMET筛选和分子对接；2. 开发连接酶条件化连接树变分自编码器（LC-JT-VAE），整合蛋白质序列嵌入和扭转角感知分子图，生成连接酶特异性小分子。

Result: 生成模型能够产生化学有效、新颖且靶向特异性的分子胶，能够促进Aβ-42降解。该方法为设计UPS靶向疗法提供了有前景的框架。

Conclusion: 该AI辅助药物设计方法通过分子胶介导的靶向降解，为神经退行性疾病（特别是阿尔茨海默病）的治疗提供了创新策略，展示了生成模型在开发UPS靶向疗法中的潜力。

Abstract: Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.

</details>


### [144] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: TSRBench是一个全面的多模态时间序列推理基准测试，包含4125个问题、14个领域、4个维度（感知、推理、预测、决策），评估了30多个领先模型，揭示了时间序列推理中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在现实场景中无处不在且至关重要，但现有通用模型基准测试缺乏时间序列维度。为了填补这一空白，需要建立一个全面的基准测试来评估通用模型的时间序列推理能力。

Method: 引入TSRBench基准测试，包含4125个问题，覆盖14个领域，分为4个主要维度（感知、推理、预测、决策），包含15个任务评估基本推理能力。对30多个领先的专有和开源LLM、VLM、TSLLM进行了广泛实验。

Result: 研究发现：1）缩放定律适用于感知和推理但在预测中失效；2）强推理能力不能保证准确的上下文感知预测，表明语义理解和数值预测之间存在解耦；3）尽管时间序列的文本和视觉表示具有互补性，但当前多模态模型未能有效融合它们以获得相互性能增益。

Conclusion: TSRBench提供了一个标准化评估平台，不仅突出了现有挑战，还为推进通用模型提供了宝贵见解。基准测试揭示了时间序列推理中语义理解与数值预测之间的关键差距，以及多模态融合的不足。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [145] [Frequency-aware Adaptive Contrastive Learning for Sequential Recommendation](https://arxiv.org/abs/2601.17057)
*Zhikai Wang,Weihua Zhang*

Main category: cs.IR

TL;DR: FACL框架通过频率感知的自适应对比学习，解决序列推荐中数据增强对低频物品和稀疏用户行为的偏见问题，显著提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 研究发现传统对比学习中的数据增强方法存在固有偏见，会损害低频物品和稀疏用户行为的完整性，导致推荐系统在长尾场景下性能下降。

Method: 提出FACL框架，包含微观层面的自适应扰动保护稀有物品完整性，以及宏观层面的重加权机制放大稀疏和稀有交互序列在训练中的影响。

Result: 在五个公共基准数据集上，FACL持续优于最先进的数据增强和模型增强方法，推荐准确率提升最高达3.8%，显著缓解了低频物品和用户的性能下降。

Conclusion: FACL框架具有强大的意图保持能力，在真实世界长尾推荐场景中表现出优越适用性，为序列推荐中的对比学习提供了频率感知的解决方案。

Abstract: In this paper, we revisited the role of data augmentation in contrastive learning for sequential recommendation, revealing its inherent bias against low-frequency items and sparse user behaviors. To address this limitation, we proposed FACL, a frequency-aware adaptive contrastive learning framework that introduces micro-level adaptive perturbation to protect the integrity of rare items, as well as macro-level reweighting to amplify the influence of sparse and rare-interaction sequences during training. Comprehensive experiments on five public benchmark datasets demonstrated that FACL consistently outperforms state-of-the-art data augmentation and model augmentation-based methods, achieving up to 3.8% improvement in recommendation accuracy. Moreover, fine-grained analyses confirm that FACL significantly alleviates the performance drop on low-frequency items and users, highlighting its robust intent-preserving ability and its superior applicability to real-world, long-tail recommendation scenarios.

</details>


### [146] [Evaluation on Entity Matching in Recommender Systems](https://arxiv.org/abs/2601.17218)
*Zihan Huang,Rohan Surana,Zhouhang Xie,Junda Wu,Yu Xia,Julian McAuley*

Main category: cs.IR

TL;DR: 本文提出了Reddit-Amazon-EM数据集，用于评估推荐系统中的跨数据集实体匹配方法，填补了该领域缺乏严谨评估框架的空白。


<details>
  <summary>Details</summary>
Motivation: 推荐系统（如对话推荐系统和基于知识的推荐系统）中的实体匹配缺乏跨数据集的严谨评估框架，这阻碍了LLM驱动的对话推荐和基于知识的推荐系统数据集构建等领域的进展。

Method: 通过手动标注Reddit-Movies和Amazon'23两个现有推荐系统数据集中的重叠项目，构建了Reddit-Amazon-EM数据集，并利用该数据集全面评估了基于规则、图、词汇、嵌入和LLM的实体匹配方法。

Result: 创建了手动标注的实体匹配黄金集，并基于实验中最优方法提供了两个数据集之间的映射关系，为推荐系统实体匹配研究提供了有价值的资源。

Conclusion: Reddit-Amazon-EM数据集填补了推荐系统实体匹配评估的空白，通过系统评估多种实体匹配方法，为未来研究提供了可复现的基础和实用资源。

Abstract: Entity matching is a crucial component in various recommender systems, including conversational recommender systems (CRS) and knowledge-based recommender systems. However, the lack of rigorous evaluation frameworks for cross-dataset entity matching impedes progress in areas such as LLM-driven conversational recommendations and knowledge-grounded dataset construction.
  In this paper, we introduce Reddit-Amazon-EM, a novel dataset comprising naturally occurring items from Reddit and the Amazon '23 dataset. Through careful manual annotation, we identify corresponding movies across Reddit-Movies and Amazon'23, two existing recommender system datasets with inherently overlapping catalogs. Leveraging Reddit-Amazon-EM, we conduct a comprehensive evaluation of state-of-the-art entity matching methods, including rule-based, graph-based, lexical-based, embedding-based, and LLM-based approaches.
  For reproducible research, we release our manually annotated entity matching gold set and provide the mapping between the two datasets using the best-performing method from our experiments. This serves as a valuable resource for advancing future work on entity matching in recommender systems.

</details>


### [147] [FinMetaMind: A Tech Blueprint on NLQ Systems for Financial Knowledge Search](https://arxiv.org/abs/2601.17333)
*Lalit Pant,Shivang Nagar*

Main category: cs.IR

TL;DR: 本文提出了一个针对金融知识搜索的现代自然语言查询系统技术蓝图，通过结合NLP、搜索工程和向量数据模型，提升金融数据检索的精度、召回率和洞察深度。


<details>
  <summary>Details</summary>
Motivation: 传统金融知识搜索方法存在精度和召回率不足的问题，难以有效连接分散的金融对象、事件和关系。自然语言查询能够使用户以自然语言与信息系统交互，从而提升搜索效果并促进更深层次的金融洞察。

Method: 采用自然语言处理、搜索工程和向量数据模型的核心构建模块，设计了包含离线索引和在线检索的架构组件。系统专门针对金融数据集和文档的独特需求，解决了发现、相关性排序、数据新鲜度和实体识别等关键挑战。

Result: 提出的NLQ系统相比传统方法在金融知识搜索中实现了更高的精度和召回率，能够有效连接分散的金融实体、事件和关系，为金融服务中的增强知识搜索提供了实际应用案例。

Conclusion: 该研究为金融领域的自然语言查询系统提供了全面的技术蓝图和理论支持，通过详细的实验方法和数据分析验证了架构的有效性，并指出了未来优化的方向，对金融知识搜索的现代化具有重要指导意义。

Abstract: Natural Language Query (NLQ) allows users to search and interact with information systems using plain, human language instead of structured query syntax. This paper presents a technical blueprint on the design of a modern NLQ system tailored to financial knowledge search. The introduction of NLQ not only enhances the precision and recall of the knowledge search compared to traditional methods, but also facilitates deeper insights by efficiently linking disparate financial objects, events, and relationships. Using core constructs from natural language processing, search engineering, and vector data models, the proposed system aims to address key challenges in discovering, relevance ranking, data freshness, and entity recognition intrinsic to financial data retrieval. In this work, we detail the unique requirements of NLQ for financial datasets and documents, outline the architectural components for offline indexing and online retrieval, and discuss the real-world use cases of enhanced knowledge search in financial services. We delve into the theoretical underpinnings and experimental evidence supporting our proposed architecture, ultimately providing a comprehensive analysis on the subject matter. We also provide a detailed elaboration of our experimental methodology, the data used, the results and future optimizations in this study.

</details>


### [148] [Beyond Correlations: A Downstream Evaluation Framework for Query Performance Prediction](https://arxiv.org/abs/2601.17339)
*Payel Santra,Partha Basuchowdhuri,Debasis Ganguly*

Main category: cs.IR

TL;DR: 该论文提出了一种面向下游应用的QPP评估框架，使用QPP估计值作为IR融合的先验，发现基于相关性的标准QPP评估与下游实际效果相关性不强。


<details>
  <summary>Details</summary>
Motivation: 传统的查询性能预测评估采用集合层面的相关性度量，既无法量化单个查询级别的QPP效果，也无法与下游应用连接，导致高相关性值的QPP方法在实际IR管道中可能无法有效支持查询特定决策。

Method: 提出下游聚焦的评估框架，将多个排序器检索到的top文档列表中的QPP估计值分布作为IR融合的先验。一方面，这些估计值分布与真实检索质量分布的匹配程度反映预测器质量；另一方面，将它们作为先验使用反映预测器在IR管道中做出明智选择的能力。

Result: 实验表明：1) QPP估计值在加权IR融合中具有重要性，相比未加权的CombSUM和RRF融合策略获得超过4.5%的显著改进；2) QPP的下游效果与标准的基于相关性的QPP评估相关性不强，揭示了新的见解。

Conclusion: 需要重新思考QPP评估方法，将下游应用效果纳入考量，因为传统的相关性评估无法准确反映QPP方法在实际IR管道中的实用价值。

Abstract: The standard practice of query performance prediction (QPP) evaluation is to measure a set-level correlation between the estimated retrieval qualities and the true ones. However, neither this correlation-based evaluation measure quantifies QPP effectiveness at the level of individual queries, nor does this connect to a downstream application, meaning that QPP methods yielding high correlation values may not find a practical application in query-specific decisions in an IR pipeline. In this paper, we propose a downstream-focussed evaluation framework where a distribution of QPP estimates across a list of top-documents retrieved with several rankers is used as priors for IR fusion. While on the one hand, a distribution of these estimates closely matching that of the true retrieval qualities indicates the quality of the predictor, their usage as priors on the other hand indicates a predictor's ability to make informed choices in an IR pipeline. Our experiments firstly establish the importance of QPP estimates in weighted IR fusion, yielding substantial improvements of over 4.5% over unweighted CombSUM and RRF fusion strategies, and secondly, reveal new insights that the downstream effectiveness of QPP does not correlate well with the standard correlation-based QPP evaluation.

</details>


### [149] [Breaking Flat: A Generalised Query Performance Prediction Evaluation Framework](https://arxiv.org/abs/2601.17359)
*Payel Santra,Partha Basuchowdhuri,Debasis Ganguly*

Main category: cs.IR

TL;DR: 该研究将查询性能预测任务泛化为三种设置：单排序器多查询、多排序器单查询和多排序器多查询，发现不同任务中QPP模型效果差异显著，且预测最佳排序器比预测查询难度更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 传统查询性能预测主要关注单个排序器下不同查询的性能差异，但更细粒度的挑战是确定特定查询下哪个排序器最有效。本研究旨在将QPP任务及其评估泛化到更全面的框架中。

Method: 提出三种QPP任务设置：SRMQ-PP（传统单排序器多查询）、MRSQ-PP（多排序器单查询，预测最佳排序器）和MRMQ-PP（多排序器多查询，联合预测所有查询-排序器对）。系统评估不同QPP模型在这些任务上的表现。

Result: 研究发现：(a) QPP模型的相对有效性在不同任务间差异显著（SRMQ-PP vs MRSQ-PP）；(b) 预测查询的最佳排序器比预测特定排序器下查询的相对难度要困难得多。

Conclusion: QPP任务需要根据具体应用场景选择合适的评估框架，多排序器环境下的性能预测更具挑战性，为未来QPP研究提供了更全面的评估视角。

Abstract: The traditional use-case of query performance prediction (QPP) is to identify which queries perform well and which perform poorly for a given ranking model. A more fine-grained and arguably more challenging extension of this task is to determine which ranking models are most effective for a given query. In this work, we generalize the QPP task and its evaluation into three settings: (i) SingleRanker MultiQuery (SRMQ-PP), corresponding to the standard use case; (ii) MultiRanker SingleQuery (MRSQ-PP), which evaluates a QPP model's ability to select the most effective ranker for a query; and (iii) MultiRanker MultiQuery (MRMQ-PP), which considers predictions jointly across all query ranker pairs. Our results show that (a) the relative effectiveness of QPP models varies substantially across tasks (SRMQ-PP vs. MRSQ-PP), and (b) predicting the best ranker for a query is considerably more difficult than predicting the relative difficulty of queries for a given ranker.

</details>


### [150] [UniGRec: Unified Generative Recommendation with Soft Identifiers for End-to-End Optimization](https://arxiv.org/abs/2601.17438)
*Jialei Li,Yang Zhang,Yimeng Bai,Shuai Zhu,Ziqi Xue,Xiaoyan Zhao,Dingxian Wang,Frank Yang,Andrew Rabinovich,Xiangnan He*

Main category: cs.IR

TL;DR: UniGRec是一个统一的生成式推荐框架，通过可微分软项目标识符将分词器和推荐器统一在最终推荐目标下，解决了训练-推理差异、项目标识符崩溃和协作信号不足三大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法通常将分词器与推荐器解耦或依赖异步交替优化，限制了端到端对齐。为了统一分词器和推荐器在最终推荐目标下进行联合端到端训练，但这种方法引入了三个挑战：软硬不匹配导致的训练-推理差异、码字使用不平衡导致的标识符崩溃、以及过度关注细粒度语义导致的协作信号不足。

Method: UniGRec采用三方面策略：1) 退火推理对齐：在分词过程中平滑连接软训练和硬推理；2) 码字均匀性正则化：防止标识符崩溃并鼓励码本多样性；3) 双重协作蒸馏：从轻量级教师模型中蒸馏协作先验，共同指导分词器和推荐器。

Result: 在真实世界数据集上的广泛实验表明，UniGRec始终优于最先进的基线方法。

Conclusion: UniGRec通过统一框架解决了生成式推荐中的关键挑战，实现了分词器和推荐器的端到端联合优化，显著提升了推荐性能。

Abstract: Generative recommendation has recently emerged as a transformative paradigm that directly generates target items, surpassing traditional cascaded approaches. It typically involves two components: a tokenizer that learns item identifiers and a recommender trained on them. Existing methods often decouple tokenization from recommendation or rely on asynchronous alternating optimization, limiting full end-to-end alignment. To address this, we unify the tokenizer and recommender under the ultimate recommendation objective via differentiable soft item identifiers, enabling joint end-to-end training. However, this introduces three challenges: training-inference discrepancy due to soft-to-hard mismatch, item identifier collapse from codeword usage imbalance, and collaborative signal deficiency due to an overemphasis on fine-grained token-level semantics.
  To tackle these challenges, we propose UniGRec, a unified generative recommendation framework that addresses them from three perspectives. UniGRec employs Annealed Inference Alignment during tokenization to smoothly bridge soft training and hard inference, a Codeword Uniformity Regularization to prevent identifier collapse and encourage codebook diversity, and a Dual Collaborative Distillation mechanism that distills collaborative priors from a lightweight teacher model to jointly guide both the tokenizer and the recommender. Extensive experiments on real-world datasets demonstrate that UniGRec consistently outperforms state-of-the-art baseline methods. Our codes are available at https://github.com/Jialei-03/UniGRec.

</details>


### [151] [Adversarial Alignment and Disentanglement for Cross-Domain CTR Prediction with Domain-Encompassing Features](https://arxiv.org/abs/2601.17472)
*Junyou He,Lixi Deng,Huichao Guo,Ye Tang,Yong Li,Sulong Xu*

Main category: cs.IR

TL;DR: A²DCDR模型通过对抗对齐和特征解耦改进跨域推荐，结合域不变特征、非对齐特征和原始上下文数据，解决现有方法仅依赖域不变特征的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法通常仅解耦域不变特征和域特定特征，并主要依赖域不变特征结合目标域特定特征，这可能导致次优性能。需要更全面地捕捉跨域信息，包括域不变特征和有价值的非对齐特征。

Method: 提出A²DCDR模型，包含三个关键组件：1) 通过对抗训练改进MMD以增强泛化能力；2) 使用特征解耦器和重构机制实现域内解耦；3) 引入融合表示，结合域不变特征、非对齐特征和原始上下文数据。

Result: 在真实世界数据集和在线A/B测试中，A²DCDR模型优于现有方法，证实了其有效性和实际适用性。

Conclusion: A²DCDR模型通过对抗对齐和特征解耦，能够更全面地捕捉跨域信息，显著提升跨域推荐性能，为解决数据稀疏和冷启动问题提供了有效方案。

Abstract: Cross-domain recommendation (CDR) has been increasingly explored to address data sparsity and cold-start issues. However, recent approaches typically disentangle domain-invariant features shared between source and target domains, as well as domain-specific features for each domain. However, they often rely solely on domain-invariant features combined with target domain-specific features, which can lead to suboptimal performance. To overcome the limitations, this paper presents the Adversarial Alignment and Disentanglement Cross-Domain Recommendation ($A^2DCDR$ ) model, an innovative approach designed to capture a comprehensive range of cross-domain information, including both domain-invariant and valuable non-aligned features. The $A^2DCDR$ model enhances cross-domain recommendation through three key components: refining MMD with adversarial training for better generalization, employing a feature disentangler and reconstruction mechanism for intra-domain disentanglement, and introducing a novel fused representation combining domain-invariant, non-aligned features with original contextual data. Experiments on real-world datasets and online A/B testing show that $A^2DCDR$ outperforms existing methods, confirming its effectiveness and practical applicability. The code is provided at https://github.com/youzi0925/A-2DCDR/tree/main.

</details>


### [152] [Towards Fair Large Language Model-based Recommender Systems without Costly Retraining](https://arxiv.org/abs/2601.17492)
*Jin Li,Huilin Gu,Shoujin Wang,Qi Zhang,Shui Yu,Chen Wang,Xiwei Xu,Fang Chen*

Main category: cs.IR

TL;DR: FUDLR提出了一种快速统一的LLM推荐系统去偏方法，将去偏问题重新定义为高效机器遗忘任务，通过偏置无关掩码识别偏置样本并进行高效去偏


<details>
  <summary>Details</summary>
Motivation: LLM推荐系统容易无意中延续训练数据中的偏置，导致严重的公平性问题。现有去偏方法存在两个主要挑战：1) 针对特定偏置类型设计，缺乏处理多样或新兴偏置的通用性；2) 依赖重新训练的方法在LLM巨大参数量下计算不可行

Method: FUDLR采用两阶段方法：1) 通过新颖的偏置无关掩码识别需要遗忘的偏置诱导样本，优化公平性改进与准确性保持的平衡；2) 通过估计并移除已识别样本对模型参数的影响进行高效去偏。偏置无关设计允许通过纳入不同公平性指标适应各种或共存的偏置

Result: 大量实验表明，FUDLR能有效且高效地提高公平性，同时保持推荐准确性，为构建社会责任的LLM推荐系统提供了实用路径

Conclusion: FUDLR为解决LLM推荐系统中的公平性问题提供了一个通用、高效的解决方案，通过将去偏重新定义为机器遗忘任务，克服了现有方法的局限性，实现了在保持准确性的同时改善公平性的目标

Abstract: Large Language Models (LLMs) have revolutionized Recommender Systems (RS) through advanced generative user modeling. However, LLM-based RS (LLM-RS) often inadvertently perpetuates bias present in the training data, leading to severe fairness issues. Addressing these fairness problems in LLM-RS faces two significant challenges. 1) Existing debiasing methods, designed for specific bias types, lack the generality to handle diverse or emerging biases in real-world applications. 2) Debiasing methods relying on retraining are computationally infeasible given the massive parameter scale of LLMs. To overcome these challenges, we propose FUDLR (Fast Unified Debiasing for LLM-RS). The core idea is to reformulate the debiasing problem as an efficient machine unlearning task with two stages. First, FUDLR identifies bias-inducing samples to unlearn through a novel bias-agnostic mask, optimized to balance fairness improvement with accuracy preservation. Its bias-agnostic design allows adaptability to various or co-existing biases simply by incorporating different fairness metrics. Second, FUDLR performs efficient debiasing by estimating and removing the influence of identified samples on model parameters. Extensive experiments demonstrate that FUDLR effectively and efficiently improves fairness while preserving recommendation accuracy, offering a practical path toward socially responsible LLM-RS. The code and data are available at https://github.com/JinLi-i/FUDLR.

</details>


### [153] [To Case or Not to Case: An Empirical Study in Learned Sparse Retrieval](https://arxiv.org/abs/2601.17500)
*Emmanouil Georgios Lionis,Jia-Huei Ju,Angelos Nalmpantis,Casper Thuis,Sean MacAvaney,Andrew Yates*

Main category: cs.IR

TL;DR: 研究评估了在稀疏检索中使用大小写敏感与不敏感基础模型的影响，发现大小写敏感模型默认表现较差，但通过文本小写化预处理可以消除性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的LSR方法几乎完全依赖大小写不敏感的基础模型，但最新的SOTA语言模型只有大小写敏感版本。基础模型的大小写敏感性对LSR的影响尚未被研究，这可能威胁到该方法的未来发展可行性。

Method: 系统评估了相同基础模型的大小写敏感和不敏感版本在多个数据集上的表现，通过文本小写化预处理来消除性能差距，并进行词元级别的分析来理解模型行为。

Result: 默认情况下，使用大小写敏感基础模型的LSR模型表现显著差于大小写不敏感版本；但通过文本小写化预处理可以完全消除这一性能差距。词元级别分析显示，在小写化处理后，大小写敏感模型几乎完全抑制了大小写敏感词汇项，行为实际上等同于大小写不敏感模型。

Conclusion: 这一发现扩展了最新大小写敏感模型在LSR设置中的适用性，促进了更强基础架构与稀疏检索的集成。通过简单的文本预处理，大小写敏感模型可以在LSR中达到与大小写不敏感模型相当的性能。

Abstract: Learned Sparse Retrieval (LSR) methods construct sparse lexical representations of queries and documents that can be efficiently searched using inverted indexes. Existing LSR approaches have relied almost exclusively on uncased backbone models, whose vocabularies exclude case-sensitive distinctions, thereby reducing vocabulary mismatch. However, the most recent state-of-the-art language models are only available in cased versions. Despite this shift, the impact of backbone model casing on LSR has not been studied, potentially posing a risk to the viability of the method going forward. To fill this gap, we systematically evaluate paired cased and uncased versions of the same backbone models across multiple datasets to assess their suitability for LSR. Our findings show that LSR models with cased backbone models by default perform substantially worse than their uncased counterparts; however, this gap can be eliminated by pre-processing the text to lowercase. Moreover, our token-level analysis reveals that, under lowercasing, cased models almost entirely suppress cased vocabulary items and behave effectively as uncased models, explaining their restored performance. This result broadens the applicability of recent cased models to the LSR setting and facilitates the integration of stronger backbone architectures into sparse retrieval. The complete code and implementation for this project are available at: https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR

</details>


### [154] [Real-Time Trend Prediction via Continually-Aligned LLM Query Generation](https://arxiv.org/abs/2601.17567)
*Zijing Hui,Wenhan Lyu,Shusen Wang,Li Chen,Chu Wang*

Main category: cs.IR

TL;DR: RTTP框架通过持续学习LLM直接从新闻内容生成搜索查询，解决低流量搜索环境中的冷启动问题，实现早期趋势发现


<details>
  <summary>Details</summary>
Motivation: 低流量搜索环境中存在冷启动问题，传统基于关键词频率或查询峰值的方法在稀疏设置下效果差、响应慢，无法及时识别新兴或长尾趋势

Method: RTTP框架使用持续学习LLM将新闻帖子转换为搜索式查询，通过参与强度+创作者权威性进行评分；提出Mix-Policy DPO方法，结合on-policy稳定性和off-policy新颖性，防止模型升级时的灾难性遗忘

Result: 在Facebook和Meta AI产品中部署，长尾趋势检测精度@500提升91.4%，查询生成准确率比行业基线提高19%，多周在线训练后保持稳定性能

Conclusion: LLM生成的合成搜索信号经过对齐和持续更新，能够在低流量搜索环境中实现及时的趋势理解，解决了冷启动问题

Abstract: Trending news detection in low-traffic search environments faces a fundamental cold-start problem, where a lack of query volume prevents systems from identifying emerging or long-tail trends. Existing methods relying on keyword frequency or query spikes are inherently slow and ineffective in these sparse settings, lagging behind real-world shifts in attention. We introduce RTTP, a novel Real-Time Trending Prediction framework that generates search queries directly from news content instead of waiting for users to issue them. RTTP leverages a continual learning LLM (CL-LLM) that converts posts into search-style queries and scores them using engagement strength + creator authority, enabling early trend surfacing before search volume forms. To ensure adaptation without degrading reasoning, we propose Mix-Policy DPO, a new preference-based continual learning approach that combines on-policy stability with off-policy novelty to mitigate catastrophic forgetting during model upgrades. Deployed at production scale on Facebook and Meta AI products, RTTP delivers +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy over industry baselines, while sustaining stable performance after multi-week online training. This work demonstrates that LLM-generated synthetic search signals, when aligned and continually updated, unlock timely trend understanding in low-traffic search environments.

</details>


### [155] [Why They Link: An Intent Taxonomy for Including Hyperlinks in Social Posts](https://arxiv.org/abs/2601.17601)
*Fangping Lan,Abdullah Aljebreen,Eduard C. Dragut*

Main category: cs.IR

TL;DR: 该研究开发了一个社交媒体超链接意图分类体系，通过众包标注和LLM辅助构建了包含6个顶层类别和26个细粒度意图的分类法，并应用于1000条用户帖子分析


<details>
  <summary>Details</summary>
Motivation: URL在社交媒体信息传播中扮演核心角色（Twitter上约20%推文包含URL），但以往研究主要关注作者分享URL的动机，这些作者中心意图在实践中难以观察。为支持更广泛的下游应用，本研究转向读者中心视角，探究用户如何理解帖子中嵌入超链接的意图

Method: 采用混合方法开发超链接意图分类体系：1）自下而上的数据驱动过程，使用大规模众包标注；2）利用大语言模型辅助生成描述性类别名称和精确定义。最终构建包含6个顶层类别和26个细粒度意图的分类法，并应用于1000条用户帖子的标注分析

Result: 最终分类法包含6个顶层类别和26个细粒度意图类别，覆盖多样化的交际目的。对1000条用户帖子的分析显示，广告、争论和分享是最普遍的意图。该分类法为意图感知的信息检索和NLP应用奠定了基础

Conclusion: 本研究开发的社交媒体超链接意图分类体系能够更准确地捕捉用户对超链接意图的感知，为意图感知的信息检索、推荐系统和社交媒体内容理解提供了实用框架，有助于提升社交媒体内容的检索准确性和理解深度

Abstract: URLs serve as bridges between social media platforms and the broader web, linking user-generated content to external information resources. On Twitter (X), approximately one in five tweets contains at least one URL, underscoring their central role in information dissemination. While prior studies have examined the motivations of authors who share URLs, such author-centered intentions are difficult to observe in practice. To enable broader downstream use, this work investigates reader-centered interpretations, i.e., how users perceive the intentions behind hyperlinks included in posts. We develop an intent taxonomy for including hyperlinks in social posts through a hybrid approach that begins with a bottom-up, data-driven process using large-scale crowdsourced annotations, and is then refined using large language model assistance to generate descriptive category names and precise definitions. The final taxonomy comprises 6 top-level categories and 26 fine-grained intention classes, capturing diverse communicative purposes. Applying this taxonomy, we annotate and analyze 1000 user posts, revealing that advertising, arguing, and sharing are the most prevalent intentions. This resulting taxonomy provides a foundation for intent-aware information retrieval and NLP applications, enabling more accurate retrieval, recommendation, and understanding of social media content.

</details>


### [156] [LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval](https://arxiv.org/abs/2601.17692)
*Yunhan Li,Mingjie Xie,Gaoli Kang,Zihan Gong,Gengshen Wu,Min Yang*

Main category: cs.IR

TL;DR: LegalMALR：一个集成多智能体查询理解系统与零样本大语言模型重排模块的法律法规检索框架，显著提升对隐含、多议题、口语化法律查询的检索效果


<details>
  <summary>Details</summary>
Motivation: 现实中的法律查询通常具有隐含性、多议题性和口语化特征，这使得传统检索增强生成管道难以准确恢复法规要素。密集检索器主要关注查询的字面形式，而轻量级重排器缺乏评估法规适用性所需的法律推理能力。

Method: 提出LegalMALR框架：1）多智能体查询理解系统生成多样化的法律基础重述并进行迭代密集检索以扩大候选覆盖；2）使用广义强化策略优化统一MAS策略以稳定LLM生成重述的随机行为；3）LLM重排模块通过自然语言法律推理对累积候选集进行最终排序。构建了包含118个困难中文法律查询的CSAID数据集。

Result: 在CSAID数据集和公开STARD基准上的实验表明，LegalMALR在分布内和分布外设置下均显著优于强检索增强生成基线，证明了结合多视角查询解释、基于强化的策略优化和大模型重排对法规检索的有效性。

Conclusion: LegalMALR通过集成多智能体查询理解、强化策略优化和LLM重排，有效解决了法律查询的隐含性、多议题性和口语化特征带来的检索挑战，为法律辅助和司法决策支持提供了更准确的法規检索解决方案。

Abstract: Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.

</details>


### [157] [Token-Weighted Multi-Target Learning for Generative Recommenders with Curriculum Learning](https://arxiv.org/abs/2601.17787)
*Wei-Ning Chiu,Chuan-Ju Wang,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: 本文提出针对语义ID生成推荐系统的两种互补的token加权策略和课程学习多目标框架，解决标准似然优化中所有token同等对待的问题，提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐系统将下一项预测视为自回归序列生成任务，但大多优化标准下一token似然，隐含地将所有token视为同等信息量，这与基于语义ID的生成不匹配

Method: 提出两种互补的信息增益token加权策略：1) Front-Greater Weighting通过优先考虑最能减少候选项不确定性的早期token来捕获条件语义信息增益；2) Frequency Weighting在长尾分布下建模边际信息增益，对稀有token加权以抵消流行度偏差。此外引入课程学习多目标框架，联合优化两个token加权目标和标准似然

Result: 在基准数据集上的广泛实验表明，该方法始终优于强基线方法和现有token加权方法，具有改进的鲁棒性、跨不同语义ID构建的强泛化能力，以及在头部和尾部项目上的显著增益

Conclusion: 提出的token加权多目标学习框架有效解决了生成式推荐中语义ID生成的问题，通过信息增益策略和课程学习实现了更优的推荐性能，特别是在处理长尾分布和语义信息捕获方面

Abstract: Generative recommender systems have recently attracted attention by formulating next-item prediction as an autoregressive sequence generation task. However, most existing methods optimize standard next-token likelihood and implicitly treat all tokens as equally informative, which is misaligned with semantic-ID-based generation. Accordingly, we propose two complementary information-gain-based token-weighting strategies tailored to generative recommendation with semantic IDs. Front-Greater Weighting captures conditional semantic information gain by prioritizing early tokens that most effectively reduce candidate-item uncertainty given their prefixes and encode coarse semantics. Frequency Weighting models marginal information gain under long-tailed item and token distributions, upweighting rare tokens to counteract popularity bias. Beyond individual strategies, we introduce a multi-target learning framework with curriculum learning that jointly optimizes the two token-weighted objectives alongside standard likelihood, enabling stable optimization and adaptive emphasis across training stages. Extensive experiments on benchmark datasets show that our method consistently outperforms strong baselines and existing token-weighting approaches, with improved robustness, strong generalization across different semantic-ID constructions, and substantial gains on both head and tail items. Code is available at https://github.com/CHIUWEINING/Token-Weighted-Multi-Target-Learning-for-Generative-Recommenders-with-Curriculum-Learning.

</details>


### [158] [Unleashing the Potential of Sparse Attention on Long-term Behaviors for CTR Prediction](https://arxiv.org/abs/2601.17836)
*Weijiang Lai,Beihong Jin,Di Zhang,Siru Chen,Jiongyan Zhang,Yuhang Gou,Jian Dong,Xingxing Wang*

Main category: cs.IR

TL;DR: SparseCTR：针对推荐系统中用户长行为序列的高效稀疏注意力模型，通过个性化分块和三分支稀疏注意力机制，在提升效率的同时展现明显的缩放定律现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的缩放定律在推荐系统中难以实际部署，因为标准自注意力机制计算复杂度高。现有稀疏注意力机制不完全适合推荐场景，因为用户行为具有个性化、时序性和分布特殊性。

Method: 1. 个性化分块：将行为序列按个性化方式分块，避免分割连续行为并支持并行处理；2. 三分支稀疏自注意力：联合识别用户全局兴趣、兴趣转移和短期兴趣；3. 复合相对时序编码：通过可学习的头特定偏置系数捕捉行为间的时序和周期性关系。

Result: SparseCTR不仅提升效率，性能优于现有方法，且在三个数量级的FLOPs范围内保持性能提升，展现明显缩放定律现象。在线A/B测试中CTR提升1.72%，CPM提升1.41%。

Conclusion: SparseCTR是针对用户长行为序列的高效有效模型，通过个性化分块和稀疏注意力机制解决了推荐场景的特殊需求，实现了效率与性能的双重提升，并验证了缩放定律在推荐系统中的可行性。

Abstract: In recent years, the success of large language models (LLMs) has driven the exploration of scaling laws in recommender systems. However, models that demonstrate scaling laws are actually challenging to deploy in industrial settings for modeling long sequences of user behaviors, due to the high computational complexity of the standard self-attention mechanism. Despite various sparse self-attention mechanisms proposed in other fields, they are not fully suited for recommendation scenarios. This is because user behaviors exhibit personalization and temporal characteristics: different users have distinct behavior patterns, and these patterns change over time, with data from these users differing significantly from data in other fields in terms of distribution. To address these challenges, we propose SparseCTR, an efficient and effective model specifically designed for long-term behaviors of users. To be precise, we first segment behavior sequences into chunks in a personalized manner to avoid separating continuous behaviors and enable parallel processing of sequences. Based on these chunks, we propose a three-branch sparse self-attention mechanism to jointly identify users' global interests, interest transitions, and short-term interests. Furthermore, we design a composite relative temporal encoding via learnable, head-specific bias coefficients, better capturing sequential and periodic relationships among user behaviors. Extensive experimental results show that SparseCTR not only improves efficiency but also outperforms state-of-the-art methods. More importantly, it exhibits an obvious scaling law phenomenon, maintaining performance improvements across three orders of magnitude in FLOPs. In online A/B testing, SparseCTR increased CTR by 1.72\% and CPM by 1.41\%. Our source code is available at https://github.com/laiweijiang/SparseCTR.

</details>


### [159] [Post-Training Denoising of User Profiles with LLMs in Collaborative Filtering Recommendation](https://arxiv.org/abs/2601.18009)
*Ervin Dervishaj,Maria Maistro,Tuukka Ruotsalo,Christina Lioma*

Main category: cs.IR

TL;DR: 提出一种基于大语言模型的协同过滤推荐后训练去噪方法，通过LLM分析用户交互历史、候选物品及其排名，从用户档案中移除噪声交互以提升推荐效果


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据存在固有噪声，影响推荐系统效果。现有去噪方法多为训练中处理，需要额外数据、改变模型架构或训练过程，成本高且数据需求大。因此需要开发无需改变模型架构、训练过程或额外数据的后训练去噪方法

Method: 提出基于大语言模型的协同过滤推荐后训练去噪方法：向LLM提供(i)用户交互历史（用户档案）、(ii)候选物品、(iii)协同过滤推荐器给出的排名，要求LLM从用户档案中移除可能降低候选物品排名的噪声交互项

Result: 实验使用最先进的协同过滤推荐器和4个开源/闭源LLM，在3个数据集上验证，去噪后的用户档案相比原始档案提升效果达13%

Conclusion: 基于大语言模型的后训练去噪方法能有效提升协同过滤推荐效果，无需改变模型架构、训练过程或额外数据，为推荐系统去噪提供了新思路

Abstract: Implicit feedback -- the main data source for training Recommender Systems (RSs) -- is inherently noisy and has been shown to negatively affect recommendation effectiveness. Denoising has been proposed as a method for removing noisy implicit feedback and improving recommendations. Prior work has focused on in-training denoising, however this requires additional data, changes to the model architecture and training procedure or fine-tuning, all of which can be costly and data hungry. In this work, we focus on post-training denoising. Different from in-training denoising, post-training denoising does not involve changing the architecture of the model nor its training procedure, and does not require additional data. Specifically, we present a method for post-training denoising user profiles using Large Language Models (LLMs) for Collaborative Filtering (CF) recommendations. Our approach prompts LLMs with (i) a user profile (user interactions), (ii) a candidate item, and (iii) its rank as given by the CF recommender, and asks the LLM to remove items from the user profile to improve the rank of the candidate item. Experiments with a state-of-the-art CF recommender and 4 open and closed source LLMs in 3 datasets show that our denoising yields improvements up to 13% in effectiveness over the original user profiles. Our code is available at https://github.com/edervishaj/denoising-user-profiles-LLM.

</details>


### [160] [Enhancing LLM-based Recommendation with Preference Hint Discovery from Knowledge Graph](https://arxiv.org/abs/2601.18096)
*Yuting Zhang,Ziliang Pei,Chao Wang,Ying Sun,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: 提出基于交互集成知识图谱的偏好提示发现模型，通过选择性提取关键属性作为提示来增强LLM推荐性能，解决传统推荐嵌入与LLM语义空间不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在推荐系统中虽受关注，但在捕捉复杂偏好模式方面不如传统推荐器。现有方法将传统推荐嵌入集成到LLMs中，但连续嵌入空间与离散语义空间存在核心差距。文本属性可作为LLMs推荐逻辑的关键偏好依据，但直接输入面临两大挑战：稀疏交互难以反映未见项目的偏好线索；将所有属性作为线索会引入大量噪声。

Method: 提出基于交互集成知识图谱的偏好提示发现模型：1) 设计协作式偏好提示提取方案，利用相似用户显式交互的语义知识作为未见项目的提示；2) 开发实例级双重注意力机制，量化候选属性的偏好可信度，识别针对每个未见项目的特定提示；3) 采用扁平化提示组织方法缩短输入长度，将文本提示信息输入LLM进行常识推理。

Result: 在成对和列表推荐任务上的广泛实验验证了框架的有效性，相对于基线方法平均相对改进超过3.02%。

Conclusion: 提出的偏好提示发现模型能够有效利用传统推荐原则选择性提取关键属性作为提示，通过协作式提示提取和实例级注意力机制解决稀疏交互和噪声问题，显著提升LLM推荐性能。

Abstract: LLMs have garnered substantial attention in recommendation systems. Yet they fall short of traditional recommenders when capturing complex preference patterns. Recent works have tried integrating traditional recommendation embeddings into LLMs to resolve this issue, yet a core gap persists between their continuous embedding and discrete semantic spaces. Intuitively, textual attributes derived from interactions can serve as critical preference rationales for LLMs' recommendation logic. However, directly inputting such attribute knowledge presents two core challenges: (1) Deficiency of sparse interactions in reflecting preference hints for unseen items; (2) Substantial noise introduction from treating all attributes as hints. To this end, we propose a preference hint discovery model based on the interaction-integrated knowledge graph, enhancing LLM-based recommendation. It utilizes traditional recommendation principles to selectively extract crucial attributes as hints. Specifically, we design a collaborative preference hint extraction schema, which utilizes semantic knowledge from similar users' explicit interactions as hints for unseen items. Furthermore, we develop an instance-wise dual-attention mechanism to quantify the preference credibility of candidate attributes, identifying hints specific to each unseen item. Using these item- and user-based hints, we adopt a flattened hint organization method to shorten input length and feed the textual hint information to the LLM for commonsense reasoning. Extensive experiments on both pair-wise and list-wise recommendation tasks verify the effectiveness of our proposed framework, indicating an average relative improvement of over 3.02% against baselines.

</details>


### [161] [Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking](https://arxiv.org/abs/2601.18146)
*Huizhong Guo,Tianjun Wei,Dongxia Wang,Yingpeng Du,Ziyan Wang,Jie Zhang,Zhu Sun*

Main category: cs.IR

TL;DR: 提出推理路由框架，通过轻量级路由器头在生成前决定是否对每个实例使用推理模式，以平衡排名效果与计算成本。


<details>
  <summary>Details</summary>
Motivation: 虽然推理提示可以提升LLM在排名任务中的效果，但其收益不稳定且计算成本高昂，因此何时推理与如何推理同等重要。

Method: 提出推理路由框架，使用轻量级、即插即用的路由器头，在生成前基于预生成信号（紧凑的排名感知特征和模型感知的难度信号）决定使用直接推理还是推理模式。路由器可沿验证帕累托前沿自适应选择操作策略。

Result: 在三个公开排名数据集和不同规模的开源LLM上实验，显示排名效果持续提升同时显著减少token消耗（例如在MovieLens上使用Qwen3-4B时，NDCG@10提升6.3%，token消耗减少49.5%）。

Conclusion: 推理路由框架是解决准确性与效率权衡的实用方案，通过智能路由决策在保持排名效果的同时大幅降低计算成本。

Abstract: Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\% NDCG@10 with -49.5\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.

</details>


### [162] [Generative Chain of Behavior for User Trajectory Prediction](https://arxiv.org/abs/2601.18213)
*Chengkai Huang,Xiaodi Chen,Hongtao Huang,Quan Z. Sheng,Lina Yao*

Main category: cs.IR

TL;DR: GCB是一个生成式行为链框架，通过将物品编码为语义ID，使用自回归变换器生成多步未来行为轨迹，超越了传统序列推荐器的单步预测限制。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐器主要关注下一项预测，忽略了多个未来动作之间的依赖关系，无法捕捉用户偏好的长期演变。需要建模长期用户行为轨迹来理解演化偏好并实现主动推荐。

Method: 1. 使用RQ-VAE和k-means细化将物品编码为语义ID，形成保持语义邻近性的离散潜在空间；2. 基于该空间，采用变换器自回归生成器，以用户历史为条件预测多步未来行为，捕捉长时意图转换并生成连贯轨迹。

Result: 在基准数据集上的实验表明，GCB在多步准确性和轨迹一致性方面持续优于最先进的序列推荐器。

Conclusion: GCB不仅取得了性能提升，还为捕捉用户偏好演化提供了一个统一的生成式框架，能够建模长期行为轨迹和意图转换。

Abstract: Modeling long-term user behavior trajectories is essential for understanding evolving preferences and enabling proactive recommendations. However, most sequential recommenders focus on next-item prediction, overlooking dependencies across multiple future actions. We propose Generative Chain of Behavior (GCB), a generative framework that models user interactions as an autoregressive chain of semantic behaviors over multiple future steps. GCB first encodes items into semantic IDs via RQ-VAE with k-means refinement, forming a discrete latent space that preserves semantic proximity. On top of this space, a transformer-based autoregressive generator predicts multi-step future behaviors conditioned on user history, capturing long-horizon intent transitions and generating coherent trajectories. Experiments on benchmark datasets show that GCB consistently outperforms state-of-the-art sequential recommenders in multi-step accuracy and trajectory consistency. Beyond these gains, GCB offers a unified generative formulation for capturing user preference evolution.

</details>


### [163] [Token-level Collaborative Alignment for LLM-based Generative Recommendation](https://arxiv.org/abs/2601.18457)
*Fake Lin,Binbin Hu,Zhi Zheng,Xi Zhu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Tong Xu*

Main category: cs.IR

TL;DR: TCA4Rec提出了一种模型无关的即插即用框架，通过令牌级协同对齐解决LLM推荐系统中难以有效整合协同过滤信号的问题


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统难以有效整合协同过滤信号，因为CF基于物品级偏好建模，而LLM基于令牌级下一令牌预测优化，两者存在根本性不匹配。先前方法通常将CF视为上下文提示或表示偏差，需要多阶段训练来减少行为语义空间差异，导致CF无法显式调控LLM生成。

Method: 提出TCA4Rec框架，包含两个核心组件：(1)协同令牌化器：将原始物品级CF对数概率投影到与LLM令牌空间对齐的令牌级分布；(2)软标签对齐：将这些CF信息化的分布与独热监督结合，优化软NTP目标。该设计保留了LLM训练的生成性质，同时实现了与CF模型核心用户偏好的协同对齐。

Result: TCA4Rec与任意传统CF模型兼容，可泛化到广泛的基于解码器的LLM推荐架构。它提供了显式机制来平衡行为对齐和语义流畅性，生成既准确又可控制的推荐。大量实验表明，TCA4Rec在各种CF模型和基于LLM的推荐系统中持续提升推荐性能。

Conclusion: TCA4Rec通过建立CF监督与LLM生成之间的显式优化级接口，解决了LLM推荐系统中协同过滤整合的难题，实现了生成性推荐中行为对齐与语义流畅性的平衡，为LLM推荐系统提供了有效的协同对齐框架。

Abstract: Large Language Models (LLMs) have demonstrated strong potential for generative recommendation by leveraging rich semantic knowledge. However, existing LLM-based recommender systems struggle to effectively incorporate collaborative filtering (CF) signals, due to a fundamental mismatch between item-level preference modeling in CF and token-level next-token prediction (NTP) optimization in LLMs. Prior approaches typically treat CF as contextual hints or representation bias, and resort to multi-stage training to reduce behavioral semantic space discrepancies, leaving CF unable to explicitly regulate LLM generation. In this work, we propose Token-level Collaborative Alignment for Recommendation (TCA4Rec), a model-agnostic and plug-and-play framework that establishes an explicit optimization-level interface between CF supervision and LLM generation. TCA4Rec consists of (i) Collaborative Tokenizer, which projects raw item-level CF logits into token-level distributions aligned with the LLM token space, and (ii) Soft Label Alignment, which integrates these CF-informed distributions with one-hot supervision to optimize a soft NTP objective. This design preserves the generative nature of LLM training while enabling collaborative alignment with essential user preference of CF models. We highlight TCA4Rec is compatible with arbitrary traditional CF models and generalizes across a wide range of decoder-based LLM recommender architectures. Moreover, it provides an explicit mechanism to balance behavioral alignment and semantic fluency, yielding generative recommendations that are both accurate and controllable. Extensive experiments demonstrate that TCA4Rec consistently improves recommendation performance across a broad spectrum of CF models and LLM-based recommender systems.

</details>


### [164] [Feature-Indexed Federated Recommendation with Residual-Quantized Codebooks](https://arxiv.org/abs/2601.18570)
*Mingzhe Han,Jiahao Liu,Dongsheng Li,Hansu Gu,Peng Zhang,Ning Gu,Tun Lu*

Main category: cs.IR

TL;DR: RQFedRec提出了一种基于特征索引通信范式的联邦推荐系统，通过残差量化K-means生成离散代码ID，传输代码本而非原始项目嵌入，显著降低通信开销并提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐方法采用ID索引通信范式，传输整个项目嵌入，存在三个主要问题：1) 通信资源消耗不可控；2) 上传的项目信息无法泛化到相关的未交互项目；3) 对客户端噪声反馈敏感。需要从根本上改变现有的通信范式。

Method: 提出特征索引通信范式，通过残差量化K-means为每个项目分配离散代码ID列表。客户端基于服务器提供的代码ID生成并训练代码嵌入作为代码本，服务器聚合这些代码本而非项目嵌入。采用协作-语义双通道聚合策略，配合课程学习策略，早期强调语义代码，逐步增加协作代码的贡献。

Result: 在真实世界数据集上的大量实验表明，RQFedRec在显著降低通信开销的同时，持续优于最先进的联邦推荐基线方法。

Conclusion: RQFedRec通过特征索引通信范式有效解决了传统ID索引范式的局限性，实现了通信可控性、泛化能力和鲁棒性的提升，为联邦推荐系统提供了更高效实用的解决方案。

Abstract: Federated recommendation provides a privacy-preserving solution for training recommender systems without centralizing user interactions. However, existing methods follow an ID-indexed communication paradigm that transmit whole item embeddings between clients and the server, which has three major limitations: 1) consumes uncontrollable communication resources, 2) the uploaded item information cannot generalize to related non-interacted items, and 3) is sensitive to client noisy feedback. To solve these problems, it is necessary to fundamentally change the existing ID-indexed communication paradigm. Therefore, we propose a feature-indexed communication paradigm that transmits feature code embeddings as codebooks rather than raw item embeddings. Building on this paradigm, we present RQFedRec, which assigns each item a list of discrete code IDs via Residual Quantization (RQ)-Kmeans. Each client generates and trains code embeddings as codebooks based on discrete code IDs provided by the server, and the server collects and aggregates these codebooks rather than item embeddings. This design makes communication controllable since the codebooks could cover all items, enabling updates to propagate across related items in same code ID. In addition, since code embedding represents many items, which is more robust to a single noisy item. To jointly capture semantic and collaborative information, RQFedRec further adopts a collaborative-semantic dual-channel aggregation with a curriculum strategy that emphasizes semantic codes early and gradually increases the contribution of collaborative codes over training. Extensive experiments on real-world datasets demonstrate that RQFedRec consistently outperforms state-of-the-art federated recommendation baselines while significantly reducing communication overhead.

</details>


### [165] [FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG](https://arxiv.org/abs/2601.18579)
*Seonho An,Chaejeong Hyun,Min-Soo Kim*

Main category: cs.IR

TL;DR: FastInsight提出了一种高效的图RAG方法，通过融合图模型搜索和向量-图搜索来克服现有方法的拓扑盲点和语义盲点，在保持高效的同时显著提升检索和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有图RAG方法依赖耗时的大型语言模型推理过程，无法实现时间高效的深入检索。通过图检索分类学分析，发现当前方法存在模型搜索的拓扑盲点和图搜索的语义盲点两大关键限制。

Method: 提出FastInsight框架，包含两个新型融合算子：1) Graph-based Reranker (GRanker)作为图模型搜索，2) Semantic-Topological eXpansion (STeX)作为向量-图搜索。通过这两个算子的交替使用来克服现有方法的局限性。

Result: 在广泛的检索和生成数据集上的实验表明，FastInsight相比最先进的基线方法，在检索准确率和生成质量方面均有显著提升，在效果与效率的权衡上实现了显著的帕累托改进。

Conclusion: FastInsight通过创新的融合算子解决了图RAG中的拓扑盲点和语义盲点问题，在保持时间效率的同时实现了更深入的检索能力，为高效图检索提供了有效解决方案。

Abstract: Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.

</details>


### [166] [S$^2$GR: Stepwise Semantic-Guided Reasoning in Latent Space for Generative Recommendation](https://arxiv.org/abs/2601.18664)
*Zihao Guo,Jian Wang,Ruxin Zhou,Youhua Liu,Jiawei Guo,Jun Zhao,Xiaoxiao Xu,Yongqi Liu,Kaiqiao Zhan*

Main category: cs.IR

TL;DR: S²GR：一种通过潜在空间逐步语义引导推理增强生成式推荐的新框架，解决了现有方法在推理与生成分离、计算不平衡和语义不可解释等问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法主要关注从交互序列直接生成语义ID，未能激活类似大语言模型的深层推理能力，限制了性能潜力。当前推理增强方法存在两个关键局限：1) 推理与生成步骤的严格顺序分离导致层次化语义ID代码间计算焦点不平衡；2) 生成的推理向量缺乏可解释语义，推理路径缺乏可验证监督。

Method: 提出S²GR框架：首先通过代码本优化建立稳健语义基础，整合物品共现关系捕捉行为模式，并采用负载均衡和一致性目标最大化代码本利用率同时强化从粗到细的语义层次。核心创新是逐步推理机制，在每个语义ID生成步骤前插入思考令牌，每个令牌明确表示粗粒度语义，通过对比学习监督确保物理基础的推理路径和所有语义ID代码间的平衡计算焦点。

Result: 大量实验证明了S²GR的优越性，在线A/B测试确认了其在大规模工业短视频平台上的有效性。

Conclusion: S²GR通过潜在空间逐步语义引导推理，解决了现有生成式推荐方法的局限性，实现了更平衡的计算分配和可解释的推理路径，在工业规模应用中表现出色。

Abstract: Generative Recommendation (GR) has emerged as a transformative paradigm with its end-to-end generation advantages. However, existing GR methods primarily focus on direct Semantic ID (SID) generation from interaction sequences, failing to activate deeper reasoning capabilities analogous to those in large language models and thus limiting performance potential. We identify two critical limitations in current reasoning-enhanced GR approaches: (1) Strict sequential separation between reasoning and generation steps creates imbalanced computational focus across hierarchical SID codes, degrading quality for SID codes; (2) Generated reasoning vectors lack interpretable semantics, while reasoning paths suffer from unverifiable supervision. In this paper, we propose stepwise semantic-guided reasoning in latent space (S$^2$GR), a novel reasoning enhanced GR framework. First, we establish a robust semantic foundation via codebook optimization, integrating item co-occurrence relationship to capture behavioral patterns, and load balancing and uniformity objectives that maximize codebook utilization while reinforcing coarse-to-fine semantic hierarchies. Our core innovation introduces the stepwise reasoning mechanism inserting thinking tokens before each SID generation step, where each token explicitly represents coarse-grained semantics supervised via contrastive learning against ground-truth codebook cluster distributions ensuring physically grounded reasoning paths and balanced computational focus across all SID codes. Extensive experiments demonstrate the superiority of S$^2$GR, and online A/B test confirms efficacy on large-scale industrial short video platform.

</details>


### [167] [Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval](https://arxiv.org/abs/2601.18747)
*Amir Aavani*

Main category: cs.IR

TL;DR: 提出一种基于有向无环图（DAG）的检索语言$\mathcal{L}_R$，证明其精确捕获复杂度类$\mathbf{P}$，并设计ComputePN算法实现高效查询评估，将搜索索引转变为通用计算引擎。


<details>
  <summary>Details</summary>
Motivation: 现代信息检索正从简单文档过滤转向复杂的神经符号推理工作流，但现有检索架构在处理逻辑和算术约束时面临效率困境：基于迭代器的引擎无法高效处理复杂嵌套逻辑图，而递归方法在强制执行广泛逻辑排除时内存消耗过高。

Method: 1. 定义基于有向无环图（DAG）的形式化检索语言$\mathcal{L}_R$；2. 证明$\mathcal{L}_R$精确捕获复杂度类$\mathbf{P}$；3. 提出ComputePN评估算法，结合原生DAG遍历和内存高效的"正-负"响应机制，确保$\mathcal{L}_R$中任何查询的高效评估。

Result: 建立了将搜索索引转变为通用计算引擎的理论基础，通过$\mathcal{L}_R$语言和ComputePN算法解决了复杂逻辑约束查询的效率困境，实现了多项式时间内的高效查询评估。

Conclusion: 检索引擎必须能够"捕获$\mathbf{P}$"——在索引上以计算高效的方式直接评估任何多项式时间属性。这项工作为将搜索索引转变为通用计算引擎奠定了理论基础，解决了现代信息检索中复杂神经符号推理工作流的效率瓶颈。

Abstract: Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.
  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\mathbf{P}$. We introduce \texttt{ComputePN}, a novel evaluation algorithm that makes $\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \texttt{ComputePN} ensures the efficient evaluation of any query in $\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.

</details>
