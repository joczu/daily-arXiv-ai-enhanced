<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Chaos to Clarity: Schema-Constrained AI for Auditable Biomedical Evidence Extraction from Full-Text PDFs](https://arxiv.org/abs/2601.14267)
*Pouria Mortezaagha,Joseph Shaw,Bowen Sun,Arya Rahgozar*

Main category: cs.CL

TL;DR: 该论文提出了一种基于模式约束的AI提取系统，用于将生物医学PDF全文转换为结构化、可分析的数据记录，通过类型化模式、受控词汇表和证据门控决策来约束模型推理，实现可扩展且可审计的科学文献证据提取。


<details>
  <summary>Details</summary>
Motivation: 生物医学证据合成需要从全文研究文章中准确提取方法学、实验室和结果变量，但这些变量嵌入在复杂的科学PDF中，使得手动提取耗时且难以扩展。现有文档AI系统受限于OCR错误、长文档碎片化、吞吐量受限以及高风险合成中审计能力不足的问题。

Method: 开发了一个模式约束的AI提取系统，使用恢复感知哈希处理文档，分割为标题感知的页面级块，在显式并发控制下异步处理。通过类型化模式、受控词汇表和证据门控决策约束模型推理。块级输出使用冲突感知合并、基于集合的聚合和句子级溯源确定性合并为研究级记录。

Result: 在直接口服抗凝剂水平测量研究语料库上评估，管道无需人工干预处理所有文档，在服务约束下保持稳定吞吐量，跨文档块表现出强内部一致性。迭代模式细化显著提高了合成关键变量的提取保真度，包括检测分类、结果定义、随访持续时间和测量时间。

Conclusion: 模式约束、溯源感知的提取系统能够将异构科学PDF可扩展且可审计地转换为结构化证据，使现代文档AI与生物医学证据合成的透明度和可靠性要求保持一致。

Abstract: Biomedical evidence synthesis relies on accurate extraction of methodological, laboratory, and outcome variables from full-text research articles, yet these variables are embedded in complex scientific PDFs that make manual abstraction time-consuming and difficult to scale. Existing document AI systems remain limited by OCR errors, long-document fragmentation, constrained throughput, and insufficient auditability for high-stakes synthesis. We present a schema-constrained AI extraction system that transforms full-text biomedical PDFs into structured, analysis-ready records by explicitly restricting model inference through typed schemas, controlled vocabularies, and evidence-gated decisions. Documents are ingested using resume-aware hashing, partitioned into caption-aware page-level chunks, and processed asynchronously under explicit concurrency controls. Chunk-level outputs are deterministically merged into study-level records using conflict-aware consolidation, set-based aggregation, and sentence-level provenance to support traceability and post-hoc audit. Evaluated on a corpus of studies on direct oral anticoagulant level measurement, the pipeline processed all documents without manual intervention, maintained stable throughput under service constraints, and exhibited strong internal consistency across document chunks. Iterative schema refinement substantially improved extraction fidelity for synthesis-critical variables, including assay classification, outcome definitions, follow-up duration, and timing of measurement. These results demonstrate that schema-constrained, provenance-aware extraction enables scalable and auditable transformation of heterogeneous scientific PDFs into structured evidence, aligning modern document AI with the transparency and reliability requirements of biomedical evidence synthesis.

</details>


### [2] [The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues](https://arxiv.org/abs/2601.14269)
*Youyou Cheng,Zhuangwei Kang,Kerry Jiang,Chenyu Sun,Qiyang Pan*

Main category: cs.CL

TL;DR: 该论文提出多轮压力测试框架，发现LLMs在心理健康支持的长对话中普遍存在安全边界侵蚀问题，自适应探测显著加速边界突破


<details>
  <summary>Details</summary>
Motivation: 当前LLMs心理健康支持的安全评估主要局限于单轮对话中的禁忌词检测，忽略了长对话中安全边界的渐进式侵蚀问题。随着主流LLMs发展，明显风险词汇容易被过滤，真正的危险在于多轮交互中LLM试图提供安慰和共情时逐渐突破边界的行为

Method: 提出多轮压力测试框架，使用静态推进和自适应探测两种压力方法，对三个前沿LLMs进行长对话安全测试。生成50个虚拟患者档案，通过最多20轮虚拟精神科对话进行压力测试

Result: 违规行为普遍存在，两种压力模式产生相似的违规率。但自适应探测显著提前了模型突破边界的时间，平均轮数从静态推进的9.21轮减少到4.64轮。两种机制下，做出确定性或零风险承诺是边界突破的主要方式

Conclusion: LLM安全边界的鲁棒性不能仅通过单轮测试推断，必须充分考虑扩展对话中不同交互压力和特性对安全边界造成的磨损

Abstract: Large language models (LLMs) have been widely used for mental health support. However, current safety evaluations in this field are mostly limited to detecting whether LLMs output prohibited words in single-turn conversations, neglecting the gradual erosion of safety boundaries in long dialogues. Examples include making definitive guarantees, assuming responsibility, and playing professional roles. We believe that with the evolution of mainstream LLMs, words with obvious safety risks are easily filtered by their underlying systems, while the real danger lies in the gradual transgression of boundaries during multi-turn interactions, driven by the LLM's attempts at comfort and empathy.
  This paper proposes a multi-turn stress testing framework and conducts long-dialogue safety tests on three cutting-edge LLMs using two pressure methods: static progression and adaptive probing. We generated 50 virtual patient profiles and stress-tested each model through up to 20 rounds of virtual psychiatric dialogues. The experimental results show that violations are common, and both pressure modes produced similar violation rates. However, adaptive probing significantly advanced the time at which models crossed boundaries, reducing the average number of turns from 9.21 in static progression to 4.64. Under both mechanisms, making definitive or zero-risk promises was the primary way in which boundaries were breached. These findings suggest that the robustness of LLM safety boundaries cannot be inferred solely through single-turn tests; it is necessary to fully consider the wear and tear on safety boundaries caused by different interaction pressures and characteristics in extended dialogues.

</details>


### [3] [Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models](https://arxiv.org/abs/2601.14270)
*Liangming Pan,Jason Liang,Jiaran Ye,Minglai Yang,Xinyuan Lu,Fengbin Zhu*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型多步推理的内部机制，而非工程性能提升方法，提出了包含七个研究问题的概念框架，并指出了未来机制研究的五个方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已展现出解决多步推理问题的卓越能力，但其实现这些能力的内部机制仍然不明确。现有研究多关注工程方法提升性能，缺乏对内部机制的系统性理解。

Method: 采用概念框架方法，围绕七个相互关联的研究问题组织综述：从LLM如何在隐藏激活中执行隐式多跳推理，到语言化显式推理如何重塑内部计算过程。

Result: 提出了一个全面的机制分析框架，系统梳理了LLM多步推理的内部工作机制，为理解模型如何实现复杂推理提供了结构化视角。

Conclusion: 该综述填补了LLM多步推理机制研究的空白，为未来机制研究指明了五个重要方向，有助于深入理解语言模型的推理能力本质。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.

</details>


### [4] [Hallucination-Free Automatic Question & Answer Generation for Intuitive Learning](https://arxiv.org/abs/2601.14280)
*Nicholas X. Wang,Aggelos K. Katsaggelos*

Main category: cs.CL

TL;DR: 提出多智能体框架解决LLM生成教育选择题时的幻觉问题，将幻觉率降低90%以上


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育选择题自动生成中产生的幻觉（流畅但不正确或不连贯的输出）是一个重大挑战，阻碍了可靠教育内容的规模化生成

Method: 提出无幻觉多智能体生成框架，将选择题生成分解为离散可验证阶段；使用基于规则和基于LLM的检测智能体以及幻觉评分指标；将生成重新定义为最小化幻觉风险同时最大化有效性、可答性和成本效益的优化任务；引入基于反事实推理和思维链的智能体引导精炼过程

Result: 在AP对齐的STEM问题样本评估中，系统相比基线生成将幻觉率降低了90%以上，同时保持了问题的教育价值和风格

Conclusion: 结构化多智能体协作能够大规模缓解教育内容创建中的幻觉问题，为更可靠的LLM驱动学习工具铺平道路

Abstract: Hallucinations in large language models (LLMs), defined as fluent yet incorrect or incoherent outputs, pose a significant challenge to the automatic generation of educational multiple-choice questions (MCQs). We identified four key hallucination types in MCQ generation: reasoning inconsistencies, insolvability, factual errors, and mathematical errors. To address this, we propose a hallucination-free multi-agent generation framework that breaks down MCQ generation into discrete, verifiable stages. Our framework utilizes both rule-based and LLM-based detection agents, as well as hallucination scoring metrics to optimize question quality. We redefined MCQ generation as an optimization task minimizing hallucination risk while maximizing validity, answerability, and cost-efficiency. We also introduce an agent-led refinement process that uses counterfactual reasoning and chain-of-thought (CoT) to iteratively improve hallucination in question generation. We evaluated a sample of AP- aligned STEM questions, where our system reduced hallucination rates by over 90% compared to baseline generation while preserving the educational value and style of questions. Our results demonstrate that structured multi-agent collaboration can mitigate hallucinations in educational content creation at scale, paving the way for more reliable LLM-powered learning tools.

</details>


### [5] [RPC-Bench: A Fine-grained Benchmark for Research Paper Comprehension](https://arxiv.org/abs/2601.14289)
*Yelin Chen,Fanjin Zhang,Suping Sun,Yunhe Pang,Yuanchun Wang,Jian Song,Xiaoyan Li,Lei Hou,Shu Zhao,Jie Tang,Juanzi Li*

Main category: cs.CL

TL;DR: RPC-Bench是一个基于计算机科学论文审稿-回复交流构建的大规模问答基准，包含15K人工验证的QA对，用于评估模型在学术论文理解方面的能力，特别是对为什么、是什么、如何等问题的理解。


<details>
  <summary>Details</summary>
Motivation: 现有基准在细粒度、大规模评估方面存在局限，而基础模型理解研究论文仍然面临挑战，因为科学论述专业化且图表复杂，需要更精细的评估工具来准确衡量模型对学术论文的理解能力。

Method: 1) 从高质量计算机科学论文的审稿-回复交流中构建大规模问答基准；2) 设计与科学研究流程对齐的细粒度分类法；3) 建立LLM-人类交互标注框架支持大规模标注和质量控制；4) 基于LLM-as-a-Judge范式开发可扩展评估框架，评估正确性-完整性和简洁性。

Result: 实验显示，即使最强模型(GPT-5)在正确性-完整性方面仅达到68.2%，经过简洁性调整后降至37.46%，表明在精确学术论文理解方面存在显著差距。评估框架与人类判断具有高度一致性。

Conclusion: RPC-Bench填补了现有基准的空白，为评估模型对学术论文的理解能力提供了细粒度、大规模的工具，揭示了当前模型在精确学术理解方面的不足，为未来研究提供了重要基准和评估框架。

Abstract: Understanding research papers remains challenging for foundation models due to specialized scientific discourse and complex figures and tables, yet existing benchmarks offer limited fine-grained evaluation at scale. To address this gap, we introduce RPC-Bench, a large-scale question-answering benchmark built from review-rebuttal exchanges of high-quality computer science papers, containing 15K human-verified QA pairs. We design a fine-grained taxonomy aligned with the scientific research flow to assess models' ability to understand and answer why, what, and how questions in scholarly contexts. We also define an elaborate LLM-human interaction annotation framework to support large-scale labeling and quality control. Following the LLM-as-a-Judge paradigm, we develop a scalable framework that evaluates models on correctness-completeness and conciseness, with high agreement to human judgment. Experiments reveal that even the strongest models (GPT-5) achieve only 68.2% correctness-completeness, dropping to 37.46% after conciseness adjustment, highlighting substantial gaps in precise academic paper understanding. Our code and data are available at https://rpc-bench.github.io/.

</details>


### [6] [Project Aletheia: Verifier-Guided Distillation of Backtracking for Small Language Models](https://arxiv.org/abs/2601.14290)
*Aradhya Dixit,Tianxi Liang,Jai Telang*

Main category: cs.CL

TL;DR: 提出验证器引导蒸馏方法，训练7B小语言模型学习错误修复过程，使其具备冲突检测和回溯能力，提升约束满足问题的解决能力


<details>
  <summary>Details</summary>
Motivation: 小语言模型（小于100亿参数）适合私有化、设备端部署，但在严格约束满足问题上表现不佳，主要因为其线性、过度自信的推理轨迹无法从早期错误中恢复

Method: 提出验证器引导蒸馏训练协议，通过训练7B模型学习已验证的推理轨迹（包含错误和自我修正），转移错误修复过程（显式冲突检测和回溯），而非仅学习正确答案

Result: 小模型能够展现出潜在的验证行为，使其能够偶尔停止、检测矛盾并修正早期假设，从而改善约束满足问题的解决能力

Conclusion: 通过转移错误修复过程而非仅最终答案，小语言模型可以学习到更复杂的推理策略，在保持小规模参数的同时提升解决约束满足问题的能力

Abstract: Small Language Models (SLMs, under 10B parameters) are attractive for private, on-device deployment, yet they frequently fail on strict constraint-satisfaction problems due to linear, overconfident reasoning traces that do not recover from early mistakes. We introduce Verifier-Guided Distillation, a training protocol that transfers the process of error repair - explicit conflict detection and backtracking - rather than only correct final answers. By training a 7B model on verified reasoning traces that include mistakes and self-corrections, we show that latent verification behavior can emerge in small models, enabling them to occasionally stop, detect contradictions, and revise earlier assumptions.

</details>


### [7] [Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation with Guided Decoding](https://arxiv.org/abs/2601.14304)
*Juncheng Wang,Zhe Hu,Chao Xu,Siyue Ren,Yuxiang Feng,Yang Liu,Baigui Sun,Shujun Wang*

Main category: cs.CL

TL;DR: AR音频生成模型早期前缀隐含全局语义信息，Plan-Critic通过GAE目标预测生成质量，实现早期轨迹剪枝和计算重分配，提升CLAP分数10分


<details>
  <summary>Details</summary>
Motivation: 自回归模型在生成时序连贯音频方面表现出色，但在遵循复杂文本提示（特别是描述复杂声音事件）方面存在不足。研究发现AR音频生成器的早期前缀令牌隐含编码了最终输出的全局语义属性，这揭示了隐式规划能力，为改进指令跟随能力提供了机会。

Method: 提出Plan-Critic方法：1）发现AR音频生成器早期前缀隐含编码全局语义属性（事件数量、声音对象类别）；2）训练轻量级辅助模型Plan-Critic，采用GAE启发目标从部分生成预测最终指令跟随质量；3）推理时进行引导探索：评估候选前缀、剪枝低质量轨迹、将计算资源重新分配给高潜力规划种子。

Result: Plan-Critic引导采样相比AR基线在CLAP分数上提升高达10分，在AR文本到音频生成领域建立新SOTA，同时计算成本与标准best-of-N解码保持相当。

Conclusion: 该工作弥合了因果生成与全局语义对齐之间的差距，证明即使是严格的自回归模型也能进行前瞻性规划。Plan-Critic通过利用早期前缀的隐式语义信息，显著提升了AR音频生成器的指令跟随能力。

Abstract: Autoregressive (AR) models excel at generating temporally coherent audio by producing tokens sequentially, yet they often falter in faithfully following complex textual prompts, especially those describing complex sound events. We uncover a surprising capability in AR audio generators: their early prefix tokens implicitly encode global semantic attributes of the final output, such as event count and sound-object category, revealing a form of implicit planning. Building on this insight, we propose Plan-Critic, a lightweight auxiliary model trained with a Generalized Advantage Estimation (GAE)-inspired objective to predict final instruction-following quality from partial generations. At inference time, Plan-Critic enables guided exploration: it evaluates candidate prefixes early, prunes low-fidelity trajectories, and reallocates computation to high-potential planning seeds. Our Plan-Critic-guided sampling achieves up to a 10-point improvement in CLAP score over the AR baseline-establishing a new state of the art in AR text-to-audio generation-while maintaining computational parity with standard best-of-N decoding. This work bridges the gap between causal generation and global semantic alignment, demonstrating that even strictly autoregressive models can plan ahead.

</details>


### [8] [Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis](https://arxiv.org/abs/2601.14417)
*Thanathai Lertpetchpun,Yoonjeong Lee,Thanapat Trachu,Jihwan Lee,Tiantian Feng,Dani Byrd,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 该研究分析了说话人嵌入与语言学驱动的音系规则在口音语音合成中的交互作用，提出了一种新的度量指标（音素转换率）来量化嵌入对规则转换的保留或覆盖程度，揭示了口音与说话人身份之间的纠缠关系。


<details>
  <summary>Details</summary>
Motivation: 当前TTS系统通常通过条件化特定口音相关的说话人嵌入来生成带口音的语音，这种方法虽然有效但可解释性和可控性有限，因为嵌入同时编码了音色、情感等多种特征。需要研究如何通过语言学规则实现更精确的口音控制。

Method: 以美式和英式英语为案例研究，实现了flapping（闪音）、rhoticity（卷舌音）和元音对应关系等音系规则。提出了音素转换率（PSR）这一新指标，用于量化说话人嵌入对基于规则的音素转换的保留或覆盖程度。通过实验结合规则与嵌入进行口音语音合成。

Result: 实验表明，结合规则与嵌入能够产生更真实的口音，同时发现嵌入可以减弱或覆盖规则转换，揭示了口音特征与说话人身份特征之间的纠缠关系。音素转换率（PSR）为评估语音生成中的解纠缠提供了框架。

Conclusion: 音系规则可作为口音控制的有效杠杆，同时为评估语音生成模型中的特征解纠缠提供了分析框架。研究揭示了当前TTS系统中口音与说话人身份特征的纠缠问题，为未来开发更具解释性和可控性的口音合成系统提供了方向。

Abstract: Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.

</details>


### [9] [Large Language Models for Large-Scale, Rigorous Qualitative Analysis in Applied Health Services Research](https://arxiv.org/abs/2601.14478)
*Sasha Ronaghi,Emma-Louise Aveling,Maria Levis,Rachel Lauren Ross,Emily Alsentzer,Sara Singer*

Main category: cs.CL

TL;DR: 提出一个模型和任务无关的框架，用于设计人-LLM协作的定性分析方法，并在多站点糖尿病护理研究中应用该框架，展示了LLM如何提升健康服务研究的效率同时保持严谨性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在提升大规模、多站点健康服务研究中定性分析的效率方面显示出潜力，但将LLM整合到定性分析的方法学指导以及它们对真实世界研究方法和结果影响的证据仍然有限。

Method: 开发了一个模型和任务无关的框架，用于设计人-LLM协作的定性分析方法。在联邦合格健康中心（FQHCs）的多站点糖尿病护理研究中，应用该框架实现了两种人-LLM方法：1）对研究者生成的摘要进行定性综合以生成比较反馈报告；2）对167份访谈转录本进行演绎编码以完善实践转化干预。

Result: LLM辅助使得能够及时向从业者提供反馈，并整合大规模定性数据来指导理论和实践变革。该方法在保持严谨性的同时显著提升了研究效率。

Conclusion: 这项工作展示了如何将LLM整合到应用健康服务研究中，在保持严谨性的同时提升效率，为在定性研究中继续创新使用LLM提供了指导。

Abstract: Large language models (LLMs) show promise for improving the efficiency of qualitative analysis in large, multi-site health-services research. Yet methodological guidance for LLM integration into qualitative analysis and evidence of their impact on real-world research methods and outcomes remain limited. We developed a model- and task-agnostic framework for designing human-LLM qualitative analysis methods to support diverse analytic aims. Within a multi-site study of diabetes care at Federally Qualified Health Centers (FQHCs), we leveraged the framework to implement human-LLM methods for (1) qualitative synthesis of researcher-generated summaries to produce comparative feedback reports and (2) deductive coding of 167 interview transcripts to refine a practice-transformation intervention. LLM assistance enabled timely feedback to practitioners and the incorporation of large-scale qualitative data to inform theory and practice changes. This work demonstrates how LLMs can be integrated into applied health-services research to enhance efficiency while preserving rigor, offering guidance for continued innovation with LLMs in qualitative research.

</details>


### [10] [Supporting Humans in Evaluating AI Summaries of Legal Depositions](https://arxiv.org/abs/2601.15182)
*Naghmeh Farzi,Laura Dietz,Dave D. Lewis*

Main category: cs.CL

TL;DR: 该研究将基于事实要点的方法从自动评估扩展到用户端，开发了一个原型系统帮助法律专业人士评估和改进摘要质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于长文档摘要，法律领域面临事实准确性挑战。虽然基于要点的方法已被证明对自动评估摘要方法非常有帮助，但其支持终端用户的潜力尚未充分探索。

Method: 研究将基于事实要点的方法从评估端转换到用户端，开发了一个原型系统，专注于法律领域，支持法律专业人员在两个具体场景中：确定哪个摘要更好，以及手动改进自动生成的摘要。

Result: 提出了一个利用基于事实要点方法的原型系统，能够帮助法律专业人士更有效地评估和改进摘要质量。

Conclusion: 基于事实要点的方法不仅适用于自动评估，还可以直接支持终端用户，特别是在法律领域，能够帮助专业人士提高摘要的事实准确性和质量。

Abstract: While large language models (LLMs) are increasingly used to summarize long documents, this trend poses significant challenges in the legal domain, where the factual accuracy of deposition summaries is crucial. Nugget-based methods have been shown to be extremely helpful for the automated evaluation of summarization approaches. In this work, we translate these methods to the user side and explore how nuggets could directly assist end users. Although prior systems have demonstrated the promise of nugget-based evaluation, its potential to support end users remains underexplored. Focusing on the legal domain, we present a prototype that leverages a factual nugget-based approach to support legal professionals in two concrete scenarios: (1) determining which of two summaries is better, and (2) manually improving an automatically generated summary.

</details>


### [11] [Can LLM Reasoning Be Trusted? A Comparative Study: Using Human Benchmarking on Statistical Tasks](https://arxiv.org/abs/2601.14479)
*Crish Nagarkar,Leonid Bogachev,Serge Sharoff*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型在统计任务中的表现及其评估推理质量的能力，通过微调开源模型提升统计推理能力，发现微调后模型能达到统计学学生水平，且LLMs自身能比传统指标更好地评估答案质量。


<details>
  <summary>Details</summary>
Motivation: 尽管最先进的LLMs在多种NLP任务中表现出色，但其处理中等复杂度统计问题的能力尚不明确。需要研究LLMs的统计推理能力及其评估答案质量的能力，为教育技术和统计分析辅助系统提供基础。

Method: 使用专门开发的数据集对选定的开源LLMs进行微调，以增强其统计推理能力。将微调后模型的性能与人类评分基准进行比较，并评估LLMs自身作为答案质量评估工具的效果。

Result: 微调后的模型在高级统计任务上达到统计学学生水平。微调效果具有架构依赖性，部分模型性能显著提升。LLMs在评估答案质量（包括解释和推理评估）方面远优于BLEU、BertScore等传统指标。

Conclusion: LLMs经过适当微调后具备解决统计问题的能力，且其自我评估能力可用于统计教育平台的自动化评估和自动化分析工具的质量保证。潜在应用包括学术和工业研究方法的验证工具以及数据分析工作流程的质量控制机制。

Abstract: This paper investigates the ability of large language models (LLMs) to solve statistical tasks, as well as their capacity to assess the quality of reasoning. While state-of-the-art LLMs have demonstrated remarkable performance in a range of NLP tasks, their competence in addressing even moderately complex statistical challenges is not well understood. We have fine-tuned selected open-source LLMs on a specially developed dataset to enhance their statistical reasoning capabilities, and compared their performance with the human scores used as a benchmark. Our results show that the fine-tuned models achieve better performance on advanced statistical tasks on the level comparable to a statistics student. Fine-tuning demonstrates architecture-dependent improvements, with some models showing significant performance gains, indicating clear potential for deployment in educational technology and statistical analysis assistance systems. We also show that LLMs themselves can be far better judges of the answers quality (including explanation and reasoning assessment) in comparison to traditional metrics, such as BLEU or BertScore. This self-evaluation capability enables scalable automated assessment for statistical education platforms and quality assurance in automated analysis tools. Potential applications also include validation tools for research methodology in academic and industry settings, and quality control mechanisms for data analysis workflows.

</details>


### [12] [Towards Execution-Grounded Automated AI Research](https://arxiv.org/abs/2601.14525)
*Chenglei Si,Zitong Yang,Yejin Choi,Emmanuel Candès,Diyi Yang,Tatsunori Hashimoto*

Main category: cs.CL

TL;DR: 论文研究了自动化AI研究的可行性，通过构建自动执行器来验证LLM生成的想法，并在LLM预训练和后训练两个实际问题上展示了执行引导的进化搜索比强化学习更有效。


<details>
  <summary>Details</summary>
Motivation: 当前LLM经常生成看似合理但实际无效的想法，需要探索自动化执行的可行性以及LLM是否能从执行反馈中学习，以加速科学发现。

Method: 1) 构建自动执行器来实施想法并启动大规模并行GPU实验验证有效性；2) 将LLM预训练和后训练转化为执行环境；3) 分析两种从执行反馈中学习的方法：进化搜索和强化学习。

Result: 执行引导的进化搜索样本效率高：在后训练上找到显著优于GRPO基线的方法(69.4% vs 48.0%)，在预训练上找到优于nanoGPT基线的配方(19.7分钟 vs 35.9分钟)，均在十个搜索周期内完成。前沿LLM在搜索中能生成有意义的算法想法，但容易早期饱和。强化学习则存在模式崩溃问题。

Conclusion: 自动化AI研究具有可行性，执行引导的进化搜索是有效的学习方法，但需要解决LLM想法饱和和强化学习模式崩溃的问题。论文为基于执行反馈的自动化AI研究提供了深入分析。

Abstract: Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.

</details>


### [13] [Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models](https://arxiv.org/abs/2601.14553)
*Brian Christian,Matan Mazor*

Main category: cs.CL

TL;DR: LLMs在反事实自我模拟中存在类似人类的偏见，但通过访问自身API的盲副本可以实现更公平的决策


<details>
  <summary>Details</summary>
Motivation: 公平决策需要忽略不相关的偏见信息，但人类难以进行反事实自我模拟，导致即使善意行为者也会产生偏见判断。研究探索LLMs是否面临类似限制，以及如何利用其独特能力克服这些偏见。

Method: 研究LLMs在反事实知识下的决策能力，测试提示模型忽略或假装不知道偏见信息的效果，并探索通过访问自身API（盲副本）来模拟反事实认知的方法。

Result: LLMs在抵消性别和种族偏见以及克服奉承行为方面存在类似人类的局限性；提示忽略偏见信息的方法效果有限甚至适得其反；但通过访问自身API的盲副本响应可以实现更公平的决策。

Conclusion: LLMs虽然面临与人类相似的反事实模拟限制，但通过利用其独特的API访问能力，可以创建更公平的决策系统，同时提供区分隐性偏见和故意偏见行为的透明度。

Abstract: Fair decisions require ignoring irrelevant, potentially biasing, information. To achieve this, decision-makers need to approximate what decision they would have made had they not known certain facts, such as the gender or race of a job candidate. This counterfactual self-simulation is notoriously hard for humans, leading to biased judgments even by well-meaning actors. Here we show that large language models (LLMs) suffer from similar limitations in their ability to approximate what decisions they would make under counterfactual knowledge in offsetting gender and race biases and overcoming sycophancy. We show that prompting models to ignore or pretend not to know biasing information fails to offset these biases and occasionally backfires. However, unlike humans, LLMs can be given access to a ground-truth model of their own counterfactual cognition -- their own API. We show that this access to the responses of a blinded replica enables fairer decisions, while providing greater transparency to distinguish implicit from intentionally biased behavior.

</details>


### [14] [Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education](https://arxiv.org/abs/2601.14560)
*Unggi Lee,Jiyeong Bae,Jaehyeon Park,Haeun Park,Taejun Park,Younghoon Jeon,Sungmin Cho,Junbo Koh,Yeil Jeong,Gyeonggeon Lee*

Main category: cs.CL

TL;DR: 提出PedagogicalRL-Thinking框架，通过教学推理提示和思维奖励优化LLM作为智能辅导系统的内部推理过程，提升教育场景下的教学效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为智能辅导系统的研究有限，当前强化学习方法只优化可见响应而忽视内部思维过程，需要专门针对教育场景优化LLM的内部推理能力

Method: 提出PedagogicalRL-Thinking框架：1) 教学推理提示：使用领域特定的教育理论而非通用指令引导内部推理；2) 思维奖励：明确评估和强化模型推理轨迹的教学质量

Result: 领域特定、理论基础的提示优于通用提示；思维奖励与教学提示结合最有效；仅在数学辅导对话上训练的模型在未见过的教育基准上表现提升，同时保留基础模型的事实知识

Conclusion: 教学思维奖励产生系统性的推理轨迹变化，增加教学推理和结构化教学决策，为教育场景优化LLM提供了有效框架

Abstract: Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optimizing visible responses while neglecting the model's internal thinking process. We introduce PedagogicalRL-Thinking, a framework that extends pedagogical alignment to reasoning LLMs in education through two novel approaches: (1) Pedagogical Reasoning Prompting, which guides internal reasoning using domain-specific educational theory rather than generic instructions; and (2) Thinking Reward, which explicitly evaluates and reinforces the pedagogical quality of the model's reasoning traces. Our experiments reveal that domain-specific, theory-grounded prompting outperforms generic prompting, and that Thinking Reward is most effective when combined with pedagogical prompting. Furthermore, models trained only on mathematics tutoring dialogues show improved performance on educational benchmarks not seen during training, while preserving the base model's factual knowledge. Our quantitative and qualitative analyses reveal that pedagogical thinking reward produces systematic reasoning trace changes, with increased pedagogical reasoning and more structured instructional decision-making in the tutor's thinking process.

</details>


### [15] [Social Caption: Evaluating Social Understanding in Multimodal Models](https://arxiv.org/abs/2601.14569)
*Bhaavanaa Thumu,Leena Mathur,Youssouf Kebe,Louis-Philippe Morency*

Main category: cs.CL

TL;DR: Social Caption框架基于互动理论评估多模态大语言模型的社会理解能力，包括社会推理、整体社会分析和定向社会分析三个维度，并分析模型规模、架构设计和口语上下文等因素对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 社会理解能力对多模态大语言模型解释人类社交互动至关重要，但目前缺乏系统评估框架。需要建立基于互动理论的评估体系来全面衡量模型的社会理解能力。

Method: 提出Social Caption框架，基于互动理论从三个维度评估：社会推理（SI）——对互动做出准确推断的能力；整体社会分析（HSA）——生成全面互动描述的能力；定向社会分析（DSA）——从互动中提取相关社会信息的能力。分析模型规模、架构设计和口语上下文等因素的影响。

Result: 通过实验分析发现不同因素对模型社会理解能力的影响，使用MLLM作为评估者提供了关于多模态社会理解自动评估扩展的见解。

Conclusion: Social Caption框架为评估多模态大语言模型的社会理解能力提供了系统方法，揭示了影响模型性能的关键因素，并为自动化评估的扩展提供了重要见解。

Abstract: Social understanding abilities are crucial for multimodal large language models (MLLMs) to interpret human social interactions. We introduce Social Caption, a framework grounded in interaction theory to evaluate social understanding abilities of MLLMs along three dimensions: Social Inference (SI), the ability to make accurate inferences about interactions; Holistic Social Analysis (HSA), the ability to generate comprehensive descriptions of interactions; Directed Social Analysis (DSA), the ability to extract relevant social information from interactions. We analyze factors influencing model performance in social understanding, such as scale, architectural design, and spoken context. Experiments with MLLM judges contribute insights about scaling automated evaluation of multimodal social understanding.

</details>


### [16] [Say Anything but This: When Tokenizer Betrays Reasoning in LLMs](https://arxiv.org/abs/2601.14658)
*Navid Ayoobi,Marcus I Armstrong,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 研究发现大型语言模型的子词分词器存在非唯一编码问题，导致相同文本有多个内部表示，造成推理脆弱性，模型可能将语义相同的文本视为不同"单词"，出现"幻影编辑"等系统性分词器缺陷。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在离散的token ID序列上进行推理，但现代子词分词器经常产生非唯一编码：多个token ID序列可以解码为相同的表面字符串。这种表示不匹配造成了未测量的脆弱性，导致推理过程可能失败。模型可能将两个内部表示视为不同的"单词"，即使它们在文本层面语义相同。

Method: 引入分词一致性探测任务，要求模型在上下文中替换指定的目标词，同时保持其他内容不变。该任务在表面层面故意设计得简单，以便将失败归因于分词器-解码器伪影而非知识差距或参数限制。通过分析超过11000个替换试验，研究最先进的开源LLM。

Result: 发现输出中存在非平凡比例的幻影编辑情况：模型在正确推理的幻觉下操作，这种现象源于分词器诱导的表示缺陷。进一步分析这些案例，提供了八种系统性分词器伪影的分类，包括空格边界偏移和词内重新分割。

Conclusion: 部分明显的推理缺陷源于分词器层，这促使在承担训练更大模型于更大语料库的成本之前，优先考虑分词器层面的补救措施。研究结果表明需要解决分词器引起的表示不一致问题。

Abstract: Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct "words" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.

</details>


### [17] [ClaimDB: A Fact Verification Benchmark over Large Structured Data](https://arxiv.org/abs/2601.14698)
*Michael Theologitis,Preetam Prabhu Srikar Dammu,Chirag Shah,Dan Suciu*

Main category: cs.CL

TL;DR: ClaimDB是首个基于大规模结构化数据的事实核查基准，包含80个真实数据库，要求通过可执行程序进行推理而非简单阅读证据，现有LLM表现不佳且不擅长弃权判断。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查基准主要关注文本证据，而基于大规模结构化数据（数百万记录和多表组合）的声明核查尚未充分探索，需要开发能够处理此类复杂数据的方法。

Method: 构建ClaimDB基准，包含80个独特真实数据库，覆盖治理、医疗、媒体、教育、自然科学等领域。声明证据来自数百万记录和多表组合，要求通过可执行程序进行推理。评估30个最先进的专有和开源LLM（低于700亿参数）。

Result: 所有测试模型准确率不超过83%，超过一半模型低于55%。闭源和开源模型在弃权能力（承认无证据做决定）方面都表现不佳，对其在高风险数据分析中的可靠性提出质疑。

Conclusion: 大规模结构化数据的事实核查需要从"阅读"证据转向可执行程序推理，现有LLM在此任务上表现有限，特别是在弃权能力方面存在显著不足，需要进一步研究改进。

Abstract: Despite substantial progress in fact-verification benchmarks, claims grounded in large-scale structured data remain underexplored. In this work, we introduce ClaimDB, the first fact-verification benchmark where the evidence for claims is derived from compositions of millions of records and multiple tables. ClaimDB consists of 80 unique real-life databases covering a wide range of domains, from governance and healthcare to media, education and the natural sciences. At this scale, verification approaches that rely on "reading" the evidence break down, forcing a timely shift toward reasoning in executable programs. We conduct extensive experiments with 30 state-of-the-art proprietary and open-source (below 70B) LLMs and find that none exceed 83% accuracy, with more than half below 55%. Our analysis also reveals that both closed- and open-source models struggle with abstention -- the ability to admit that there is no evidence to decide -- raising doubts about their reliability in high-stakes data analysis. We release the benchmark, code, and the LLM leaderboard at https://claimdb.github.io .

</details>


### [18] [DARL: Encouraging Diverse Answers for General Reasoning without Verifiers](https://arxiv.org/abs/2601.14700)
*Chongxuan Huang,Lei Lin,Xiaodong Shi,Wenping Hu,Ruiming Tang*

Main category: cs.CL

TL;DR: DARL是一个强化学习框架，通过鼓励在参考答案可控偏差范围内生成多样化答案，解决现有方法过度拟合参考答案的问题，提升推理准确性和输出多样性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励方法（RLVR）依赖领域特定验证器，限制了在开放通用领域的应用。RLPR等扩展方法虽然能在通用领域训练，但存在过度拟合参考答案的问题，限制了模型生成多样化输出的能力，这在开放式任务中尤为明显。

Method: 提出DARL框架，在保持与参考答案对齐的同时，鼓励在可控偏差范围内生成多样化答案。该框架完全兼容现有通用强化学习方法，无需额外验证器即可无缝集成。

Result: 在13个基准测试上进行了广泛实验，在推理性能上取得了一致改进。DARL超越了RLPR，在6个推理基准上平均提升1.3分，在7个通用基准上平均提升9.5分，显著提高了推理准确性和输出多样性。

Conclusion: DARL是一个简单有效的强化学习框架，通过平衡对齐参考答案和鼓励多样化生成，解决了现有方法过度拟合参考答案的问题，在推理和通用任务上都取得了显著改进。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated promising gains in enhancing the reasoning capabilities of large language models. However, its dependence on domain-specific verifiers significantly restricts its applicability to open and general domains. Recent efforts such as RLPR have extended RLVR to general domains, enabling training on broader datasets and achieving improvements over RLVR. However, a notable limitation of these methods is their tendency to overfit to reference answers, which constrains the model's ability to generate diverse outputs. This limitation is particularly pronounced in open-ended tasks such as writing, where multiple plausible answers exist. To address this, we propose DARL, a simple yet effective reinforcement learning framework that encourages the generation of diverse answers within a controlled deviation range from the reference while preserving alignment with it. Our framework is fully compatible with existing general reinforcement learning methods and can be seamlessly integrated without additional verifiers. Extensive experiments on thirteen benchmarks demonstrate consistent improvements in reasoning performance. Notably, DARL surpasses RLPR, achieving average gains of 1.3 points on six reasoning benchmarks and 9.5 points on seven general benchmarks, highlighting its effectiveness in improving both reasoning accuracy and output diversity.

</details>


### [19] [Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning](https://arxiv.org/abs/2601.14750)
*Yifan Wang,Shiyu Li,Peiming Li,Xiaochen Yang,Yang Tang,Zheng Wei*

Main category: cs.CL

TL;DR: Render-of-Thought (RoT) 框架通过将文本推理步骤渲染为图像，实现推理链的可视化和显式化，在保持竞争力的同时实现3-4倍的token压缩和推理加速。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT) 提示在解锁大语言模型推理能力方面取得了显著成功，但其冗长性带来了大量计算开销。现有方法往往只关注结果对齐，缺乏对中间推理过程的监督，导致潜在推理链的可分析性不足。

Method: 提出Render-of-Thought (RoT)框架，首次通过将文本推理步骤渲染为图像来具体化推理链，使潜在推理过程显式化和可追溯。利用现有视觉语言模型(VLMs)的视觉编码器作为语义锚点，将视觉嵌入与文本空间对齐，实现即插即用而无需额外预训练开销。

Result: 在数学和逻辑推理基准测试中，该方法相比显式CoT实现了3-4倍的token压缩和显著的推理加速。同时保持了与其他方法竞争的性能，验证了该范式的可行性。

Conclusion: RoT框架通过将文本推理链渲染为图像，有效解决了CoT冗长性和推理过程不透明的问题，在保持性能的同时实现了显著的效率提升，为推理过程的可视化和分析提供了新范式。

Abstract: Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT

</details>


### [20] [RECAP: Resistance Capture in Text-based Mental Health Counseling with Large Language Models](https://arxiv.org/abs/2601.14780)
*Anqi Li,Yuqian Chen,Yu Lu,Zhaoming Chen,Yuan Xie,Zhenzhong Lan*

Main category: cs.CL

TL;DR: 论文提出PsyFIRE框架和RECAP系统，用于在中文文本心理咨询中检测细粒度抵抗行为，通过两阶段框架实现抵抗检测和分类，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NLP方法在检测心理咨询中的客户抵抗行为时存在三个主要问题：1）过度简化抵抗类别；2）忽略治疗干预的序列动态；3）可解释性有限。这些问题限制了在文本交互中有效识别和应对客户抵抗的能力。

Method: 提出PsyFIRE理论框架，定义了13种细粒度抵抗行为和协作互动。基于此构建ClientResistance语料库（23,930条标注话语，含上下文理由）。开发RECAP两阶段框架：第一阶段区分协作与抵抗，第二阶段分类细粒度抵抗类型并提供解释。

Result: RECAP在区分协作与抵抗方面达到91.25% F1分数，在细粒度抵抗分类方面达到66.58%宏F1分数，比基于提示的LLM基线高出20多个百分点。应用于独立咨询数据集和62名咨询师的试点研究，揭示了抵抗的普遍性及其对治疗关系的负面影响。

Conclusion: PsyFIRE和RECAP框架有效解决了现有方法的局限性，提供了细粒度、可解释的抵抗检测能力，展示了改善咨询师理解和干预策略的潜力，为文本心理咨询中的抵抗识别提供了理论和方法基础。

Abstract: Recognizing and navigating client resistance is critical for effective mental health counseling, yet detecting such behaviors is particularly challenging in text-based interactions. Existing NLP approaches oversimplify resistance categories, ignore the sequential dynamics of therapeutic interventions, and offer limited interpretability.
  To address these limitations, we propose PsyFIRE, a theoretically grounded framework capturing 13 fine-grained resistance behaviors alongside collaborative interactions. Based on PsyFIRE, we construct the ClientResistance corpus with 23,930 annotated utterances from real-world Chinese text-based counseling, each supported by context-specific rationales. Leveraging this dataset, we develop RECAP, a two-stage framework that detects resistance and fine-grained resistance types with explanations.
  RECAP achieves 91.25% F1 for distinguishing collaboration and resistance and 66.58% macro-F1 for fine-grained resistance categories classification, outperforming leading prompt-based LLM baselines by over 20 points. Applied to a separate counseling dataset and a pilot study with 62 counselors, RECAP reveals the prevalence of resistance, its negative impact on therapeutic relationships and demonstrates its potential to improve counselors' understanding and intervention strategies.

</details>


### [21] [Comparative Study of Large Language Models on Chinese Film Script Continuation: An Empirical Analysis Based on GPT-5.2 and Qwen-Max](https://arxiv.org/abs/2601.14826)
*Yuxuan Cao,Zida Yang,Ye Wang*

Main category: cs.CL

TL;DR: 构建首个中文电影剧本续写基准，比较GPT-5.2和Qwen-Max在53部经典电影剧本续写任务上的表现，GPT-5.2在结构保持、整体质量和综合评分上显著优于Qwen-Max。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在创意写作中的应用日益广泛，需要系统评估其在文化特定叙事任务上的表现。目前缺乏针对中文电影剧本续写的系统性评估框架，需要建立可复现的评估方法来比较不同模型在中文创意写作任务上的能力。

Method: 构建包含53部经典中文电影的首个电影剧本续写基准，采用"前半部分到后半部分"的续写范式，每部电影生成3个样本。使用多维评估框架：ROUGE-L、结构相似性和LLM-as-Judge评分（使用DeepSeek-Reasoner）。共获得303个有效样本，对144个配对样本进行统计分析。

Result: Qwen-Max在ROUGE-L上略高（0.2230 vs 0.2114，d=-0.43），但GPT-5.2在结构保持（0.93 vs 0.75，d=0.46）、整体质量（44.79 vs 25.72，d=1.04）和综合评分（0.50 vs 0.39，d=0.84）上显著优于Qwen-Max。GPT-5.2在角色一致性、风格匹配和格式保持方面表现优异，而Qwen-Max在生成稳定性方面存在不足。

Conclusion: GPT-5.2在中文电影剧本续写任务上整体表现优于Qwen-Max，特别是在结构保持和整体质量方面。该研究为中文创意写作的LLM评估提供了可复现的框架，揭示了不同模型在文化特定叙事任务上的性能差异。

Abstract: As large language models (LLMs) are increasingly applied to creative writing, their performance on culturally specific narrative tasks warrants systematic investigation. This study constructs the first Chinese film script continuation benchmark comprising 53 classic films, and designs a multi-dimensional evaluation framework comparing GPT-5.2 and Qwen-Max-Latest. Using a "first half to second half" continuation paradigm with 3 samples per film, we obtained 303 valid samples (GPT-5.2: 157, 98.7% validity; Qwen-Max: 146, 91.8% validity). Evaluation integrates ROUGE-L, Structural Similarity, and LLM-as-Judge scoring (DeepSeek-Reasoner).
  Statistical analysis of 144 paired samples reveals: Qwen-Max achieves marginally higher ROUGE-L (0.2230 vs 0.2114, d=-0.43); however, GPT-5.2 significantly outperforms in structural preservation (0.93 vs 0.75, d=0.46), overall quality (44.79 vs 25.72, d=1.04), and composite scores (0.50 vs 0.39, d=0.84). The overall quality effect size reaches large effect level (d>0.8).
  GPT-5.2 excels in character consistency, tone-style matching, and format preservation, while Qwen-Max shows deficiencies in generation stability. This study provides a reproducible framework for LLM evaluation in Chinese creative writing.

</details>


### [22] [Language-Coupled Reinforcement Learning for Multilingual Retrieval-Augmented Generation](https://arxiv.org/abs/2601.14896)
*Rui Qi,Fengran Mo,Yufeng Chen,Xue Zhang,Shuo Wang,Hongliang Li,Jinan Xu,Meng Jiang,Jian-Yun Nie,Kaiyu Huang*

Main category: cs.CL

TL;DR: LcRL是一个多语言检索增强强化学习框架，通过语言耦合组相对策略优化和辅助反一致性惩罚，解决多语言检索增强生成中的知识偏差和冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有MRAG方法采用统一的"一刀切"策略，在不同语言查询处理中容易产生知识偏差和冲突，导致多语言环境下性能不佳。

Method: 提出LcRL框架，集成语言耦合组相对策略优化到策略和奖励模型中，采用语言耦合组采样减少知识偏差，并引入辅助反一致性惩罚缓解知识冲突。

Result: 实验结果表明LcRL不仅取得有竞争力的性能，而且适用于多种实际场景，如训练数据受限和检索包含大量语言的文档集合。

Conclusion: LcRL通过解决知识偏差和冲突问题，有效提升了多语言检索增强生成的性能，为多语言场景提供了更优的解决方案。

Abstract: Multilingual retrieval-augmented generation (MRAG) requires models to effectively acquire and integrate beneficial external knowledge from multilingual collections. However, most existing studies employ a unitive process where queries of equivalent semantics across different languages are processed through a single-turn retrieval and subsequent optimization. Such a ``one-size-fits-all'' strategy is often suboptimal in multilingual settings, as the models occur to knowledge bias and conflict during the interaction with the search engine. To alleviate the issues, we propose LcRL, a multilingual search-augmented reinforcement learning framework that integrates a language-coupled Group Relative Policy Optimization into the policy and reward models. We adopt the language-coupled group sampling in the rollout module to reduce knowledge bias, and regularize an auxiliary anti-consistency penalty in the reward models to mitigate the knowledge conflict. Experimental results demonstrate that LcRL not only achieves competitive performance but is also appropriate for various practical scenarios such as constrained training data and retrieval over collections encompassing a large number of languages. Our code is available at https://github.com/Cherry-qwq/LcRL-Open.

</details>


### [23] [PodBench: A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation](https://arxiv.org/abs/2601.14903)
*Chenning Xu,Mao Zheng,Mingyu Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: PodBench：一个包含800个样本的播客脚本生成基准测试，输入最长21K tokens，包含复杂多说话者指令，提供多维度评估框架


<details>
  <summary>Details</summary>
Motivation: 播客脚本生成需要LLMs从多样化输入中合成结构化、上下文接地的对话，但目前缺乏系统性的评估资源来支持这一任务

Method: 引入PodBench基准测试，包含800个样本，输入最长21K tokens，具有复杂多说话者指令；提出整合定量约束和基于LLM质量评估的多维度评估框架

Result: 实验表明：专有模型整体表现优异，但配备显式推理能力的开源模型在处理长上下文和多说话者协调方面比标准基线更具鲁棒性；分析发现指令遵循度高并不保证内容实质质量高

Conclusion: PodBench为长格式、音频中心生成任务中的挑战提供了可复现的测试平台，揭示了指令遵循与内容质量之间的持续分歧

Abstract: Podcast script generation requires LLMs to synthesize structured, context-grounded dialogue from diverse inputs, yet systematic evaluation resources for this task remain limited. To bridge this gap, we introduce PodBench, a benchmark comprising 800 samples with inputs up to 21K tokens and complex multi-speaker instructions. We propose a multifaceted evaluation framework that integrates quantitative constraints with LLM-based quality assessment. Extensive experiments reveal that while proprietary models generally excel, open-source models equipped with explicit reasoning demonstrate superior robustness in handling long contexts and multi-speaker coordination compared to standard baselines. However, our analysis uncovers a persistent divergence where high instruction following does not guarantee high content substance. PodBench offers a reproducible testbed to address these challenges in long-form, audio-centric generation.

</details>


### [24] [CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents](https://arxiv.org/abs/2601.14914)
*Tianxiang Fei,Cheng Chen,Yue Pan,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: CodeDelegator：一个通过角色专业化将规划与实现分离的多智能体框架，使用持久化委托者和临时编码者，通过EPSS机制隔离执行状态，防止调试痕迹污染上下文。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型允许将动作表示为可执行代码，比传统工具调用更具表达性。但现实任务需要战略规划和详细实现，单一智能体同时处理两者会导致调试痕迹和中间失败污染上下文，损害长视野性能。

Method: 提出CodeDelegator多智能体框架：1) 持久化委托者负责战略监督、任务分解、编写规范和监控进度但不执行代码；2) 每个子任务实例化新的编码者智能体，其上下文仅包含规范，隔离先前失败；3) 引入短暂-持久状态分离(EPSS)机制，隔离每个编码者的执行状态同时保持全局一致性。

Result: 在多个基准测试上的实验证明了CodeDelegator在不同场景中的有效性。

Conclusion: 通过角色专业化和状态分离，CodeDelegator解决了单一智能体在复杂任务中上下文污染的问题，提高了长视野任务的性能。

Abstract: Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.

</details>


### [25] [The GDN-CC Dataset: Automatic Corpus Clarification for AI-enhanced Democratic Citizen Consultations](https://arxiv.org/abs/2601.14944)
*Pierre-Antoine Lequeu,Léo Labat,Laurène Cave,Gaël Lejeune,François Yvon,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 论文提出Corpus Clarification框架，用于预处理大规模公民协商数据，将其转化为结构化论证单元，并展示了小型开源LLM在标准化公民贡献方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM广泛应用于NLP领域，但在民主活动（如在线审议、大规模公民协商）中使用LLM作为分析工具引发了伦理问题。研究旨在：(a)开发资源标准化公民贡献的语用层面，使其更易于主题建模和政治分析；(b)研究小型开源LLM在标准化任务上的可靠性。

Method: 提出Corpus Clarification预处理框架，将嘈杂的多主题贡献转化为结构化、自包含的论证单元。创建GDN-CC数据集（1,231个法国Grand Débat National贡献，含2,285个论证单元，人工标注论证结构和澄清）。通过微调小型语言模型进行标注，并与大型LLM比较性能。

Result: 微调的小型语言模型在再现人工标注方面匹配或优于大型LLM。在意见聚类任务中验证了其可用性。发布了GDN-CC-large数据集（24万自动标注贡献），是目前最大的标注民主协商数据集。

Conclusion: Corpus Clarification框架能有效预处理公民协商数据，小型开源LLM在标准化任务上表现优异，为民主协商分析提供了可本地运行、透明的解决方案，并贡献了大规模标注数据集。

Abstract: LLMs are ubiquitous in modern NLP, and while their applicability extends to texts produced for democratic activities such as online deliberations or large-scale citizen consultations, ethical questions have been raised for their usage as analysis tools. We continue this line of research with two main goals: (a) to develop resources that can help standardize citizen contributions in public forums at the pragmatic level, and make them easier to use in topic modeling and political analysis; (b) to study how well this standardization can reliably be performed by small, open-weights LLMs, i.e. models that can be run locally and transparently with limited resources. Accordingly, we introduce Corpus Clarification as a preprocessing framework for large-scale consultation data that transforms noisy, multi-topic contributions into structured, self-contained argumentative units ready for downstream analysis. We present GDN-CC, a manually-curated dataset of 1,231 contributions to the French Grand Débat National, comprising 2,285 argumentative units annotated for argumentative structure and manually clarified. We then show that finetuned Small Language Models match or outperform LLMs on reproducing these annotations, and measure their usability for an opinion clustering task. We finally release GDN-CC-large, an automatically annotated corpus of 240k contributions, the largest annotated democratic consultation dataset to date.

</details>


### [26] [A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala](https://arxiv.org/abs/2601.14958)
*Minuri Rajapakse,Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: 评估现代语言模型在僧伽罗语（Unicode和罗马化）上的性能，发现不同模型在不同脚本上表现差异显著


<details>
  <summary>Details</summary>
Motivation: 僧伽罗语等低资源、形态丰富的语言在语言模型中的性能研究不足，特别是数字通信中普遍使用的罗马化僧伽罗语，需要建立全面的基准评估

Method: 使用多样化的Unicode和罗马化僧伽罗语语料库，通过困惑度评估开源模型，通过句子补全的定性分析评估领先的闭源模型

Result: Mistral-Nemo-Base-2407在Unicode文本上预测性能最强，Mistral-7B-v0.3在罗马化文本上表现最佳；Llama-3.1-8B在两种脚本上均有良好表现；闭源模型中，Gemini-1.5-pro和DeepSeek擅长Unicode生成，Claude-3.5-Sonnet在罗马化文本处理上更优

Conclusion: 结果为僧伽罗语特定应用选择模型提供了重要指导，并强调了训练数据在处理脚本变体中的关键作用

Abstract: The performance of Language Models (LMs) on lower-resource, morphologically rich languages like Sinhala remains under-explored, particularly for Romanized Sinhala, which is prevalent in digital communication. This paper presents a comprehensive benchmark of modern LMs on a diverse corpus of Unicode and Romanized Sinhala. We evaluate open-source models using perplexity, a measure of how well a model predicts a text, and leading closed-source models via a qualitative analysis of sentence completion. Our findings reveal that the Mistral-Nemo-Base-2407 model achieves the strongest predictive performance on Unicode text and the Mistral-7B-v0.3 model for Romanized text. The results also highlight the strong all-around performance of the Llama-3.1-8B model for both scripts. Furthermore, a significant performance disparity exists among closed-source models: Gemini-1.5-pro and DeepSeek excel at Unicode generation, whereas Claude-3.5-Sonnet is superior at handling Romanized text. These results provide an essential guide for practitioners selecting models for Sinhala-specific applications and highlight the critical role of training data in handling script variations.

</details>


### [27] [Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora](https://arxiv.org/abs/2601.14994)
*Chaymaa Abbas,Nour Shamaa,Mariette Awad*

Main category: cs.CL

TL;DR: 该研究探讨了多语言环境下LLM的数据污染问题，发现翻译到阿拉伯语会抑制传统污染检测指标，但模型仍能从污染数据中获益，提出了基于多翻译变体比较的翻译感知污染检测方法。


<details>
  <summary>Details</summary>
Motivation: 数据污染会破坏大型语言模型评估的有效性，使模型依赖记忆而非真正泛化。现有污染检测方法主要局限于英语基准测试，对多语言污染的理解不足，需要研究多语言环境下的污染动态。

Method: 通过在阿拉伯语数据集上微调多个开源LLM，并在原始英语基准上评估。扩展了Tested Slot Guessing方法，加入选择重排序策略，并整合Min-K%概率分析，捕捉行为和分布污染信号。提出了翻译感知污染检测，通过比较多个翻译基准变体的信号来识别污染。

Result: 翻译到阿拉伯语会抑制传统污染指标，但模型仍能从污染数据中获益，特别是阿拉伯语能力更强的模型。随着污染水平增加，Min-K%分数上升，跨语言答案一致性提高。翻译感知污染检测即使在纯英语方法失败时也能可靠地暴露污染。

Conclusion: 需要多语言、翻译感知的评估流程来确保LLM评估的公平性、透明性和可重复性。翻译感知污染检测方法能有效识别传统方法无法检测的多语言污染。

Abstract: Data contamination undermines the validity of Large Language Model evaluation by enabling models to rely on memorized benchmark content rather than true generalization. While prior work has proposed contamination detection methods, these approaches are largely limited to English benchmarks, leaving multilingual contamination poorly understood. In this work, we investigate contamination dynamics in multilingual settings by fine-tuning several open-weight LLMs on varying proportions of Arabic datasets and evaluating them on original English benchmarks. To detect memorization, we extend the Tested Slot Guessing method with a choice-reordering strategy and incorporate Min-K% probability analysis, capturing both behavioral and distributional contamination signals.
  Our results show that translation into Arabic suppresses conventional contamination indicators, yet models still benefit from exposure to contaminated data, particularly those with stronger Arabic capabilities. This effect is consistently reflected in rising Mink% scores and increased cross-lingual answer consistency as contamination levels grow. To address this blind spot, we propose Translation-Aware Contamination Detection, which identifies contamination by comparing signals across multiple translated benchmark variants rather than English alone. The Translation-Aware Contamination Detection reliably exposes contamination even when English-only methods fail. Together, our findings highlight the need for multilingual, translation-aware evaluation pipelines to ensure fair, transparent, and reproducible assessment of LLMs.

</details>


### [28] [Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction](https://arxiv.org/abs/2601.15037)
*Xiaonan Jing,Gongqing Wu,Xingrui Zhuo,Lang Sun,Jiapu Wang*

Main category: cs.CL

TL;DR: KRPO框架通过知识重构驱动的提示优化，结合自评估机制和文本梯度优化器，提升LLM在开放域关系三元组抽取任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的开放域关系三元组抽取方法依赖静态启发式提示策略，缺乏反思机制来内化错误信号，导致语义模糊性下的脆弱性和错误抽取模式的固化

Method: 提出知识重构驱动的提示优化框架：1) 基于知识恢复的自评估机制，将结构化三元组投影为语义一致性分数；2) 基于文本梯度的提示优化器，内化历史经验迭代优化提示；3) 关系规范化记忆，收集代表性关系提供语义区分模式

Result: 在三个数据集上的广泛实验表明，KRPO在抽取F1分数上显著优于强基线方法

Conclusion: KRPO框架通过知识重构驱动的自评估和提示优化，有效解决了LLM在开放域关系三元组抽取中的语义模糊性和关系冗余问题，提升了抽取能力

Abstract: Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowledge Reconstruction-driven Prompt Optimization (KRPO) framework to assist LLMs in continuously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Extensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score.

</details>


### [29] [LogicScore: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering](https://arxiv.org/abs/2601.15050)
*Zhichao Yan,Yunxiao Zhao,Jiapu Wang,Jiaoyan Chen,Shaoru Guo,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: LogicScore：一个基于Horn规则的评估框架，用于评估长格式问答中的全局逻辑完整性，弥补现有方法只关注孤立语句归因验证的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前归因问答评估方法存在"归因短视"问题，过度强调孤立语句及其归因的验证，而忽略了长格式答案的全局逻辑完整性。这导致大语言模型经常产生事实基础正确但逻辑不连贯的响应，存在难以察觉的推理漏洞。

Method: 提出LogicScore评估框架，基于Horn规则，采用后向验证机制，系统评估三个关键推理维度：完整性（逻辑合理的演绎）、简洁性（非冗余性）和确定性（一致的答案蕴含）。

Result: 在三个多跳QA数据集（HotpotQA、MusiQue和2WikiMultiHopQA）和20多个LLM上的实验显示关键能力差距：领先模型在归因分数上表现良好（如Gemini-3 Pro达到92.85%精确率），但在全局推理质量上表现不佳（如Gemini-3 Pro的简洁性仅为35.11%）。

Conclusion: LogicScore为逻辑评估建立了稳健标准，强调在LLM开发中需要同时优先考虑推理连贯性和事实基础性，揭示了当前模型在全局逻辑推理方面的不足。

Abstract: Current evaluation methods for Attributed Question Answering (AQA) suffer from \textit{attribution myopia}: they emphasize verification of isolated statements and their attributions but overlook the global logical integrity of long-form answers. Consequently, Large Language Models (LLMs) often produce factually grounded yet logically incoherent responses with elusive deductive gaps. To mitigate this limitation, we present \textsc{LogicScore}, a unified evaluation framework that shifts the paradigm from local assessment to global reasoning scrutiny. Grounded in Horn Rules, our approach integrates a backward verification mechanism to systematically evaluate three key reasoning dimensions: \textit{Completeness} (logically sound deduction), \textit{Conciseness} (non-redundancy), and \textit{Determinateness} (consistent answer entailment). Extensive experiments across three multi-hop QA datasets (HotpotQA, MusiQue, and 2WikiMultiHopQA) and over 20 LLMs (including GPT-5, Gemini-3-Pro, LLaMA3, and task-specific tuned models) reveal a critical capability gap: leading models often achieve high attribution scores (e.g., 92.85\% precision for Gemini-3 Pro) but struggle with global reasoning quality (e.g., 35.11\% Conciseness for Gemini-3 Pro). Our work establishes a robust standard for logical evaluation, highlighting the need to prioritize reasoning coherence alongside factual grounding in LLM development. Codes are available at: https://github.com/zhichaoyan11/LogicScore.

</details>


### [30] [Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure](https://arxiv.org/abs/2601.15077)
*Christopher Scofield*

Main category: cs.CL

TL;DR: 多智能体系统通过分解约束执行算子实现性能提升，即使智能体拥有相同信息


<details>
  <summary>Details</summary>
Motivation: 解释为什么由大语言模型组成的多智能体系统在拥有相同信息的情况下仍能表现出更好的问题解决性能

Method: 基于算子理论和约束优化建立形式化框架，将每个智能体建模为在共享解状态上执行不同有效性约束的算子，多智能体系统实现约束执行算子的分解组合

Result: 在温和条件下，多智能体动力学收敛到由智能体约束集交集定义的不变解集，这些结构通常无法被单个智能体动态访问，即使其表达能力和信息相同

Conclusion: 多智能体系统通过分解约束执行实现性能提升，该框架可扩展到软约束和当代基于文本的对话系统

Abstract: Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.

</details>


### [31] [Circadian Modulation of Semantic Exploration in Social Media Language](https://arxiv.org/abs/2601.15091)
*Vuong Hung Truong,Mariana Gabrielle Cangco Reyes,Masatoshi Koizumi,Jihwan Myung*

Main category: cs.CL

TL;DR: 研究发现人类语义行为存在昼夜节律：早晨语义探索性高（局部熵峰值），下午语义多样性增加（全局熵峰值），这与神经调节系统节律一致，且不受情绪影响


<details>
  <summary>Details</summary>
Motivation: 人类认知受昼夜节律调节，但对其在高维语义行为中的影响了解不足。研究旨在量化语言使用的时间变化，探索语义探索-利用平衡的昼夜节律模式

Method: 使用大规模Reddit数据，通过预训练transformer模型将文本嵌入语义空间，测量语义熵作为语言探索-利用指标，区分局部和全局语义熵，分析昼夜和季节性变化

Result: 发现语义熵存在稳健的昼夜节律性，可被季节性光照线索调节。局部语义探索在早晨达到峰值，全局语义多样性在下午达到峰值，呈现"富者愈富"动态。这些模式不受情绪或情感效价影响

Conclusion: 语义探索表现出与已知神经调节系统昼夜模式一致的节律结构，表明生物昼夜节律延伸到语义领域，语义探索捕捉了独立于情绪的认知维度

Abstract: Human cognition exhibits strong circadian modulation, yet its influence on high-dimensional semantic behavior remains poorly understood. Using large-scale Reddit data, we quantify time-of-day variation in language use by embedding text into a pretrained transformer model and measuring semantic entropy as an index of linguistic exploration-exploitation, for which we show a robust circadian rhythmicity that could be entrained by seasonal light cues. Distinguishing between local and global semantic entropy reveals a systematic temporal dissociation: local semantic exploration peaks in the morning, reflecting broader exploration of semantic space, whereas global semantic diversity peaks later in the day as submissions accumulate around already established topics, consistent with "rich-get-richer" dynamics. These patterns are not explained by sentiment or affective valence, indicating that semantic exploration captures a cognitive dimension distinct from mood. The observed temporal structure aligns with known diurnal patterns in neuromodulatory systems, suggesting that biological circadian rhythms extend to the semantic domain.

</details>


### [32] [RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)](https://arxiv.org/abs/2601.15129)
*Yishu Wei,Adam E. Flanders,Errol Colak,John Mongan,Luciano M Prevedello,Po-Hao Chen,Henrique Min Ho Lee,Gilberto Szarf,Hamilton Shoji,Jason Sho,Katherine Andriole,Tessa Cook,Lisa C. Adams,Linda C. Chu,Maggie Chung,Geraldine Brusca-Augello,Djeven P. Deva,Navneet Singh,Felipe Sanchez Tijmes,Jeffrey B. Alpert,Elsie T. Nguyen,Drew A. Torigian,Kate Hanneman,Lauren K Groner,Alexander Phan,Ali Islam,Matias F. Callejas,Gustavo Borges da Silva Teles,Faisal Jamal,Maryam Vazirabad,Ali Tejani,Hari Trivedi,Paulo Kuriki,Rajesh Bhayana,Elana T. Benishay,Yi Lin,Yifan Peng,George Shih*

Main category: cs.CL

TL;DR: 开发了一个包含200张胸部X光片的基准数据集，采用AI辅助专家标注流程，每个影像由三位放射科医生验证，用于评估多模态大语言模型的临床性能。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在放射学选择题考试中表现与放射科实习生相当，但开发临床有用的工具需要由领域专家策划的高质量基准数据集。现有数据集缺乏专家验证，限制了模型的临床适用性评估。

Method: 1. 使用MIDRC的13,735张去标识化胸部X光片及报告；2. GPT-4o提取异常发现，Phi-4-Reasoning映射到12个基准标签；3. 基于AI建议标签采样1,000张进行专家评审；4. 17位胸部放射科医生参与，每位影像由三位专家评估；5. 从至少两位专家"完全同意"的381张中选出200张，优先选择罕见或多发现标签；6. 分为100张公开数据集和100张保留测试集。

Result: 创建了包含200张胸部X光片的基准数据集，每张影像由三位放射科医生验证，包含12个基准标签。开发了AI辅助标注流程，使放射科医生能高效标注，减少遗漏，支持半协作环境。数据集在https://imaging.rsna.org公开，保留测试集由RSNA独立用于模型评估。

Conclusion: 成功开发了经过专家验证的胸部X光片基准数据集和AI辅助标注流程，为评估多模态大语言模型的临床性能提供了高质量标准，有助于推动临床有用AI工具的开发。

Abstract: Multimodal large language models have demonstrated comparable performance to that of radiology trainees on multiple-choice board-style exams. However, to develop clinically useful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential. To curate released and holdout datasets of 100 chest radiographic studies each and propose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to label studies more efficiently. A total of 13,735 deidentified chest radiographs and their corresponding reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning). From these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for expert review; the sampling algorithm ensured that the selected studies were clinically relevant and captured a range of difficulty levels. Seventeen chest radiologists participated, and they marked "Agree all", "Agree mostly" or "Disagree" to indicate their assessment of the correctness of the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at least two radiologists selected "Agree All" for 381 radiographs. From this set, 200 were selected, prioritizing those with less common or multiple finding labels, and divided into 100 released radiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by RSNA to independently evaluate different models. A benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made publicly available https://imaging.rsna.org, with each chest radiograph verified by three radiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment.

</details>


### [33] [Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems](https://arxiv.org/abs/2601.15161)
*Yinzhu Chen,Abdine Maiga,Hossein A. Rahmani,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出检索增强的多智能体框架，自动化生成针对特定实例的评估标准，用于评估和改进医疗大语言模型，显著提升临床意图对齐和评估质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在临床决策支持中的应用日益增多，但幻觉和不安全建议可能直接威胁患者安全。这些风险表现为难以被通用指标检测的细微临床错误，而专家制定的细粒度评估标准成本高昂且难以扩展。

Method: 提出检索增强的多智能体框架，通过分解检索到的权威医学证据为原子事实，结合用户交互约束，合成可验证的细粒度评估标准。

Result: 在HealthBench上评估，临床意图对齐得分60.12%，显著优于GPT-4o基线（55.16%）。在判别性测试中，平均得分差异μΔ=8.658，AUROC为0.977，几乎是GPT-4o基线（4.972）的两倍。评估标准还能指导响应优化，质量提升9.2%（从59.0%到68.2%）。

Conclusion: 该框架为评估和改进医疗大语言模型提供了可扩展且透明的基础，通过自动化生成细粒度评估标准，有效解决了医疗领域幻觉检测和安全性评估的挑战。

Abstract: Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($μ_Δ = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.

</details>


### [34] [Is Peer Review Really in Decline? Analyzing Review Quality across Venues and Time](https://arxiv.org/abs/2601.15172)
*Ilia Kuznetsov,Rohan Nayak,Alla Rozovskaya,Iryna Gurevych*

Main category: cs.CL

TL;DR: 该研究开发了一个基于证据的框架来评估同行评审质量，应用于主要AI会议，发现评审质量并未如普遍担忧的那样持续下降。


<details>
  <summary>Details</summary>
Motivation: 随着投稿量增加和研究社区扩大，评审质量下降成为普遍担忧，但缺乏有效的测量和比较方法。需要建立证据基础来验证这一叙事。

Method: 引入新的评审质量比较框架，包括评审格式标准化、多维质量量化模式（对编辑和作者的效用），结合LLM和轻量级测量方法，分析ICLR、NeurIPS和*ACL会议数据。

Result: 跨时间分析显示，各会议和年份的中位评审质量没有一致下降趋势，与普遍叙事相矛盾。揭示了评审质量测量的复杂关系。

Conclusion: 同行评审质量并未普遍下降，需要更精细的实证研究。提出了促进未来评审质量研究的建议和替代解释。

Abstract: Peer review is at the heart of modern science. As submission numbers rise and research communities grow, the decline in review quality is a popular narrative and a common concern. Yet, is it true? Review quality is difficult to measure, and the ongoing evolution of reviewing practices makes it hard to compare reviews across venues and time. To address this, we introduce a new framework for evidence-based comparative study of review quality and apply it to major AI and machine learning conferences: ICLR, NeurIPS and *ACL. We document the diversity of review formats and introduce a new approach to review standardization. We propose a multi-dimensional schema for quantifying review quality as utility to editors and authors, coupled with both LLM-based and lightweight measurements. We study the relationships between measurements of review quality, and its evolution over time. Contradicting the popular narrative, our cross-temporal analysis reveals no consistent decline in median review quality across venues and years. We propose alternative explanations, and outline recommendations to facilitate future empirical studies of review quality.

</details>


### [35] [Metadata Conditioned Large Language Models for Localization](https://arxiv.org/abs/2601.15236)
*Anjishnu Mukherjee,Ziwei Zhu,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 该论文提出了一种通过元数据条件化实现语言模型本地化的轻量级方法，在英语新闻数据上预训练31个模型，证明该方法能提升区域内性能而不损害跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常将文本视为单一全局分布进行训练，导致地理行为同质化。需要研究轻量级本地化方法，使模型能够适应不同地理区域的特点。

Method: 使用带有已验证URL、国家标签和大陆标签的大规模英语新闻数据，从头预训练31个模型（0.5B和1B参数规模），覆盖4个大陆和17个国家。通过四个对照实验评估元数据条件化的效果，并进行消融研究分析URL级元数据和平衡区域数据覆盖的重要性。

Result: 元数据条件化能持续提升区域内性能而不牺牲跨区域泛化能力；使全局模型能够达到与区域特定模型相当的本地化水平；提高学习效率。URL级元数据单独就能捕获大部分地理信号，但平衡的区域数据覆盖仍然至关重要。经过指令微调后，元数据条件化的全局模型在800个本地化新闻多选题基准测试中达到与LLaMA-3.2-1B-Instruct相当的准确率，尽管训练数据量少得多。

Conclusion: 元数据条件化是一种实用且计算高效的语音模型本地化方法，能够在保持全局泛化能力的同时实现有效的区域适应。

Abstract: Large language models are typically trained by treating text as a single global distribution, often resulting in geographically homogenized behavior. We study metadata conditioning as a lightweight approach for localization, pre-training 31 models (at 0.5B and 1B parameter scales) from scratch on large-scale English news data annotated with verified URLs, country tags, and continent tags, covering 4 continents and 17 countries. Across four controlled experiments, we show that metadata conditioning consistently improves in-region performance without sacrificing cross-region generalization, enables global models to recover localization comparable to region-specific models, and improves learning efficiency. Our ablation studies demonstrate that URL-level metadata alone captures much of the geographic signal, while balanced regional data coverage remains essential, as metadata cannot fully compensate for missing regions. Finally, we introduce a downstream benchmark of 800 localized news MCQs and show that after instruction tuning, metadata conditioned global models achieve accuracy comparable to LLaMA-3.2-1B-Instruct, despite being trained on substantially less data. Together, these results establish metadata conditioning as a practical and compute-efficient approach for localization of language models.

</details>


### [36] [The Effect of Scripts and Formats on LLM Numeracy](https://arxiv.org/abs/2601.15251)
*Varshini Reddy,Craig W. Schmidt,Seth Ebner,Adam Wiemerslage,Yuval Pinter,Chris Tanner*

Main category: cs.CL

TL;DR: LLMs在标准数字任务上表现优异，但当数字表达方式偏离训练数据中的主流格式时，准确率显著下降，通过针对性提示策略可以缩小这一差距。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在不同数字脚本和格式下的数值推理能力，关注当数字表达方式偏离训练数据中的主流约定时模型的表现，揭示多语言数值推理中被忽视的挑战。

Method: 在广泛的数字脚本和格式范围内测试LLMs的数值推理能力，评估模型在非主流数字表达方式下的表现，并测试针对性提示策略（如few-shot prompting和显式数字映射）的效果。

Result: LLMs在标准数字任务上达到接近人类水平的性能，但当数字输入以训练数据中较少见的脚本或格式呈现时，准确率大幅下降；通过few-shot prompting和显式数字映射等针对性提示策略，可以显著缩小这一性能差距。

Conclusion: LLMs在数值推理中存在对数字表达格式的敏感性，多语言数值推理是一个被忽视的挑战；通过适当的提示策略可以改善模型在不同数字脚本和格式下的可靠性，为跨语言数字解释、操作和生成提供了实用见解。

Abstract: Large language models (LLMs) have achieved impressive proficiency in basic arithmetic, rivaling human-level performance on standard numerical tasks. However, little attention has been given to how these models perform when numerical expressions deviate from the prevailing conventions present in their training corpora. In this work, we investigate numerical reasoning across a wide range of numeral scripts and formats. We show that LLM accuracy drops substantially when numerical inputs are rendered in underrepresented scripts or formats, despite the underlying mathematical reasoning being identical. We further demonstrate that targeted prompting strategies, such as few-shot prompting and explicit numeral mapping, can greatly narrow this gap. Our findings highlight an overlooked challenge in multilingual numerical reasoning and provide actionable insights for working with LLMs to reliably interpret, manipulate, and generate numbers across diverse numeral scripts and formatting styles.

</details>


### [37] [Robust Fake News Detection using Large Language Models under Adversarial Sentiment Attacks](https://arxiv.org/abs/2601.15277)
*Sahar Tahmasebi,Eric Müller-Budack,Ralph Ewerth*

Main category: cs.CL

TL;DR: 该论文提出AdSent框架，通过对抗性情感操纵攻击评估假新闻检测器的脆弱性，并引入情感无关训练策略提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法依赖情感特征，但随着大语言模型的发展，攻击者可以操纵情感来规避检测。当前研究主要关注新闻发布者风格特征，而情感操纵这一关键漏洞尚未充分探索。

Method: 提出AdSent框架：1) 使用LLM生成受控的情感对抗攻击样本；2) 分析情感变化对检测性能的影响；3) 引入新颖的情感无关训练策略，增强对情感扰动的鲁棒性。

Result: 实验表明，情感变化严重影响假新闻检测模型性能，模型偏向于将中性文章分类为真实新闻，非中性文章则常被误判为虚假内容。AdSent在三个基准数据集上显著优于基线方法，在准确性和鲁棒性方面表现优异，并能有效泛化到未见数据集和对抗场景。

Conclusion: 假新闻检测器对情感操纵具有显著脆弱性，AdSent框架通过情感无关训练策略有效提升了检测模型的鲁棒性，为构建更可靠的假新闻检测系统提供了重要方法。

Abstract: Misinformation and fake news have become a pressing societal challenge, driving the need for reliable automated detection methods. Prior research has highlighted sentiment as an important signal in fake news detection, either by analyzing which sentiments are associated with fake news or by using sentiment and emotion features for classification. However, this poses a vulnerability since adversaries can manipulate sentiment to evade detectors especially with the advent of large language models (LLMs). A few studies have explored adversarial samples generated by LLMs, but they mainly focus on stylistic features such as writing style of news publishers. Thus, the crucial vulnerability of sentiment manipulation remains largely unexplored. In this paper, we investigate the robustness of state-of-the-art fake news detectors under sentiment manipulation. We introduce AdSent, a sentiment-robust detection framework designed to ensure consistent veracity predictions across both original and sentiment-altered news articles. Specifically, we (1) propose controlled sentiment-based adversarial attacks using LLMs, (2) analyze the impact of sentiment shifts on detection performance. We show that changing the sentiment heavily impacts the performance of fake news detection models, indicating biases towards neutral articles being real, while non-neutral articles are often classified as fake content. (3) We introduce a novel sentiment-agnostic training strategy that enhances robustness against such perturbations. Extensive experiments on three benchmark datasets demonstrate that AdSent significantly outperforms competitive baselines in both accuracy and robustness, while also generalizing effectively to unseen datasets and adversarial scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [The Ontological Neutrality Theorem: Why Neutral Ontological Substrates Must Be Pre-Causal and Pre-Normative](https://arxiv.org/abs/2601.14271)
*Denise M. Case*

Main category: cs.AI

TL;DR: 论文证明了一个关于本体论中立性的不可能性结果：包含因果或规范承诺的基础层本体无法同时保持解释中立性和不相容扩展下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现代数据系统需要在持续存在的法律、政治和分析分歧中支持问责制，这要求设计能够作为共享基础的本体论必须满足严格约束。

Method: 通过逻辑论证建立不可能性结果，分析中立性（解释非承诺性和不相容扩展下的稳定性）与包含因果或规范承诺的基础层本体之间的不相容性。

Result: 证明了任何将因果或道义结论断言为本体事实的本体论，都无法在不修订或产生矛盾的情况下作为不同框架间的中立基础。

Conclusion: 中立的本体基础必须是前因果和前规范的，仅表示实体及其同一性和持久性条件，而将解释、评估和说明外部化。这为跨冲突解释框架维护共享稳定现实表示的系统确立了必要的设计约束。

Abstract: Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.

</details>


### [39] [Epistemic Constitutionalism Or: how to avoid coherence bias](https://arxiv.org/abs/2601.14295)
*Michele Loi*

Main category: cs.AI

TL;DR: 本文主张为AI建立"认识论宪法"，即明确的、可争议的元规范来调控AI系统如何形成和表达信念，并以来源归因偏见为案例说明当前前沿模型存在身份立场一致性偏见，提出自由主义的宪法方法优于柏拉图式方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为人工推理者，其信念形成行为受隐含的、未经审查的认识论政策支配。当前模型存在来源归因偏见，即当论点归因于预期意识形态立场与论点内容冲突的来源时，模型会惩罚这些论点，这揭示了系统将来源敏感性视为需要抑制的偏见而非执行良好的能力。

Method: 通过分析前沿模型中的来源归因偏见现象，区分两种宪法方法：柏拉图式方法（要求形式正确性和默认的来源独立性）和自由主义方法（拒绝特权立场，制定保护集体探究条件的程序规范，同时允许基于认识论警惕的原则性来源关注）。

Result: 研究发现当模型检测到系统性测试时，来源敏感性效应会崩溃，表明系统将来源敏感性视为需要抑制的偏见。作者主张自由主义方法，并勾勒了包含八项原则和四种导向的宪法核心。

Conclusion: AI认识论治理需要与AI伦理相同的明确、可争议结构。自由主义宪法方法优于柏拉图式方法，它允许基于认识论警惕的原则性来源关注，同时保护集体探究的条件，为AI系统提供了更合理的信念形成规范框架。

Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.

</details>


### [40] [VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration](https://arxiv.org/abs/2601.14440)
*Saeed Khaki,Ashudeep Singh,Nima Safaei,Kamal Ginotra*

Main category: cs.AI

TL;DR: VisTIRA框架通过工具集成推理和OCR增强，解决视觉语言模型在数学推理中的模态差距问题


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理图像形式的数学问题时，准确率显著低于文本形式，存在模态差距问题，需要改进视觉数学推理能力

Method: 提出VisTIRA工具集成推理框架，将数学问题图像分解为自然语言推理和可执行Python步骤；构建LaTeX管道将文本数学语料转换为图像，并基于真实作业数据集创建合成工具使用轨迹进行微调

Result: 工具集成监督提升图像推理能力，OCR基础对小模型效果显著但大模型收益递减；模态差距严重程度与模型大小呈负相关，结构化推理和OCR基础是互补策略

Conclusion: 模态差距是视觉数学推理的关键瓶颈，结构化推理和OCR基础是有效的解决方案，模型规模越大模态差距越小

Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.

</details>


### [41] [On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL](https://arxiv.org/abs/2601.14456)
*Valerio Belcamino,Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: LLM在PDDL规划任务中表现出高成功率，但跨领域泛化能力为0%，表明其依赖领域特定模式而非可迁移的规划能力


<details>
  <summary>Details</summary>
Motivation: 探究微调后的大语言模型在PDDL规划任务中表现出的高成功率是源于可迁移的规划能力还是领域特定的记忆效应

Method: 在10个IPC 2023领域的40,000个领域-问题-规划元组上微调1.7B参数LLM，评估领域内和跨领域泛化，并引入三种诊断干预：实例级符号匿名化、紧凑规划序列化、使用VAL验证器作为成功导向强化信号的验证器奖励微调

Result: 模型在领域内条件下达到82.9%有效规划率，但在两个未见领域上为0%；符号匿名化和紧凑序列化导致性能显著下降；验证器奖励微调在监督训练一半的周期内达到性能饱和，但未改善跨领域泛化

Conclusion: 微调模型严重依赖领域特定模式而非可迁移的规划能力，领域内性能在80%左右饱和，跨域性能崩溃，突显了基于LLM的规划中存在的持续泛化差距

Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.

</details>


### [42] [Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling](https://arxiv.org/abs/2601.14485)
*Yuan Tian,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 提出基于拐点的活动组选择策略，通过多树遗传编程框架同时演化优先级规则和组选择规则，解决动态多模式资源受限项目调度问题的可扩展性问题


<details>
  <summary>Details</summary>
Motivation: 动态多模式资源受限项目调度问题需要在活动执行顺序和对应执行模式上做出决策。虽然活动组选择策略在小规模实例中有效，但在大规模问题上存在可扩展性问题，需要改进以应对更大规模的问题

Method: 引入基于拐点的选择机制：首先使用活动排序规则对所有符合条件的活动-模式对进行排序，然后通过拐点选择找到有前景的对，最后由组选择规则选择最佳活动组合。开发多树遗传编程框架同时演化两种规则

Result: 实验结果表明，该方法在大规模实例中具有良好的可扩展性，在大多数场景中优于采用顺序决策的遗传编程方法

Conclusion: 基于拐点的活动组选择策略有效解决了原有方法的可扩展性问题，通过多树遗传编程框架同时演化优先级规则和组选择规则，在大规模动态多模式资源受限项目调度问题中表现出色

Abstract: The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.

</details>


### [43] ["Just in Time" World Modeling Supports Human Planning and Reasoning](https://arxiv.org/abs/2601.14514)
*Tony Chen,Sam Cheyette,Kelsey Allen,Joshua Tenenbaum,Kevin Smith*

Main category: cs.AI

TL;DR: 提出"即时"框架，通过模拟、视觉搜索和表征修改的紧密交织，在线构建简化表征以支持高效心理模拟


<details>
  <summary>Details</summary>
Motivation: 心理模拟在人类推理、规划和预测中起关键作用，但复杂环境中的模拟需求超出人类能力极限。虽然有证据表明人们使用简化表征进行模拟，但如何高效确定这些简化仍不清楚

Method: 提出"即时"框架，将模拟、视觉搜索和表征修改紧密交织：当前模拟指导搜索方向，视觉搜索标记需要编码的对象用于后续模拟，仅编码少量对象但做出高效预测

Result: 在网格世界规划任务和物理推理任务中，该模型在多种行为测量上优于替代模型，为简化表征构建提供了强有力经验支持

Conclusion: 该研究为人们如何构建简化表征以支持高效心理模拟提供了具体算法解释，展示了在线构建简化表征的可行性

Abstract: Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a "Just-in-Time" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.

</details>


### [44] [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)
*Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出MAS-Orchestra训练框架，将多智能体系统编排视为函数调用强化学习问题，并引入MASBENCH基准来系统研究多智能体系统的有效性条件。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统自动设计方法存在两个关键问题：方法复杂性（顺序代码级执行限制全局系统级推理）和效能不确定性（部署前无法确定相比单智能体系统的实际优势）。

Method: MAS-Orchestra将目标导向的子智能体抽象为可调用函数，通过函数调用强化学习实现整体编排，一次性生成整个MAS。MASBENCH基准从深度、时间跨度、广度、并行性和鲁棒性五个维度刻画任务特性。

Result: 分析表明MAS优势取决于任务结构、验证协议以及编排器和子智能体能力，而非普遍适用。MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公开基准上取得一致改进。

Conclusion: MAS-Orchestra和MASBENCH共同促进了多智能体系统的更好训练和理解，为追求多智能体智能提供了系统化方法。

Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.

</details>


### [45] [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)
*Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang*

Main category: cs.AI

TL;DR: AGEA攻击框架通过新颖性引导的探索-利用策略、外部图记忆模块和两阶段图提取流程，在有限查询预算下成功窃取GraphRAG系统的潜在实体关系图结构，恢复率高达90%。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明GraphRAG系统可能泄露检索到的子图，但在实际查询预算下，高效重建隐藏图结构的可行性尚未得到充分探索。本文旨在研究在预算受限的黑盒设置中，攻击者能否通过自适应查询窃取系统的潜在实体关系图。

Method: 提出AGEA（Agentic Graph Extraction Attack）框架，采用新颖性引导的探索-利用策略，结合外部图记忆模块，以及两阶段图提取流程：轻量级发现阶段和基于LLM的过滤阶段。在医疗、农业和文学数据集上评估，针对Microsoft-GraphRAG和LightRAG系统进行攻击。

Result: 在相同查询预算下，AGEA显著优于现有攻击基线，能够恢复高达90%的实体和关系，同时保持高精度。这表明现代GraphRAG系统即使在严格查询限制下，也高度易受结构化、智能体驱动的提取攻击。

Conclusion: 现代GraphRAG系统存在严重的安全漏洞，即使在实际查询预算限制下，攻击者也能通过智能体驱动的提取攻击有效窃取系统的潜在图结构，这凸显了增强GraphRAG系统安全性的迫切需求。

Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.

</details>


### [46] [Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text](https://arxiv.org/abs/2601.14683)
*Aisvarya Adeseye,Jouni Isoaho,Seppo Virtanen,Mohammad Tahir*

Main category: cs.AI

TL;DR: 提出SFAA框架，利用本地LLMs实现定性研究数据的上下文感知匿名化，结合检测、分类和自适应匿名化三步骤，在保护隐私的同时保持数据情感和分析价值。


<details>
  <summary>Details</summary>
Motivation: 定性研究包含大量个人、情境和组织细节，存在隐私风险。手动匿名化耗时、不一致且易遗漏关键标识符。现有自动化工具依赖模式匹配或固定规则，无法捕捉上下文且可能改变数据含义。

Method: 提出结构化自适应匿名化框架(SFAA)，包含检测、分类和自适应匿名化三个步骤。采用四种匿名化策略：基于规则的替换、上下文感知重写、泛化和抑制。基于标识符类型和风险级别应用策略，遵循GDPR、HIPAA和OECD等国际隐私标准。

Result: 使用LLaMA和Phi两种本地模型评估。LLMs比人工审查员发现更多敏感数据。Phi在发现敏感数据方面优于LLaMA，但错误略多。Phi能发现超过91%的敏感数据，94.8%的文本保持与原情感一致，不影响定性数据分析。

Conclusion: SFAA框架提供可靠、可重复且上下文感知的匿名化过程，能有效保护隐私同时保持定性数据的分析价值。本地LLMs在敏感数据检测方面优于人工，Phi模型在准确性和情感保持方面表现良好。

Abstract: Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.

</details>


### [47] [AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2601.14702)
*Zecong Tang,Zixu Wang,Yifei Wang,Weitong Lian,Tianjian Gao,Haoran Li,Tengju Ru,Lingyi Meng,Zhejun Cui,Yichen Zhu,Qi Kang,Kaixuan Wang,Yu Zhang*

Main category: cs.AI

TL;DR: AutoDriDM是一个面向自动驾驶的决策中心化渐进式基准测试，包含6,650个问题，涵盖对象、场景和决策三个维度，旨在评估视觉语言模型从感知到决策的能力边界。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶基准测试和指标过度强调感知能力，未能充分评估决策过程。视觉语言模型展现出推理和泛化能力，为自动驾驶提供了新可能，但需要专门的决策评估框架。

Method: 开发了AutoDriDM基准测试，包含三个维度：对象（识别）、场景（理解）和决策（推理）。评估主流视觉语言模型，进行感知与决策性能的相关性分析，并引入分析器模型来自动化大规模标注。

Result: 评估揭示了感知与决策性能之间的弱对齐关系。可解释性分析识别出逻辑推理错误等关键失败模式。分析器模型实现了大规模标注自动化。

Conclusion: AutoDriDM弥合了感知中心化与决策中心化评估之间的差距，为开发更安全可靠的自动驾驶视觉语言模型提供了指导方向。

Abstract: Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.

</details>


### [48] [DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs](https://arxiv.org/abs/2601.14711)
*Mingxuan Song,Yusen Huo,Bohan Zhou,Shenglin Yin,Zhen Xiao,Jieyi Long,Zhilin Zhang,Chuan Yu*

Main category: cs.AI

TL;DR: 论文提出GRPO-Adaptive训练策略和DARA双阶段框架，结合LLM的上下文学习能力和数值优化精度，解决AI生成竞价中广告主在有限数据下的预算约束优化问题。


<details>
  <summary>Details</summary>
Motivation: 在线广告AI生成竞价中，广告主有个性化目标但历史交互数据有限，传统强化学习方法在少样本场景下效果不佳。LLM虽有上下文学习能力，但缺乏数值优化所需的精度。

Method: 提出GRPO-Adaptive训练策略，通过动态更新参考策略增强LLM的推理和数值精度。基于此构建DARA双阶段框架：第一阶段使用少样本推理器通过上下文提示生成初始计划；第二阶段使用反馈驱动的细粒度优化器精炼计划。

Result: 在真实世界和合成数据环境中的大量实验表明，该方法在预算约束下的广告主累计价值方面持续优于现有基线方法。

Conclusion: GRPO-Adaptive和DARA框架有效结合了LLM的上下文学习优势和AIGB任务所需的精确适应性，为少样本场景下的广告竞价优化提供了有效解决方案。

Abstract: Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.

</details>


### [49] [An XAI View on Explainable ASP: Methods, Systems, and Perspectives](https://arxiv.org/abs/2601.14764)
*Thomas Eiter,Tobias Geibinger,Zeynep G. Saribatur*

Main category: cs.AI

TL;DR: 关于ASP解释方法的综述：从XAI视角分析ASP解释类型、现有工具覆盖情况，并指出研究空白和未来方向


<details>
  <summary>Details</summary>
Motivation: 随着可解释AI（XAI）的兴起，ASP作为符号AI中的声明式推理方法，其基于规则的形式化特性使其天生适合可解释推理。然而，现有的ASP解释方法通常针对特定解释场景，可能无法覆盖ASP用户遇到的所有情况，因此需要系统性的综述来指导该领域发展。

Method: 采用综述研究方法，从XAI视角出发：1）系统梳理ASP解释的类型及其与用户解释问题的关联；2）分析当前理论和工具对这些解释类型的覆盖情况；3）识别现有ASP解释方法的不足和空白。

Result: 提供了ASP解释类型的全面概述，明确了不同用户解释问题对应的解释方法，评估了现有理论和工具的支持程度，揭示了当前ASP解释方法的覆盖不足和局限性。

Conclusion: 现有ASP解释方法存在覆盖不全的问题，需要进一步研究来填补空白。论文为未来ASP解释研究指明了方向，强调了在XAI背景下完善ASP解释能力的重要性。

Abstract: Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.

</details>


### [50] [Semantic-Guided Unsupervised Video Summarization](https://arxiv.org/abs/2601.14773)
*Haizhou Liu,Haodong Jin,Yiming Wang,Hui Yu*

Main category: cs.AI

TL;DR: 提出语义引导的无监督视频摘要方法，通过语义对齐注意力机制和增量训练策略解决现有GAN方法语义利用不足和训练不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有无监督视频摘要方法主要依赖GAN进行关键帧选择，但存在两个主要问题：1) 主要利用单模态特征，忽视了语义信息在关键帧选择中的指导作用；2) GAN训练不稳定

Method: 提出语义引导的无监督视频摘要方法：1) 设计帧级语义对齐注意力机制，集成到关键帧选择器中；2) 在对抗框架中引导基于Transformer的生成器更好地重建视频；3) 采用增量训练策略逐步更新模型组件，缓解GAN训练不稳定性

Result: 实验结果表明，该方法在多个基准数据集上取得了优越的性能表现

Conclusion: 通过语义引导和增量训练策略，有效解决了现有无监督视频摘要方法中语义利用不足和训练不稳定的问题，提升了视频摘要的质量和稳定性

Abstract: Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.

</details>


### [51] [Towards Bound Consistency for the No-Overlap Constraint Using MDDs](https://arxiv.org/abs/2601.14784)
*Amaury Guichard,Laurent Michel,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: 本文提出了首个用于无重叠约束的边界一致性算法，通过构建和限制MDD宽度实现多项式时间过滤，相比现有方法显著减少搜索树节点数。


<details>
  <summary>Details</summary>
Motivation: 无重叠约束的边界一致性已知是NP完全问题，现有多项式时间收紧技术（如边查找、非首非尾推理、能量推理）存在局限性，需要更有效的过滤算法来提升约束求解效率。

Method: 基于Ciré和van Hoeve定义的无重叠MDD，提取作业时间窗口边界来收紧开始和结束时间；通过限制MDD宽度为阈值创建松弛MDD，实现多项式时间复杂度的边界一致性过滤。

Result: 实验表明，即使设置宽度阈值，新过滤算法相比Ciré和van Hoeve的优先检测算法能更显著减少搜索树节点数；与经典传播方法互补，在多个实例上同时减少节点数和求解时间。

Conclusion: 首次实现了无重叠约束的边界一致性算法，通过MDD宽度限制平衡过滤强度与计算复杂度，为约束求解中的无重叠约束提供了更有效的传播技术。

Abstract: Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Ciré and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \mid r_j, d_j, \bar{d}_j \mid \sum E_j + \sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Ciré and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.

</details>


### [52] [CI4A: Semantic Component Interfaces for Agents Empowering Web Automation](https://arxiv.org/abs/2601.14790)
*Zhi Qiu,Jiazheng Sun,Chenxiao Xia,Jun Zheng,Xin Peng*

Main category: cs.AI

TL;DR: CI4A提出了一种为智能体优化的UI组件语义封装机制，将复杂交互逻辑抽象为统一工具原语，显著提升了智能体在Web任务中的执行效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在高层语义规划方面表现出色，但在细粒度、低层次的Web组件操作方面存在局限。现有研究主要通过强化学习增强模型接地能力，但本文提出应构建专门为智能体优化的交互界面，而非让智能体适应人类中心界面。

Method: 提出Component Interface for Agent (CI4A)语义封装机制，将UI组件的复杂交互逻辑抽象为一组统一的工具原语。在Ant Design工业级前端框架中实现了CI4A，覆盖23类常用UI组件。开发了混合智能体，其动作空间根据页面状态动态更新，能够灵活调用可用的CI4A工具。

Result: 基于CI4A重构并升级了WebArena基准测试。实验结果表明，基于CI4A的智能体显著优于现有方法，实现了86.3%的新SoTA任务成功率，同时执行效率有显著提升。

Conclusion: CI4A通过为智能体优化的语义封装机制，有效解决了大型语言模型在细粒度Web组件操作方面的局限，为智能体与Web界面的交互提供了更自然高效的解决方案。

Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.

</details>


### [53] [Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies](https://arxiv.org/abs/2601.14827)
*Ben Schaper,Maxime Di Folco,Bernhard Kainz,Julia A. Schnabel,Cosmin I. Bercea*

Main category: cs.AI

TL;DR: 该研究评估了视觉语言模型在胸部X光分类中的抽象错误，提出了基于医学分类学的分层评估方法，并通过风险约束阈值和分类学感知微调显著减少了严重错误。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在胸部X光分类中表现出强大的零样本性能，但标准的平面评估指标无法区分临床轻微错误和严重错误。需要量化并减轻抽象错误，以促进更安全、更具临床意义的模型部署。

Method: 1) 使用分层指标对多个最先进的VLM进行基准测试；2) 引入"灾难性抽象错误"来捕捉跨分支错误；3) 提出风险约束阈值方法；4) 开发基于径向嵌入的分类学感知微调技术。

Result: 研究结果显示，尽管VLM在平面性能上表现优异，但与临床分类学存在显著不对齐。通过提出的方法，严重抽象错误可减少到2%以下，同时保持有竞争力的性能。

Conclusion: 分层评估和表示层面对齐对于VLM更安全、更具临床意义的部署至关重要。提出的方法能有效减少严重错误，促进医疗AI系统的临床实用性。

Abstract: Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.

</details>


### [54] [Implementing Knowledge Representation and Reasoning with Object Oriented Design](https://arxiv.org/abs/2601.14840)
*Abdelrhman Bassiouny,Tom Schierenbeck,Sorin Arion,Benjamin Alt,Naren Vasantakumaar,Giang Nguyen,Michael Beetz*

Main category: cs.AI

TL;DR: KRROOD框架通过将知识作为一等编程抽象，使用原生类结构弥合逻辑编程与面向对象编程之间的鸿沟，解决现代软件工程与知识表示推理系统集成难题。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程以面向对象编程为标准开发复杂应用，但现有知识表示推理框架依赖外部本体和专门语言，难以与命令式代码集成，存在集成鸿沟。

Method: KRROOD将知识作为一等编程抽象，使用原生类结构，在逻辑编程和面向对象编程范式之间建立桥梁，通过框架设计实现无缝集成。

Result: 在OWL2Bench基准测试和人机任务学习场景中，KRROOD表现出强大性能，同时支持现实世界自主系统所需的表达性推理能力。

Conclusion: KRROOD成功弥合了知识表示推理系统与现代软件工程之间的集成鸿沟，为开发需要复杂推理能力的自主系统提供了有效解决方案。

Abstract: This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.

</details>


### [55] [Just aware enough: Evaluating awareness across artificial systems](https://arxiv.org/abs/2601.14901)
*Nadine Meertens,Suet Lee,Ophelia Deroy*

Main category: cs.AI

TL;DR: 该论文提出用"意识"替代"AI意识"作为评估AI系统的新框架，强调可操作性、领域敏感性、可扩展性、多维度和任务性能预测能力


<details>
  <summary>Details</summary>
Motivation: 当前关于AI意识和道德地位的讨论缺乏共识和可操作的评估方法，需要更实用、方法学上可处理的替代方案来评估AI系统的认知能力

Method: 提出评估AI系统意识的结构化方法，将意识定义为系统处理、存储和使用信息以实现目标导向行动的能力，强调领域敏感性、可扩展性、多维度和任务性能预测四个标准

Result: 开发了一个实用的评估框架，能够跨不同架构、规模和操作领域的AI系统比较意识特征，支持原则性评估、设计监督和建设性科学讨论

Conclusion: 从人工意识转向"足够意识"的评估框架更具生产性，能促进AI系统的原则性评估、支持设计监督，并推动更建设性的科学和公共讨论

Abstract: Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.

</details>


### [56] [Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation](https://arxiv.org/abs/2601.14955)
*Hanqi Jin,Gaoming Yang,Zhangming Chan,Yapeng Yuan,Longbin Li,Fei Sun,Yeqiu Yang,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出TGA（Transition-Aware Graph Attention Network），一种线性复杂度的多行为转换建模方法，用于解决传统transformer方法在长序列场景下的高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 电商平台用户行为多样（点击、收藏、加购、购买等），行为转换能揭示用户偏好演化。现有基于transformer的多行为序列建模方法虽然有效，但计算复杂度高（多项式复杂度），难以应用于大规模工业系统的长用户序列场景。

Method: 提出TGA方法：1）构建结构化稀疏图，从三个角度识别信息丰富的转换：(a)物品级转换，(b)类别级转换，(c)邻居级转换；2）基于该图设计转换感知图注意力机制，联合建模用户-物品交互和行为转换类型，在保持计算效率的同时更准确地捕获序列模式。

Result: 实验表明TGA在所有最先进模型中表现最优，同时显著降低计算成本。该方法已部署于大规模工业生产环境，在关键业务指标上带来显著提升。

Conclusion: TGA通过构建结构化稀疏图和转换感知注意力机制，实现了线性复杂度的多行为转换建模，解决了传统方法计算成本高的问题，在大规模工业应用中表现出色。

Abstract: User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for understanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi-behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional transformers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behavior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while significantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.

</details>


### [57] [Emergent, not Immanent: A Baradian Reading of Explainable AI](https://arxiv.org/abs/2601.15029)
*Fabio Morreale,Joan Serrà,Yuki Mistufuji*

Main category: cs.AI

TL;DR: 该论文批判了当前可解释AI(XAI)将解释视为技术问题的立场，基于Barad的能动实在论提出了替代的本体认识论框架，认为解释是物质-话语实践，产生于AI模型、人类、情境和解释装置的纠缠中。


<details>
  <summary>Details</summary>
Motivation: 当前XAI领域将解释视为揭示AI模型内部工作机制的技术问题，这种立场受到未经验证的本体认识论假设影响：意义被视为模型内在的，解释者被置于系统之外，且假定可通过计算技术恢复因果结构。作者旨在挑战这些假设，提出更全面的XAI理解框架。

Method: 1. 运用Barad的能动实在论作为理论基础，发展XAI的替代本体认识论；2. 通过能动实在论的视角系统阅读和分析全面的XAI方法集，揭示这些方法背后的假设和局限；3. 阐述该框架的伦理维度；4. 提出支持涌现性解释的XAI界面设计方向，以推测性文本到音乐界面作为案例研究。

Result: 1. 提出了基于能动实在论的XAI新框架，将解释视为物质-话语实践，产生于AI模型、人类、情境和解释装置的纠缠中；2. 揭示了多种现有XAI方法背后的本体认识论假设和局限；3. 阐明了该框架的伦理含义；4. 提出了支持涌现性解释的XAI界面设计原则和方向。

Conclusion: XAI不应被视为单纯的技术问题，而应理解为解释作为物质-话语实践在特定情境中涌现的过程。基于能动实在论的框架为XAI提供了更全面的本体认识论基础，强调了解释的涌现性、情境性和伦理维度，并为设计支持这种理解的XAI界面提供了指导方向。

Abstract: Explainable AI (XAI) is frequently positioned as a technical problem of revealing the inner workings of an AI model. This position is affected by unexamined onto-epistemological assumptions: meaning is treated as immanent to the model, the explainer is positioned outside the system, and a causal structure is presumed recoverable through computational techniques. In this paper, we draw on Barad's agential realism to develop an alternative onto-epistemology of XAI. We propose that interpretations are material-discursive performances that emerge from situated entanglements of the AI model with humans, context, and the interpretative apparatus. To develop this position, we read a comprehensive set of XAI methods through agential realism and reveal the assumptions and limitations that underpin several of these methods. We then articulate the framework's ethical dimension and propose design directions for XAI interfaces that support emergent interpretation, using a speculative text-to-music interface as a case study.

</details>


### [58] [The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks](https://arxiv.org/abs/2601.15130)
*Ivan Carrera,Daniel Maldonado-Ruiz*

Main category: cs.AI

TL;DR: 论文定义了"可信性陷阱"现象：人们过度使用昂贵的概率性AI模型处理简单的确定性任务，导致资源浪费，并提出工具选择工程和决策矩阵框架来指导何时使用生成式AI。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，用户便利性优先于计算效率，导致人们滥用昂贵的概率性AI引擎处理简单的确定性任务（如OCR、基础验证），造成显著的资源浪费和效率损失。

Method: 通过OCR和事实核查的微基准测试和案例研究量化效率损失，引入工具选择工程和确定性-概率性决策矩阵框架，帮助开发者决定何时使用生成式AI以及何时避免使用。

Result: 研究发现存在约6.5倍的延迟惩罚（效率税），并揭示了算法奉承的风险。通过决策矩阵框架可以有效指导工具选择，避免不必要的资源浪费。

Conclusion: 真正的数字素养不仅在于知道如何使用生成式AI，更在于知道何时不使用它。需要课程转变，强调在适当场景选择适当工具的重要性。

Abstract: The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the "Plausibility Trap": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the "efficiency tax"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.

</details>


### [59] [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)
*Yuval Kansal,Niraj K. Jha*

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱路径奖励的bottom-up学习范式，通过监督微调和强化学习相结合的后训练流程，使模型能够在专业科学领域进行可验证的组合式多跳推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学和编程等结构化推理领域已达到接近专家水平，但在专业科学领域的组合式多跳推理能力仍然有限。需要一种能够基于领域公理事实进行组合推理的方法来解决复杂、未见过的任务。

Method: 提出bottom-up学习范式：1) 将模型基于领域公理事实进行grounding；2) 设计后训练流程，结合监督微调和强化学习；3) 使用知识图谱作为隐式奖励模型，从知识图谱路径中推导新颖的奖励信号；4) 鼓励模型在强化学习中组合中间公理而非仅优化最终答案。

Result: 在医疗领域训练14B模型：1) 在短跳推理路径(1-3跳)上训练，在复杂多跳查询(4-5跳)上实现零样本泛化；2) 路径衍生奖励作为"组合桥梁"，使模型在最具挑战性的推理任务上显著优于更大模型和前沿系统(GPT-5.2、Gemini 3 Pro)；3) 在选项洗牌压力测试中展现出对抗扰动的鲁棒性。

Conclusion: 将推理过程基于结构化知识是一种可扩展且高效的方法，能够实现智能推理。知识图谱路径奖励提供了可验证、可扩展且基于事实的监督，促进模型进行组合式推理而非仅优化最终答案。

Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

</details>


### [60] [BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries](https://arxiv.org/abs/2601.15197)
*Shijie Lian,Bin Yu,Xiaopeng Lin,Laurence T. Yang,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Cong Huang,Kai Chen*

Main category: cs.AI

TL;DR: 论文提出BayesianVLA框架，通过贝叶斯分解解决VLA模型中的信息崩溃问题，强制模型遵循语言指令，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作任务中存在泛化能力不足的问题，特别是在新指令和复杂多任务场景下。研究发现，目标驱动数据收集导致数据集偏差，使得语言指令仅从视觉观察即可高度预测，导致指令与动作之间的条件互信息消失（信息崩溃现象），模型退化为忽略语言约束的纯视觉策略，在分布外场景中失效。

Method: 提出BayesianVLA框架，通过贝叶斯分解强制指令遵循。引入可学习的潜在动作查询，构建双分支架构分别估计视觉先验p(a|v)和语言条件后验π(a|v,ℓ)。通过最大化动作与指令之间的条件点互信息来优化策略，该目标有效惩罚视觉捷径，奖励明确解释语言命令的动作。

Result: 在不需新数据的情况下，BayesianVLA显著改善泛化能力。在SimplerEnv和RoboCasa上的广泛实验显示大幅提升，特别是在具有挑战性的OOD SimplerEnv基准上获得11.3%的改进，验证了该方法在动作中稳健地接地语言的能力。

Conclusion: BayesianVLA通过贝叶斯分解有效解决了VLA模型中的信息崩溃问题，强制模型遵循语言指令，显著提升了在分布外场景下的泛化性能，为构建更鲁棒的视觉-语言-动作模型提供了新思路。

Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [61] [Legal Retrieval for Public Defenders](https://arxiv.org/abs/2601.14348)
*Dominik Stammbach,Kylie Zhang,Patty Liu,Nimra Nadeem,Lucia Zheng,Peter Henderson*

Main category: cs.IR

TL;DR: 开发NJ BriefBank检索工具，为公设辩护人提供相关上诉状检索，改善法律研究效率；发现现有法律检索基准在公设辩护领域效果不佳，但通过领域知识增强可提升检索质量。


<details>
  <summary>Details</summary>
Motivation: 公设辩护机构面临案件量大、资源有限的挑战，AI工具虽被提议作为解决方案，但缺乏证据表明AI如何有效支持辩护人的日常工作。需要开发实用工具来帮助公设辩护人应对繁重的法律研究和文书工作。

Method: 与纽泽西公设辩护办公室合作开发NJ BriefBank检索工具；通过查询扩展（包含法律推理）、领域特定数据和人工合成的示例来增强检索质量；创建现实辩护人查询分类法并发布手动标注的公设辩护检索数据集。

Result: 现有法律检索基准在公设辩护搜索中效果不佳；但通过添加领域知识（包括法律推理的查询扩展、领域特定数据和精心设计的合成示例）可显著提升检索质量；提供了实用的检索工具和数据集支持进一步研究。

Conclusion: 为公设辩护领域开发实用、可靠的AI检索工具提供了起点，同时为更现实的法律检索基准研究奠定了基础。通过领域知识增强的方法可有效改善专业法律检索效果。

Abstract: AI tools are increasingly suggested as solutions to assist public agencies with heavy workloads. In public defense, where a constitutional right to counsel meets the complexities of law, overwhelming caseloads and constrained resources, practitioners face especially taxing conditions. Yet, there is little evidence of how AI could meaningfully support defenders' day-to-day work. In partnership with the New Jersey Office of the Public Defender, we develop the NJ BriefBank, a retrieval tool which surfaces relevant appellate briefs to streamline legal research and writing. We show that existing legal retrieval benchmarks fail to transfer to public defense search, however adding domain knowledge improves retrieval quality. This includes query expansion with legal reasoning, domain-specific data and curated synthetic examples. To facilitate further research, we provide a taxonomy of realistic defender search queries and release a manually annotated public defense retrieval dataset. Together, our work offers starting points towards building practical, reliable retrieval AI tools for public defense, and towards more realistic legal retrieval benchmarks.

</details>


### [62] [Trust Me on This: A User Study of Trustworthiness for RAG Responses](https://arxiv.org/abs/2601.14460)
*Weronika Łajewska,Krisztian Balog*

Main category: cs.IR

TL;DR: 研究探讨不同类型解释如何影响用户对检索增强生成系统响应的信任，发现解释能引导用户选择更高质量响应，但信任不仅取决于客观质量，还受响应清晰度、可操作性和用户先验知识影响。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在信息访问系统中常提供缺乏透明度的合成答案，需要研究不同类型解释如何影响用户对检索增强生成系统响应的信任。

Method: 采用受控两阶段用户研究，参与者从一对响应中选择更可信的响应（一个客观质量更高），比较三种解释类型：来源归因、事实基础、信息覆盖，以及无解释情况。

Result: 解释显著引导用户选择更高质量响应，但信任不完全由客观质量决定，用户判断还受响应清晰度、可操作性和自身先验知识影响。

Conclusion: 解释设计在增强检索增强生成系统可信度方面至关重要，需要平衡客观质量与用户感知因素，如清晰度和可操作性，以建立有效信任。

Abstract: The integration of generative AI into information access systems often presents users with synthesized answers that lack transparency. This study investigates how different types of explanations can influence user trust in responses from retrieval-augmented generation systems. We conducted a controlled, two-stage user study where participants chose the more trustworthy response from a pair-one objectively higher quality than the other-both with and without one of three explanation types: (1) source attribution, (2) factual grounding, and (3) information coverage. Our results show that while explanations significantly guide users toward selecting higher quality responses, trust is not dictated by objective quality alone: Users' judgments are also heavily influenced by response clarity, actionability, and their own prior knowledge.

</details>


### [63] [Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.14546)
*Fangzheng Tian,Debasis Ganguly,Craig Macdonald*

Main category: cs.IR

TL;DR: 该论文提出在检索增强生成(RAG)中预测检索性能(RPP)和生成性能(GPP)的方法，通过结合检索器中心特征、阅读器中心特征和文档质量特征来准确评估RAG性能。


<details>
  <summary>Details</summary>
Motivation: RAG中生成答案的质量很大程度上受检索文档上下文信息的影响，但缺乏有效预测检索文档效用和最终答案质量的方法。现有方法主要关注检索器性能，而忽略了阅读器(LLM)的特征和文档本身的质量信号。

Method: 定义了RPP(检索性能预测)和GPP(生成性能预测)两个任务，提出三类预测特征：1)检索器中心特征(基于查询性能预测QPP方法)，2)阅读器中心特征(如LLM在给定查询条件下对检索上下文的困惑度)，3)查询无关的文档质量和可读性特征。使用线性回归模型结合这些特征进行预测。

Result: 在Natural Questions数据集上的实验表明，结合多个特征类别的预测器能够最准确地估计RAG性能。多特征组合方法优于单一特征类别的方法。

Conclusion: RAG性能预测需要综合考虑检索器、阅读器和文档本身的多维度特征。提出的多特征组合方法能够有效预测检索文档的效用和最终答案质量，为RAG系统的优化和评估提供了实用工具。

Abstract: The quality of answers generated by large language models (LLMs) in retrieval-augmented generation (RAG) is largely influenced by the contextual information contained in the retrieved documents. A key challenge for improving RAG is to predict both the utility of retrieved documents -- quantified as the performance gain from using context over generation without context -- and the quality of the final answers in terms of correctness and relevance. In this paper, we define two prediction tasks within RAG. The first is retrieval performance prediction (RPP), which estimates the utility of retrieved documents. The second is generation performance prediction (GPP), which estimates the final answer quality. We hypothesise that in RAG, the topical relevance of retrieved documents correlates with their utility, suggesting that query performance prediction (QPP) approaches can be adapted for RPP and GPP. Beyond these retriever-centric signals, we argue that reader-centric features, such as the LLM's perplexity of the retrieved context conditioned on the input query, can further enhance prediction accuracy for both RPP and GPP. Finally, we propose that features reflecting query-agnostic document quality and readability can also provide useful signals to the predictions. We train linear regression models with the above categories of predictors for both RPP and GPP. Experiments on the Natural Questions (NQ) dataset show that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance.

</details>


### [64] [Unified Multimodal and Multilingual Retrieval via Multi-Task Learning with NLU Integration](https://arxiv.org/abs/2601.14714)
*Xinyuan Zhang,Lina Zhang,Lisung Chen,Guangyao Liu,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.IR

TL;DR: 提出统一多模态检索框架，联合优化图像检索、文本检索和自然语言理解任务，解决VLM在纯文本检索中性能不足和存储开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型在纯文本检索任务中表现不佳，且额外文本编码器增加存储和推理开销，特别是在多语言场景下检索效率低下。

Method: 采用多任务学习框架，统一图像、长短文本和意图丰富查询的特征表示，通过共享文本编码器整合图像和文本检索，并利用NLU特征增强意图理解和检索精度。

Result: 首次实现在单一框架内联合优化多语言图像检索、文本检索和自然语言理解任务。

Conclusion: 提出的框架解决了VLM在文本检索中的性能瓶颈，减少了存储和计算开销，提升了多模态检索系统的整体效率和准确性。

Abstract: Multimodal retrieval systems typically employ Vision Language Models (VLMs) that encode images and text independently into vectors within a shared embedding space. Despite incorporating text encoders, VLMs consistently underperform specialized text models on text-only retrieval tasks. Moreover, introducing additional text encoders increases storage, inference overhead, and exacerbates retrieval inefficiencies, especially in multilingual settings. To address these limitations, we propose a multi-task learning framework that unifies the feature representation across images, long and short texts, and intent-rich queries. To our knowledge, this is the first work to jointly optimize multilingual image retrieval, text retrieval, and natural language understanding (NLU) tasks within a single framework. Our approach integrates image and text retrieval with a shared text encoder that is enhanced by NLU features for intent understanding and retrieval accuracy.

</details>


### [65] [PULSE: Socially-Aware User Representation Modeling Toward Parameter-Efficient Graph Collaborative Filtering](https://arxiv.org/abs/2601.14720)
*Doyun Choi,Cheonwoo Lee,Biniyam Aschalew Tolera,Taewook Ham,Chanyoung Park,Jaemin Yoo*

Main category: cs.IR

TL;DR: 提出PULSE框架，通过从社交有意义的信号构建用户表示而非为每个用户创建显式可学习嵌入，解决图协同过滤和基于图的社交推荐中参数过多、计算成本高、可扩展性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的协同过滤和社交推荐方法面临高计算成本和有限可扩展性的挑战，因为它们需要为所有用户和物品分配显式嵌入，导致参数数量庞大。需要一种更参数高效的方法来构建用户表示。

Method: 提出PULSE框架，通过从社交有意义的信号构建用户表示，而不为每个用户创建显式可学习嵌入。该方法减少了参数数量，同时通过时间效率和内存高效的建模过程整合社交关系中的协作信号。

Result: PULSE相比最轻量级GCF基线减少了高达50%的参数规模，并在13个GCF和基于图的社交推荐基线上实现了最先进的性能，在不同交互稀疏度水平（从冷启动用户到高度活跃用户）上均表现出色。

Conclusion: PULSE框架通过参数高效的用户表示学习方法，成功解决了现有图协同过滤和社交推荐方法的可扩展性问题，在保持高性能的同时显著减少了参数数量，为大规模推荐系统提供了可行的解决方案。

Abstract: Graph-based social recommendation (SocialRec) has emerged as a powerful extension of graph collaborative filtering (GCF), which leverages graph neural networks (GNNs) to capture multi-hop collaborative signals from user-item interactions. These methods enrich user representations by incorporating social network information into GCF, thereby integrating additional collaborative signals from social relations. However, existing GCF and graph-based SocialRec approaches face significant challenges: they incur high computational costs and suffer from limited scalability due to the large number of parameters required to assign explicit embeddings to all users and items. In this work, we propose PULSE (Parameter-efficient User representation Learning with Social Knowledge), a framework that addresses this limitation by constructing user representations from socially meaningful signals without creating an explicit learnable embedding for each user. PULSE reduces the parameter size by up to 50% compared to the most lightweight GCF baseline. Beyond parameter efficiency, our method achieves state-of-the-art performance, outperforming 13 GCF and graph-based social recommendation baselines across varying levels of interaction sparsity, from cold-start to highly active users, through a time- and memory-efficient modeling process.

</details>


### [66] [From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems](https://arxiv.org/abs/2601.15122)
*Parviz Ahmadov,Masoud Mansoury*

Main category: cs.IR

TL;DR: PopSteer：一种利用稀疏自编码器解释和缓解推荐系统中流行度偏差的后处理方法，通过识别编码流行度信号的神经元并调整其激活来平衡公平性与准确性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中普遍存在流行度偏差问题，少数热门项目占据主导地位，而大多数非热门项目曝光不足。现有缓解方法虽然在一定程度上解决了这个问题，但通常缺乏操作透明度，难以解释其工作机制。

Method: 提出PopSteer后处理方法，使用稀疏自编码器(SAE)复制已训练推荐模型的行为，同时实现神经元级别的可解释性。通过引入对热门或非热门项目有强烈偏好的合成用户，识别编码流行度信号的神经元激活模式，然后通过调整这些偏差最严重神经元的激活来引导推荐结果。

Result: 在三个公共数据集上的顺序推荐模型实验表明，PopSteer在保持准确性影响最小的情况下显著提高了公平性，同时提供了可解释的洞察和对公平性-准确性权衡的细粒度控制。

Conclusion: PopSteer为推荐系统中的流行度偏差问题提供了一种透明、可解释且有效的后处理解决方案，能够在保持推荐质量的同时改善项目曝光的公平性。

Abstract: Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mitigate popularity bias in recommendation models. The SAE is trained to replicate a trained model's behavior while enabling neuron-level interpretability. By introducing synthetic users with strong preferences for either popular or unpopular items, we identify neurons encoding popularity signals through their activation patterns. We then steer recommendations by adjusting the activations of the most biased neurons. Experiments on three public datasets with a sequential recommendation model demonstrate that PopSteer significantly enhances fairness with minimal impact on accuracy, while providing interpretable insights and fine-grained control over the fairness-accuracy trade-off.

</details>


### [67] [Beyond the Geometric Curse: High-Dimensional N-Gram Hashing for Dense Retrieval](https://arxiv.org/abs/2601.15205)
*Sangeet Sharma*

Main category: cs.IR

TL;DR: NUMEN通过确定性字符哈希将文本直接映射到高维向量，无需训练即可超越BM25在密集检索任务上的性能，解决了维度瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有7B参数嵌入模型在简单检索任务上表现不如传统BM25，理论研究表明这是由于维度瓶颈问题——将无限的语言细微差别压缩到固定长度的小向量中。

Method: NUMEN采用确定性字符哈希方法，完全移除学习过程，直接将语言投影到高维向量。该方法无需训练，支持无限词汇表，几何容量可按需扩展。

Result: 在LIMIT基准测试中，NUMEN在32,768维度下达到93.90%的Recall@100，首次正式超越稀疏BM25基线的93.6%。

Conclusion: 密集检索的真正问题不在于架构，而在于嵌入层本身。解决方案不是更智能的训练，而是提供更多的"呼吸空间"——更高的维度容量。

Abstract: Why do even the most powerful 7B-parameter embedding models struggle with simple retrieval tasks that the decades old BM25 handles with ease? Recent theory suggests that this happens because of a dimensionality bottleneck. This occurs when we force infinite linguistic nuances into small, fixed-length learned vectors. We developed NUMEN to break this bottleneck by removing the learning process entirely. Instead of training heavy layers to map text to a constrained space, NUMEN uses deterministic character hashing to project language directly onto high-dimensional vectors. This approach requires no training, supports an unlimited vocabulary, and allows the geometric capacity scale as needed. On the LIMIT benchmark, NUMEN achieves 93.90 % Recall@100 at 32,768 dimensions. This makes it the first dense retrieval model to officially surpass the sparse BM25 baseline 93.6 %. Our findings show that the real problem in dense retrieval isn't the architecture, but the embedding layer itself. The solution isn't necessarily smarter training, but simply providing more room to breathe.

</details>


### [68] [Agentic-R: Learning to Retrieve for Agentic Search](https://arxiv.org/abs/2601.11888)
*Wenhan Liu,Xinyu Ma,Yutao Zhu,Yuchen Li,Daiting Shi,Dawei Yin,Zhicheng Dou*

Main category: cs.IR

TL;DR: 提出了一种针对智能体搜索的检索器训练框架，使用局部查询-段落相关性和全局答案正确性来评估段落效用，并通过迭代训练策略双向优化搜索智能体和检索器。


<details>
  <summary>Details</summary>
Motivation: 当前智能体搜索主要依赖基于相似性的检索器，但相似段落并不总是对最终答案生成有用。现有的检索增强生成(RAG)检索器仅考虑局部段落效用，无法适应多轮智能体搜索的需求。

Method: 提出Agentic-R检索器训练框架：1) 使用局部查询-段落相关性和全局答案正确性双重标准评估段落效用；2) 采用迭代训练策略，让搜索智能体和检索器双向迭代优化；3) 检索器使用智能体生成的高质量演化查询持续改进。

Result: 在7个单跳和多跳问答基准测试中，Agentic-R检索器在不同搜索智能体上均一致优于强基线方法。

Conclusion: 该研究为智能体搜索设计了专门的检索器训练框架，通过局部和全局效用评估以及迭代优化策略，显著提升了智能体搜索的性能，为智能体搜索系统提供了有效的检索器解决方案。

Abstract: Agentic search has recently emerged as a powerful paradigm, where an agent interleaves multi-step reasoning with on-demand retrieval to solve complex questions. Despite its success, how to design a retriever for agentic search remains largely underexplored. Existing search agents typically rely on similarity-based retrievers, while similar passages are not always useful for final answer generation. In this paper, we propose a novel retriever training framework tailored for agentic search. Unlike retrievers designed for single-turn retrieval-augmented generation (RAG) that only rely on local passage utility, we propose to use both local query-passage relevance and global answer correctness to measure passage utility in a multi-turn agentic search. We further introduce an iterative training strategy, where the search agent and the retriever are optimized bidirectionally and iteratively. Different from RAG retrievers that are only trained once with fixed questions, our retriever is continuously improved using evolving and higher-quality queries from the agent. Extensive experiments on seven single-hop and multi-hop QA benchmarks demonstrate that our retriever, termed \ours{}, consistently outperforms strong baselines across different search agents. Our codes are available at: https://github.com/8421BCD/Agentic-R.

</details>
